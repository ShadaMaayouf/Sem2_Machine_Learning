{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T15:05:33.584082Z",
     "start_time": "2024-01-22T15:05:33.537800Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# 4.1. Markov-Entscheidungsprozesse\n",
    "\n",
    ">## <ins>Table of contents</ins>\n",
    ">* [**4.1.1. Modellierung einer Umgebung**](#4_1_1)\n",
    ">    * [**Das Markov-Entscheidungsprozess**](#4_1_1_a)\n",
    ">    * [**Strategie**](#4_1_1_b)\n",
    ">* [**4.1.2. Iterative Entwicklung der Zustandsnutzen**](#4_1_2)\n",
    ">* [**4.1.3. Iterative Strategieentwicklung**](#4_1_3)\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damit ein Agent lernt Entscheidungen in einer gewissen Umgebung zu treffen, muss diese Umgebung erst modelliert werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ände.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.1. Modellierung einer Umgebung <a name=\"4_1_1\"></a>\n",
    "\n",
    "### Der Markov-Entscheidungsprozess <a name=\"4_1_1_a\"></a>\n",
    "Ein MDP ist ein mathematisches Modell, das die Umgebung eines lernenden Agenten beschreibt. Es besteht aus einem <ins>Zustandsraum</ins>, einem <ins>Aktionsraum</ins>, einer <ins>Übergangswahrscheinlichkeitsfunktion</ins>, einer <ins>Belohnungsfunktion</ins>, einem <ins>Startzustand</ins> und einer <ins>Menge von Endzuständen</ins>.\n",
    "\n",
    "\n",
    "> **Definition 1.: Markov-Entscheidungsprozess**\n",
    ">\n",
    "> Ein Markov-Entscheidungsprozess (engl. Markov decision process, MDP) D ist ein Tupel $D = (S,A,P,R,s_0,S_t)$ mit folgenden Eigenschaften:\n",
    "> 1. $S$ ist eine Menge von Zuständen (der Zustandsraum)\n",
    "> 2. $A$ ist eine Menge von Aktionen (der Aktionsraum)\n",
    "> 3. $P : (S \\setminus S_t) \\times A \\times S \\rightarrow [0,1]$ ist eine Funktion (die Übergangswahrscheinlichkeitsfunktion) mit $\\sum_{s' \\in S} P(s,a,s') = 1$ für alle $s \\in S$, $a \\in A$.\n",
    "> 4. $R : (S \\setminus S_t) \\times A \\times S \\rightarrow \\mathbb{R}$ ist eine beliebige reellwertige Funktion (die Belohnungsfunktion, engl. reward function).\n",
    "> 5. $s_0 \\in S$ ist der Startzustand.\n",
    "> 6. $S_t \\subseteq S$ ist die Menge der Zielzustände.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In jedem *Zustand* $s \\in S$ kann der Agent eine *Aktion* $a \\in A$ ausführen, die zu einem neuen Zustand führt. Die *Übergangswahrscheinlichkeitsfunktion* $P$ gibt die Wahrscheinlichkeit an, dass eine bestimmte Aktion in einem bestimmten Zustand zu einem bestimmten neuen Zustand führt. Die *Belohnungsfunktion* $R$ gibt an, welche Belohnung (oder Strafe, wenn negativ) der Agent erhält, wenn er eine bestimmte Aktion in einem bestimmten Zustand ausführt und zu einem bestimmten neuen Zustand wechselt. \n",
    "\n",
    "Das **Ziel** des Agenten im Reinforcement Learning ist es, eine Strategie zu lernen, die die kumulative Belohnung über die Zeit <ins>maximiert</ins>. Dies wird oft durch eine Kombination aus **Exploration** (neue Aktionen ausprobieren, um mehr über die Umgebung zu erfahren) und **Exploitation** (die besten bekannten Aktionen ausführen) erreicht. \n",
    "\n",
    "Es ist wichtig zu beachten, dass MDPs die Markov-Eigenschaft haben, d.h., die Wahrscheinlichkeit, einen bestimmten neuen Zustand zu erreichen, hängt nur vom aktuellen Zustand und der ausgeführten Aktion ab, nicht von der vorherigen Geschichte des Agenten. Dies vereinfacht das Problem erheblich, da der Agent nicht die gesamte Geschichte seiner Interaktionen mit der Umgebung speichern muss. \n",
    "\n",
    ">\n",
    "> #### **Beipiel:** Staupsauger\n",
    ">\n",
    "> 1. **Zusatndsmenge $S$**: Jeder Zustand könnte eine Kombination aus der aktuellen Position des Roboters (z.B. die verschiedenen Räume der Wohnung) und dem Ladestand der Batterie sein.\n",
    ">    Wenn es beispielsweise vier Räume gibt, in denen der Roboter sich aufhalten kann, und die Batterie drei verschiedene Ladestufen hat (niedrig, mittel, hoch), dann gibt es insgesamt 12 mögliche Zustände.\n",
    "> \n",
    "> 2. **Aktionsmenge $A$**: ”saugen“, ”laden“ oder sich in einen Nachbarraum bewegen.\n",
    ">    In allgemeinen MDPs ist das Resultat einer Aktion nicht immer deterministisch bestimmt (beispielsweise kann das Laden des Roboters fehlschlagen, wenn die Batterie eine Fehlfunktion hat).\n",
    ">    \n",
    "> 3. **Übergangswahrscheinlichkeitsfunktion $P$**:  Diese Funktion könnte davon abhängen, wie der Roboter sich bewegt und wie die Batterie entladen wird. Zum Beispiel könnte die Wahrscheinlichkeit, dass der Roboter sich erfolgreich zu einem anderen Raum bewegt, von der aktuellen Ladung der Batterie abhängen. \n",
    ">\n",
    "> 4. **Belohnungsfunktion $R$**: Die Belohnung könnte positiv sein, wenn der Roboter einen Raum saugt, und negativ sein, wenn der Roboter versucht, sich zu bewegen, aber die Batterie leer ist. Die genaue Definition der Belohnungsfunktion hängt davon ab, was wir den Roboter optimieren wollen. Wenn wir wollen, dass der Roboter so viel wie möglich saugt, könnten wir eine hohe positive Belohnung für das Saugen geben. Wenn wir wollen, dass der Roboter seine Batterie effizient nutzt, könnten wir eine negative Belohnung für das Bewegen geben, wenn die Batterie fast leer ist.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beispiel 1. Ein stark vereinfachtes Modell des Staubsaugerroboterproblems\n",
    "\n",
    "Wir gehen davon aus, dass unsere Wohnung aus zwei Räumen, r1 und r2, besteht, die entweder sauber oder verschmutzt sein können. Der Zustandsraum ist dann definiert durch:\n",
    "\n",
    "$$S^{vc} = \\{s^{1,1}_1, s^{1,1}_2, s^{0,1}_1, s^{0,1}_2, s^{1,0}_1, s^{1,0}_2, s^{0,0}_1, s^{0,0}_2, s^t\\}$$\n",
    "\n",
    "wobei der Zustand $s^{k,l}_i$ repräsentiert, dass der Roboter in Raum $r_i$ ist:\n",
    "- Raum $r_1$ bei $k = 0$ sauber und bei $k = 1$ verschmutzt ist\n",
    "- und Raum $r_2$ bei $l = 0$ sauber und bei $l = 1$ verschmutzt ist.\n",
    "\n",
    "| Zustand | Raum $r_i$ | Raum $r_1$ | Raum $r_2$ |\n",
    "|---------|------------|------------|------------|\n",
    "| $s^{0,0}_1$ | Roboter in Raum $r_1$ | Sauber | Sauber |\n",
    "| $s^{1,0}_1$ | Roboter in Raum $r_1$ | Verschmutzt | Sauber |\n",
    "| $s^{0,1}_1$ | Roboter in Raum $r_1$ | Sauber | Verschmutzt |\n",
    "| $s^{1,1}_1$ | Roboter in Raum $r_1$ | Verschmutzt | Verschmutzt |\n",
    "| $s^{0,0}_2$ | Roboter in Raum $r_2$ | Sauber | Sauber |\n",
    "| $s^{1,0}_2$ | Roboter in Raum $r_2$ | Verschmutzt | Sauber |\n",
    "| $s^{0,1}_2$ | Roboter in Raum $r_2$ | Sauber | Verschmutzt |\n",
    "| $s^{1,1}_2$ | Roboter in Raum $r_2$ | Verschmutzt | Verschmutzt |\n",
    "\n",
    "\n",
    "$s^t$ ist der Zielzustand und $s^{1,1}_1$ ist der Startzustand. \n",
    "\n",
    "In jedem Zustand (außer $s^t$) hat der Roboter drei mögliche Aktionen:\n",
    "\n",
    "1. move: Gehe in den anderen Raum.\n",
    "2. clean: Reinige den aktuellen Raum.\n",
    "3. charge: Lade die Batterie.\n",
    "\n",
    "Also, $A_{vc} = \\{move, clean, charge\\}$. \n",
    "\n",
    "Die Übergangswahrscheinlichkeiten (modelliert durch $P_{vc}$) sind so definiert, dass sich in Raum $r_1$ eine Ladestation befindet und nach Ausführung der Aktion charge in $r_1$ der aktuelle \"Tag\" endet und wir zum finalen Zustand $s^t$ wechseln. \n",
    "\n",
    "Die Aktion move ist zu 90% erfolgreich (d.h., mit Wahrscheinlichkeit 0.1 bleibt der Roboter im aktuellen Raum) und die Aktion clean ist zu 80% erfolgreich (d.h., ein schmutziger Raum bleibt mit Wahrscheinlichkeit 0.2 schmutzig; ein sauberer Raum bleibt sauber). \n",
    "\n",
    "Die Belohnungen ($R_{vc}$) sind wie folgt definiert:\n",
    "\n",
    "1. Wenn der Roboter erfolgreich einen zuvor schmutzigen Raum reinigt, erhält er 10 Punkte.\n",
    "2. Wenn der Roboter einen sauberen Raum reinigt, erhält er -2 Punkte (da er unnötig Energie verbraucht hat).\n",
    "3. Wenn der Roboter in Raum $r_2$ zu laden versucht, erhält er -5 Punkte (er hat bei dem Versuch seine Batterie beschädigt).\n",
    "4. Wenn der Roboter in $r_1$ auflädt, ohne vorher alle Räume gereinigt zu haben, erhält er -7 Punkte.\n",
    "5. Für jede Bewegung mittels move (unabhängig davon, ob erfolgreich oder nicht) erhält der Roboter -1 Punkt (der Roboter soll möglichst effizient die Wohnung reinigen).\n",
    "\n",
    "Für alle weiteren Situationen beträgt die Belohnung 0 Punkte.\n",
    "\n",
    "Wir erhalten: $$ D_{vc} = (S_{vc}, A_{vc}, P_{vc}, R_{vc}, s^{1,1}_1, S_{vc}^t = \\{s^t\\})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategie <a name=\"4_1_1_b\"></a>\n",
    "\n",
    "Um den Agenten in einem MDP zu steuern, besitzt dieser eine *Strategie* (engl. policy), die (im einfachsten Fall) für jeden Zustand die auszuwählende Aktion bestimmt.\n",
    "\n",
    ">**Definition 2.:** Strategie\n",
    ">\n",
    ">Sei $D = (S,A,P,R,s_0,S_t)$ ein MDP. Eine konstante Strategie $\\pi$ für $D$ ist eine Funktion $$\\pi : S \\setminus S_t \\rightarrow A$$\n",
    "\n",
    "\n",
    "Eine **konstante Strategie** ist eine Funktion, die jedem Zustand eine bestimmte Aktion zuweist. Sie ist deterministisch, d.h., sie gibt für jeden Zustand genau eine Aktion vor.\n",
    "\n",
    "Es gibt jedoch auch **probabilistische Strategien**, die jedem Zustand eine Wahrscheinlichkeitsverteilung über die möglichen Aktionen zuweisen. Diese Art von Strategie kann nützlich sein, wenn es Unsicherheit oder Zufälligkeit in der Umgebung gibt. \n",
    "\n",
    "In vielen Fällen, insbesondere bei komplexen MDPs, kann es vorteilhaft sein, probabilistische Strategien zu verwenden, um eine bessere Leistung zu erzielen. Aber für den Anfang ist es absolut sinnvoll, sich auf konstante Strategien zu konzentrieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Beispiel 2** Fortsetzung von Beispiel 1\n",
    "\n",
    "Eine mögliche Strategie $π_{vc}$ ist wie folgt definiert:\n",
    "\n",
    "$$\\pi_{vc}(s^{1,1}_{1}) = \\pi_{vc}(s^{1,0}_{1}) = \\pi_{vc}(s^{1,1}_{2}) = \\pi_{vc}(s^{0,1}_{2}) = \\text{clean}$$\n",
    "$$\\pi_{vc}(s^{0,1}_{1}) = \\pi_{vc}(s^{1,0}_{2}) = \\pi_{vc}(s^{0,0}_{2}) = \\text{move}$$\n",
    "$$\\pi_{vc}(s^{0,0}_{1}) = \\text{charge}$$\n",
    "\n",
    "Das bedeutet, wenn der Roboter sich in einem verschmutzten Raum befindet, führt er die Aktion `clean` aus. Wenn der aktuelle Raum sauber ist und der andere Raum verschmutzt ist, wechselt der Roboter in den anderen Raum (`move`). Wenn beide Räume sauber sind, wechselt der Roboter zuerst in Raum `r1` und lädt dann auf (`charge`).\n",
    "\n",
    "Die Aufgabe eines Offline-Lernverfahrens besteht darin, <ins>eine optimale Strategie `π*` für ein gegebenes festes MDP zu erlernen</ins>. \n",
    "\n",
    "Eine **optimale Strategie** ist diejenige, die die akkumulierte erwartete Belohnung maximiert, die der Agent durch die Anwendung der Strategie erhält. \n",
    "\n",
    "Um dies zu definieren, benötigen wir einige zusätzliche Konzepte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Definition 3.:** Episode\n",
    ">\n",
    ">Sei $D = (S,A,P,R,s_0,S_t)$ ein MDP. Eine Episode `e` in $D$ ist eine potenziell unendliche Folge $e = (s_0, a_1, s_1, a_2, s_2, ...)$ mit $s_0, s_1 ... \\in S$ und $a_0, a_1 ... \\in A$ Die Wahrscheinlichkeit $P(e)$ ist definiert durch\n",
    ">$$P(e) = \\prod_{i>0} P(s_{i-1},a_i,s_i)$$\n",
    "\n",
    "Eine **Episode** `e` in einem MDP ist eine potenziell unendliche Folge von Zuständen und Aktionen, die mit dem Anfangszustand `s0` beginnt und die Aktionen `a1, a2, ...` ausführt, wobei der Agent in den Zuständen `s1, s2, ...` landet. \n",
    "\n",
    "Eine Episode ist **initial**, wenn sie mit dem Anfangszustand $s_0$ beginnt bzw. wenn $s_0 = s^0$ gilt, und sie ist **terminierend**, wenn sie mit einem Zustand $s_n$ endet, der zu den Endzuständen $S^t$ gehört d.h. $s_n \\in S^t$ .\n",
    "\n",
    "Der unendliche Fall tritt beispielsweise für den Roboter aus unserem Staubsaugerbeispiel auf, wenn dieser permanent nur die move-Aktion ausführt oder wenn eine move-Aktion permanent fehlschlägt (wobei für solch eine Episode die Wahrscheinlichkeit gegen 0 geht).\n",
    "Jede Episode akkumuliert gewisse Belohnungen und die Summe aller Belohnungen nennen wir Nutzen der Episode (engl. utility oder auch return)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Der **Nutzen** einer Episode ist die Summe aller akkumulierten Belohnungen, wobei jede Belohnung mit einem **Diskontfaktor** `γ` gewichtet wird. Dieser Diskontfaktor liegt im Bereich von 0 bis 1 und bestimmt, wie stark zukünftige Belohnungen im Vergleich zu sofortigen Belohnungen bewertet werden. Bei kleinen Werten von `γ` werden Strategien bevorzugt, die schnell hohe Belohnungen erhalten, während bei größeren Werten nahe 1 spätere Belohnungen wichtiger werden.\n",
    "\n",
    "Die Definition des diskontierten Nutzens ist wie folgt:\n",
    "\n",
    "$$\n",
    "U^{\\gamma}_D(e) = \\sum_{i>0} \\gamma^{i-1} R(s_{i-1}, a_i, s_i)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
