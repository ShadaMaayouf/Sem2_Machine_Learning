{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T15:05:33.584082Z",
     "start_time": "2024-01-22T15:05:33.537800Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# 4.2. Passives Reinforcement-Learning\n",
    "\n",
    ">## <ins>Table of contents</ins> <a name=\"up\"></a>[<sup>[1]</sup>](#cite_note-1)\n",
    ">* [**4.2.1. Probeläufe und Zustandsnutzen**](#4_2_1)\n",
    ">* [**4.2.2. Adaptive dynamische Programmierung (ADP)**](#4_2_2)\n",
    ">* [**4.2.3. Temporal Difference Learning (TD)**](#4_2_3)\n",
    ">* [**4.2.4. Unterschiede zwischen ADP und TD**](#4_2_4)\n",
    ">\n",
    ">## <ins>Beispiele</ins>\n",
    ">* [**Beispiel 1**: Staubsauger](#b1)\n",
    ">* [**Beispiel 2**: Multimenge der Probeläufen und der Nutzen $U^{\\gamma}_{\\partial}(s | \\pi)$](#b2)\n",
    ">* [**Beispiel 3**: Nutzen einer Episode $e$ *Staubsauger*](#b3)\n",
    ">* [**Beispiel 4**: Nutzen einer Strategie $\\pi$ von *Staubsauger*](#b4)\n",
    "\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Problemstellung**:\n",
    "\n",
    "- Die im vorherigen Unterkapitel diskutierten Methoden lernen eine optimale Strategie $\\pi^*$, vorausgesetzt, es gibt ein korrektes Modell der Umgebung in Form eines Markov-Entscheidungsprozesses. In den meisten realistischen Anwendungsszenarien existiert kein solches Modell. Beispielsweise hat ein Staubsaugerroboter normalerweise keinen Lageplan der zu reinigenden Wohnung und ein Schachspieler kein Modell der gegnerischen Strategie.\n",
    "\n",
    "- Ein weiterer Nachteil der Methoden des vorherigen Unterkapitels ist ihre hohe Lernzeit. Zum Beispiel hat Schach ungefähr $10^{45}$ Zustände und eine einzige Iteration des SI-Algorithmus müsste bereits die Nutzenwerte aller dieser Zustände in Bezug auf eine initiale Strategie approximieren.\n",
    "  \n",
    "- Im Folgenden werden Ansätze diskutiert, <ins>die eine optimale Strategie erlernen</ins>, ohne ein Modell der Umgebung zu nutzen. Außerdem werden Methoden des passiven Reinforcement Learnings betrachtet, d.h., Methoden, die bereits eine festgelegte Strategie annehmen und ausschließlich die Nutzenwerte der Zustände erlernen (die dann auch zur Verbesserung der Strategie genutzt werden können).\n",
    "- Im nächsten Unterkapitel wird aktives Reinforcement Learning betrachtet, bei dem während des Lernens der Nutzenwerte auch direkt die optimale Strategie erlernt wird.\n",
    "  \n",
    "## 4.2.1. Probeläufe und Zustandsnutzen <a name=\"4_2_1\"></a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sei $π$ eine Strategie.\n",
    "- Imm Algemeinen, ist das Ziel der folgenden Methoden die Bestimmung des Nutzens $U^{\\gamma}_D(s | \\pi)$ aller Zustände $s \\in S \\setminus St$ bezüglich π eines unbekannten Markov-Entscheidungsprozesses $D = (S,A,P,R,s_0,St)$ und mit einen gegebenen Diskontfaktor $γ$.\n",
    "\n",
    "- <a name=\"U_gamma_D\"></a> Nach Definition 6 aus Unterkapitel 4.1 ist $U^{\\gamma}_D(s | \\pi)$ definiert als $$U^{\\gamma}_D(s | \\pi) = \\sum_{\\pi \\sim e=(s,a_1,s_1,...)} P(e) \\sum_i \\gamma^{i-1} R(s_{i-1},a_i,s_i)$$ \n",
    "```\r\n",
    "\r\n",
    "Wenn Sie auf \"Google\" klicken, werden Sie zur Google-Homepage weitergeleitet. Bitte beachten Sie, dass es keine Leerzeichen zwischen den eckigen und den runden Klammern geben sollte.\n",
    "\n",
    "- Die **Aufgabe des passiven Reinforcement Learnings** entspricht der Strategieevaluation aus Abschnitt 4.1.3 (*Policy evaluation*), mit dem Unterschied, dass die Effekte von Aktionen (insbesondere die Übergangswahrscheinlichkeiten *P*) zunächst nicht bekannt sind.\n",
    "- Um $U^{\\gamma}_D(s | \\pi)$ zu lernen, führt der Agent eine Anzahl von \"Probefahrten\" in der Umgebung aus und protokolliert sowohl die besuchten Zustände als auch die erhaltenen Belohnungen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Definition 1. Beobachtung $o$ und Probelauf $O$**\n",
    ">\n",
    "> Sei $\\pi : S \\rightarrow A$ eine Strategie bzgl. einer Menge von Zuständen $S$ und einer Menge von Aktionen $A$.\n",
    ">\n",
    "> Eine Beobachtung $o$ ist ein Tupel $o = (s,a,s',r)$ mit $s,s' \\in S$, $a \\in A$, $r \\in R$.\n",
    ">\n",
    "> Eine Beobachtung $o = (s,a,s',r)$ ist $\\pi$-induziert, falls $a = \\pi(s)$. Ein Probelauf $O$ bzgl.\n",
    ">\n",
    "> $\\pi$ ist eine (endliche) Sequenz $O = (o_1,...,o_n)$ mit $\\pi$-induzierten Beobachtungen $o_i = (s_i,a_i,s'_i,r_i)$ mit $s'_i = s_{i+1}$, für $i = 1,...,n-1$. Für $\\gamma \\in [0,1]$ definiere\n",
    "$$rew^{\\gamma}(O) = \\sum_{i=1}^{n} \\gamma^{i-1}r_i$$\n",
    "\n",
    "Die Definition beschreibt die Konzepte der Beobachtung und des Probelaufs in einem Entscheidungsprozess, der durch eine Strategie $\\pi$ gesteuert wird.\n",
    "\n",
    "- **Beobachtung ($o$)**: Ein Tupel bestehend aus einem Zustand $s$, einer Aktion $a$, einem Folgezustand $s'$ und einer Belohnung $r$. Eine Beobachtung ist $\\pi$-induziert, wenn die Aktion $a$ gleich der Aktion ist, die von der Strategie $\\pi$ für den Zustand $s$ ausgewählt wurde.\n",
    "\n",
    "- **Probelauf ($O$)**: Eine Sequenz von $\\pi$-induzierten Beobachtungen D.h.  unter Ausführung der Strategie $π$. Jede Beobachtung in der Sequenz besteht aus einem Zustand $s_i$, einer Aktion $a_i$, einem Folgezustand $s'_i$ und einer Belohnung $r_i$. Der Folgezustand einer Beobachtung ist der Zustand der nächsten Beobachtung in der Sequenz.\n",
    "\n",
    "  **!!** Beachten Sie, dass ein Probelauf eine direkte Entsprechung zu einer (initialen) Episode hat, der Unterschied ist nur dass ein Probelauf zusätzlich Informationen zu Belohnungen enthält.\n",
    "\n",
    "- **Belohnungsfunktion ($rew^\\gamma$)**: Eine Funktion, die eine Sequenz von Beobachtungen nimmt und eine gewichtete Summe der Belohnungen bzw. die diskontierte Summe der Belohnungen für einen Probelauf $O$ berechnet. Jede Belohnung wird mit $\\gamma^{i-1}$ gewichtet, wobei $i$ der Index der Beobachtung in der Sequenz ist und $\\gamma$ ein Diskontierungsfaktor zwischen 0 und 1 ist. Diese Funktion wird verwendet, um die Gesamtbelohnung eines Probelaufs zu berechnen. Je größer der Diskontierungsfaktor $\\gamma$ (Je näher an 1), desto stärker werden zukünftige Belohnungen gewichtet. Wenn $\\gamma = 0$, zählt nur die sofortige Belohnung."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Die Lernaufgabe -->` ist aus einer Reihe von Probelaufen $O_1,...,O_m$, sollen die Werte $U^{\\gamma}_D(s | \\pi)$ für alle $s \\in S \\setminus St$ approximiert werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Beispiel 1.** <a name=\"b1\"></a> Fortsetzung des Staubsaugerroboterproblems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir betrachten wieder das Beispiel des Staubsaugerroboters aus [Unterkapitel 4.1.](http://localhost:8888/lab/tree/KE4_Reinforcement_Learning/4.1.-Markov-Entscheidungsprozess.ipynb). In diesem Beispiel geht es um einen Staubsaugerroboter, der in einer Umgebung mit zwei Räumen (Raum r1 und Raum r2) agiert. Der Roboter kann drei Aktionen ausführen: `clean` (reinigen), `move` (bewegen) und `charge` (aufladen).\n",
    "\n",
    "Die Zustände des Roboters sind durch die Menge $S_{vc}$ repräsentiert:\n",
    "\n",
    "$$\n",
    "S_{vc} = \\{s_{1,1}^{1}, s_{1,1}^{2}, s_{0,1}^{1}, s_{0,1}^{2}, s_{1,0}^{1}, s_{1,0}^{2}, s_{0,0}^{1}, s_{0,0}^{2}, s_{t}\\}\n",
    "$$\n",
    "\n",
    "Diese Zustände beinhalten verschiedene Kombinationen von Raumposition und Raumreinigungsstatus. Zum Beispiel repräsentiert der Zustand $s_{1,1}^{1}$, dass der Roboter sich in Raum r1 befindet, und Beide Räume sind sauber. Der Zustand $s_{t}$ repräsentiert einen Terminalzustand, in dem der Roboter seine Aufgabe abgeschlossen hat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Menge der Aktionen $A_{vc}$ ist\n",
    "\n",
    "$$\n",
    "A_{vc} = \\{\\text{move}, \\text{clean}, \\text{charge}\\}\n",
    "$$\n",
    "\n",
    "Wir betrachten weiterhin die Strategie $\\pi_{vc}$ mit\n",
    "\n",
    "$$\\pi_{vc}(s_{1,1}^{1}) = \\pi_{vc}(s_{1,0}^{1}) = \\pi_{vc}(s_{1,1}^{2}) = \\pi_{vc}(s_{0,1}^{2}) = \\text{clean} $$\n",
    "$$\\pi_{vc}(s_{0,1}^{1}) = \\pi_{vc}(s_{1,0}^{2}) = \\pi_{vc}(s_{0,0}^{2}) = \\text{move}$$\n",
    "$$\\pi_{vc}(s_{0,0}^{1}) = \\text{charge}$$\n",
    "\n",
    "$\\pi_{vc}$ definiert, welche Aktion der Roboter in jedem Zustand ausführen soll. Zum Beispiel führt der Roboter die Aktion `clean` aus, wenn er sich im Zustand $s_{1,1}^{1}$, $s_{1,0}^{1}$, $s_{1,1}^{2}$ oder $s_{0,1}^{2}$ befindet.\n",
    "\n",
    "und die folgenden drei Probelaufe\n",
    "\n",
    "$$\n",
    "O1 = ((s_{1,1}^{1}, \\text{clean}, s_{0,1}^{1}, 10), (s_{0,1}^{1}, \\text{move}, s_{0,1}^{2}, -1), (s_{0,1}^{2}, \\text{clean}, s_{0,0}^{2}, 10), (s_{0,0}^{2}, \\text{move}, s_{0,0}^{1}, -1), (s_{0,0}^{1}, \\text{charge}, s_{t}, 0)) $$\n",
    "$$O2 = ((s_{1,1}^{1}, \\text{clean}, s_{1,1}^{1}, 0), (s_{1,1}^{1}, \\text{clean}, s_{0,1}^{1}, 10), (s_{0,1}^{1}, \\text{move}, s_{0,1}^{1}, -1), (s_{0,1}^{1}, \\text{move}, s_{0,1}^{2}, -1), (s_{0,1}^{2}, \\text{clean}, s_{0,0}^{2}, 10), (s_{0,0}^{2}, \\text{move}, s_{0,0}^{1}, -1), (s_{0,0}^{1}, \\text{charge}, s_{t}, 0)) $$\n",
    "$$O3 = ((s_{1,1}^{1}, \\text{clean}, s_{0,1}^{1}, 10), (s_{0,1}^{1}, \\text{move}, s_{0,1}^{1}, -1), (s_{0,1}^{1}, \\text{move}, s_{0,1}^{1}, -1), (s_{0,1}^{1}, \\text{move}, s_{0,1}^{2}, -1), (s_{0,1}^{2}, \\text{clean}, s_{0,0}^{2}, 10), (s_{0,0}^{2}, \\text{move}, s_{0,0}^{1}, -1), (s_{0,0}^{1}, \\text{charge}, s_{t}, 0))\n",
    "$$\n",
    "\n",
    "Die Probelaufe $O1$, $O2$ und $O3$ sind Sequenzen von Zuständen und Aktionen, die der Roboter durchläuft, wobei Probelauf $O1$ entspricht dem erwarteten, idealen Ablauf (der Roboter reinigt zunächst Raum $r1$, bewegt sich anschließend in Raum $r2$, reinigt diesen, kehrt zurück in Raum $r1$, und lädt sich auf). Die Probelaufe $O2$ und $O3$ enthalten dagegen einige fehlgeschlagene Aktionen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach der Definition von [$U^{\\gamma}_D(s | \\pi)$](#U_gamma_D) ist der Nutzen eines Zustands das gewichtete Mittel aller Episoden, die in diesem Zustand starten, gemäß einer bestimmten Strategie $\\pi$. \n",
    "\n",
    "Wenn ein Zustand $s$ in einem Probelauf $O$ auftritt, enthält $O$ als Teilsequenz eine Episode, die in $s$ startet. Es kann auch mehrere solcher Episoden geben, wenn $s$ mehrmals in $O$ besucht wurde. Eine Episode, die mit dem Zustand $s_{1,1}^{1}$ beginnt und als Teilsequenzen in $O_1$ enthalten ist, könnte wie folgt aussehen:\n",
    "\n",
    "$$\n",
    "(s_{0,1}^{1}, \\text{move}, s_{0,1}^{2}, -1), (s_{0,1}^{2}, \\text{clean}, s_{0,0}^{2}, 10), (s_{0,0}^{2}, \\text{move}, s_{0,0}^{1}, -1), (s_{0,0}^{1}, \\text{charge}, s_{t}, 0)\n",
    "$$\n",
    "\n",
    "Der Nutzen $U^{\\gamma}_D(s | \\pi)$ kann dann durch den Durchschnitt der Nutzen dieser beobachteten Teilsequenzen approximiert werden. \n",
    "\n",
    "Das bedeutet, dass der erwartete Nutzen eines Zustands über alle möglichen Episoden, die von diesem Zustand ausgehen, gemittelt wird. Dies berücksichtigt sowohl die unmittelbaren Belohnungen als auch die zukünftigen Belohnungen, die durch das Ausführen der Strategie $\\pi$ ab diesem Zustand erzielt werden können. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Definition 2. Multimenge der Probeläufen und der Nutzen $U^{\\gamma}_{\\partial}(s | \\pi)$**\n",
    ">\n",
    "> Sei $O$ ein Probelauf mit $O = (o_1,...,o_n)$, eine Sequenz von Beobachtungen $o_i = (s_i,a_i,s'_i,r_i)$ (für $i = 1,...,n$) und $s \\in S \\setminus S^t$ ein Zustand. Definiere die Menge $O_s$ durch\n",
    "$$\n",
    "O_s = \\{(o_k,...,o_n) | s_k = s\\}\n",
    "$$\n",
    "Für eine (Multi-)Menge $\\vartheta = \\{O^1,...,O^m\\}$ an Probelaufen definiere die Multimenge\n",
    "$$\n",
    "\\partial_s = O_s^1 \\cup ... \\cup O_s^m\n",
    "$$\n",
    "und\n",
    "$$\n",
    "U^\\gamma_{\\partial}(s | \\pi) = \\frac{\\sum_{O \\in \\partial_s} \\text{rew}^\\gamma (O)}{|\\partial_s|}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Menge $O_s$ ist definiert als die Menge aller Teilsequenzen von $O$, die mit dem Zustand $s$ beginnen. Das heißt, für jeden Probelauf in $O$, wenn der Zustand $s$ erreicht wird, wird die verbleibende Sequenz von Ereignissen als Teil von $O_s$ betrachtet.\n",
    "\n",
    "Wenn wir eine Menge von Probeläufen $O = \\{O_1, ..., O_m\\}$ haben, dann ist die Multimenge $\\partial_s$ definiert als die Vereinigung der Mengen $O_s^1, ..., O_s^m$, die jeweils die Teilsequenzen von $O_1, ..., O_m$ sind, die mit dem Zustand $s$ beginnen.\n",
    "\n",
    "Schließlich ist der Nutzen $U^{\\gamma}_{\\partial}(s | \\pi)$ eines Zustands $s$ unter einer Strategie $\\pi$ definiert als der Durchschnitt der diskontierten Belohnungen aller Teilsequenzen in $\\partial_s$. Hierbei ist $\\gamma$ der Diskontierungsfaktor und $rew^{\\gamma}(O)$ die Funktion, die die diskontierte Belohnung einer Sequenz von Ereignissen berechnet.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Beispiel 2.** <a name=\"b2\"></a> Fortsetzung von Beispiel 1\n",
    "\n",
    "Wir führen Beispiel 1 fort und betrachten den Zustand $s_{0,1}^1$. Insgesamt gibt es **6 (Teil-)Probeläufe** die in $s_{0,1}^1$ starten:\n",
    "\n",
    "$$ \\partial_{s_{0,1}^{1}} = \\{ $$\n",
    "$$((s_{0,1}^{1}, \\text{move}, s_{0,1}^{2}, -1), (s_{0,1}^{2}, \\text{clean}, s_{0,0}^{2}, 10), (s_{0,0}^{2}, \\text{move}, \\\\ s_{0,0}^{1}, -1), (s_{0,0}^{1}, \\text{charge}, s_{t}, 0)), $$\n",
    "$$((s_{0,1}^{1}, \\text{move}, s_{0,1}^{1}, -1), (s_{0,1}^{1}, \\text{move}, s_{0,1}^{2}, -1), (s_{0,1}^{2}, \\text{clean}, s_{0,0}^{2}, 10), (s_{0,0}^{2}, \\text{move}, s_{0,0}^{1}, -1), (s_{0,0}^{1}, \\text{charge}, s_{t}, 0)),$$\n",
    "$$((s_{0,1}^{1}, \\text{move}, s_{0,1}^{2}, -1), (s_{0,1}^{2}, \\text{clean}, s_{0,0}^{2}, 10), (s_{0,0}^{2}, \\text{move}, s_{0,0}^{1}, -1), (s_{0,0}^{1}, \\text{charge}, s_{t}, 0)), $$\n",
    "$$((s_{0,1}^{1}, \\text{move}, s_{0,1}^{1}, -1), (s_{0,1}^{1}, \\text{move}, s_{0,1}^{1}, -1), (s_{0,1}^{1}, \\text{move}, s_{0,1}^{2}, -1), (s_{0,1}^{2}, \\text{clean}, s_{0,0}^{2}, 10), (s_{0,0}^{2}, \\text{move}, s_{0,0}^{1}, -1), (s_{0,0}^{1}, \\text{charge}, s_{t}, 0)),$$\n",
    "$$((s_{0,1}^{1}, \\text{move}, s_{0,1}^{1}, -1), (s_{0,1}^{1}, \\text{move}, s_{0,1}^{2}, -1), (s_{0,1}^{2}, \\text{clean}, s_{0,0}^{2}, 10), (s_{0,0}^{2}, \\text{move}, s_{0,0}^{1}, -1), (s_{0,0}^{1}, \\text{charge}, s_{t}, 0)), $$\n",
    "$$((s_{0,1}^{1}, \\text{move}, s_{0,1}^{2}, -1), (s_{0,1}^{2}, \\text{clean}, s_{0,0}^{2}, 10), (s_{0,0}^{2}, \\text{move}, s_{0,0}^{1}, -1), (s_{0,0}^{1}, \\text{charge}, s_{t}, 0))$$\n",
    "$$\\}$$\n",
    "\n",
    "Dieser Zustand kommt (als erste Komponente einer Beobachtung) **einmal** in $O_1$, **zweimal** in $O_2$ und **dreimal** in $O_3$ vor. Deshalb erhalten wir weiterhin (für $\\gamma = 0.9$)\n",
    "\n",
    "$$U^\\gamma_\\partial(s_{0,1}^1 | \\pi_{vc}) = \\frac{1}{6} \\left(3(-1+0.9 \\cdot 10 + 0.9^2(-1)+0.9^3 \\cdot 0)+2(-1+0.9(-1)+0.9^2 \\cdot 10+0.9^3(-1)+0.9^4 \\cdot 0)+( -1+0.9(-1)+0.9^2(-1)+0.9^3 \\cdot 10+0.9^4(-1)+0.9^5 \\cdot 0)\\right) $$\n",
    "$$\\approx \\frac{1}{6} \\left(3 \\cdot 7.19+2 \\cdot 5.471+3.924\\right) \\approx 6.073$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **!!**\n",
    "> \n",
    "> Bitte Beachte, dass die Werte $U^{\\gamma}_\\partial(s | \\pi)$, die auf der Grundlage einer großen Anzahl von Probeläufen berechnet werden, sich den tatsächlichen Werten $U^{\\gamma}_D(s | \\pi)$ des generierenden Markov-Entscheidungsprozesses $D$ annähern bzw. gegen den tatsächlichen Werten $U^{\\gamma}_D(s | \\pi)$ konvergieren.\n",
    ">\n",
    ">Mit anderen Worten, wenn Sie eine ausreichend große Anzahl von Probeläufen durchführen, werden die geschätzten Werte $U^{\\gamma}_\\partial(s | \\pi)$ immer näher an die tatsächlichen Werte $U^{\\gamma}_D(s | \\pi)$ herankommen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Ansatz hat zwei grundlegende **Nachteile**:\n",
    "1. Nur die Werte $U^{\\gamma}_{\\partial}(s | \\pi)$ berechnet werden können, für die $s$ in einem Probelauf vorkommt. Ungesehene Zustände $s$ können also nicht approximiert werden.\n",
    "2. die Approximation über $U^{\\gamma}_{\\partial}(s | \\pi)$ nutzt nicht die Zusammenhänge zwischen den Zuständen. Der Nutzen eines Zustands $s$ ist direkt abhängig von den Nutzen seiner direkten Nachfolgezustände. Die Definition von $U^{\\gamma}_{\\partial}(s | \\pi)$ als Durchschnitt der Nutzen aller beobachteten Probeläufe berechnet diese Nutzen für jeden Zustand $s$ unabhängig von allen anderen Zuständen.\n",
    "\n",
    "In der Praxis bedeutet dies, dass eine Berechnung von $U^{\\gamma}_D(s | \\pi)$ über $U^{\\gamma}_{\\partial}(s | \\pi)$ üblicherweise nur sehr langsam konvergiert. Während der erste Nachteil inhärent für alle Methoden des passiven Reinforcement Learnings ist, da wir eine feste Strategie annehmen und dadurch üblicherweise nicht alle möglichen Zustände sehen werden, können wir durch alternative Methoden den zweiten Nachteil umgehen.\n",
    "\n",
    "Diese Methoden werden in den nächsten Abschnitten diskutiert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.2. Adaptive dynamische Programmierung <a name=\"4_2_2\"></a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In der adaptiven dynamischen Programmierung wird der rekursive Zusammenhang zwischen den Nutzen der Zustände in Bezug auf die Strategie $π$ genutzt. Dieser Zusammenhang wird durch die Bellman-Gleichung gegeben:\n",
    "\n",
    "[$$U^{\\gamma}_D(s | \\pi) = \\sum_{s' \\in S} P(s,\\pi(s),s') \\left[ R(s,\\pi(s),s')+\\gamma U^{\\gamma}_D(s' | \\pi) \\right]$$](http://localhost:8888/lab/tree/KE4_Reinforcement_Learning/4.1.-Markov-Entscheidungsprozess.ipynb)\n",
    "\n",
    "Aus einer gegebenen Menge von Probelaufdaten $\\partial = {O^1,...,O^m}$ können zunächst die Werte $P(s,π(s),s')$ und $R(s,π(s),s')$ für alle beobachteten Zustände $s$ und $s'$ approximiert werden. Anschließend kann das durch die Gleichung aufgestellte Gleichungssystem nach den unbekannten Werten $U^γ_D(s | π)$ gelöst werden.\n",
    "\n",
    "Um die Wahrscheinlichkeiten $P(s,π(s),s')$ aus einer Menge von Probelaufdaten $\\partial = {O^1,...,O^m}$ zu approximieren, wird gezählt, wie oft nach Ausführung der Aktion $π(s)$ in Zustand $s$ der Zustand $s'$ beobachtet wird, und die relative Häufigkeit wird genommen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Definition 3: Die approximierte Übergangswahrscheinlichkeit $\\tilde{P}_{\\partial}(s,\\pi(s),s')$**\n",
    "> \n",
    "> Sei $\\partial = \\{O_1,...,O_m\\}$ eine Menge von Probelaufen (bzgl. $\\pi$) mit $O_j = (o^j_1,...,o^j_{n_j})$, $o^j_i = (s^j_i,\\pi(s^j_i),(s')^j_i,r^j_i)$ (für $j = 1,...,m$ und $i = 1,...,n_j$ für passende $n_j$).\n",
    ">\n",
    "> Definiere\n",
    ">\n",
    "> $$\\tilde{P}_{\\partial}(s,\\pi(s),s') = \\frac{\\left|\\{o^j_i | s^j_i = s,(s')^j_{i} = s'\\}\\right|}{\\left|\\{o^j_i | s^j_i = s\\}\\right|}$$\n",
    ">\n",
    "> als die approximierte Übergangswahrscheinlichkeit, die angibt, wie wahrscheinlich es ist, vom Zustand $s$ in den Zustand $s'$ zu gelangen, wenn die Aktion $\\pi(s)$ ausgeführt wird. Es wird berechnet als das Verhältnis der Anzahl der Übergänge zu den jeweiligen Zuständen zur Gesamtzahl der Beobachtungen.\n",
    "> \n",
    "> Falls $\\left|\\{o^j_i | s^j_i = s\\}\\right| = 0$, definiere $\\tilde{P}(s,\\pi(s),s') = 0$.\n",
    "\n",
    "Diese Definition bezieht sich auf die Methode zur Approximation der Übergangswahrscheinlichkeiten in einem Markov-Entscheidungsprozess basierend auf einer Menge von Probelaufdaten. \n",
    "\n",
    "Diese Definition beschreibt, wie die Übergangswahrscheinlichkeiten $P(s,\\pi(s),s')$ aus den Probelaufdaten approximiert werden können. Es wird gezählt, wie oft nach Ausführung der Aktion $\\pi(s)$ im Zustand $s$ der Zustand $s'$ beobachtet wird, und diese relative Häufigkeit wird als Schätzung für $P(s,\\pi(s),s')$ genommen. Wenn der Zustand $s$ nie beobachtet wird, wird $P(s,\\pi(s),s')$ als 0 definiert. Diese Methode ist nützlich, wenn die genauen Übergangswahrscheinlichkeiten des zugrunde liegenden Prozesses nicht bekannt sind und aus den Daten geschätzt werden müssen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Definition 4: Die approximierte unmittelbare Belohnung $\\tilde{R}_{\\partial}(s,\\pi(s),s')$**\n",
    "> \n",
    ">Sei $\\partial = \\{O_1,...,O_m\\}$ eine Menge von Probelaufen (bzgl. $\\pi$) mit $O_j = (o_{j1},...,o_{j_{n_j}})$, $o_{ji} = (s_{ji},\\pi(s_{ji}),(s'_{j})_{i},r_{ji})$ (für $j = 1,...,m$ und $i = 1,...,n_j$ für passende $n_j$).\n",
    ">\n",
    "> Definiere\n",
    ">$$\\tilde{R}_{\\partial}(s,\\pi(s),s') = \\begin{cases} r^j_i & \\text{für beliebige } i, j \\text{ mit } s^j_i = s \\text{ und } (s')^j_{i} = s' \\\\ 0 & \\text{sonst} \\end{cases}$$\n",
    ">\n",
    "> als die approximierte unmittelbare Belohnung, die der Agent erhält, wenn er im Zustand $s$ die Aktion $\\pi(s)$ ausführt und in den Zustand $s'$ gelangt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$.\r\n",
    "\r\n",
    "- $\\tilde{R}_{\\partial}(s,\\pi(s),s')$ ist eine Funktion, die eine approximierte Belohnung für einen gegebenen Zustand $s$, eine Aktion $\\pi(s)$ und einen Nachfolgezustand $s'$ liefert.\r\n",
    "\r\n",
    "- Die approximierte Belohnung ist gleich $r^j_i$ für irgendein $i, j$ mit $s^j_i = s$ und $(s')^j_{i} = s'$. Das bedeutet, wenn der aktuelle Zustand und der Nachfolgezustand mit einem Zustand und einem Nachfolgezustand in den Probelaufen übereinstimmen, wird die entsprechende Belohnung verwendet.\r\n",
    "\r\n",
    "- Wenn es keine Übereinstimmung in den Probelaufen gibt, wird die approximierte Belohnung als 0 dndet werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun können wir das durch die sogenannte Bellman-Gleichung lösen. Die Gleichung lautet:\n",
    "\n",
    "$$\\tilde{U}^\\gamma_{\\partial}(s | \\pi) = \\sum_{s' \\in S} \\tilde{P}_{\\partial}(s,\\pi(s),s') \\left[ \\tilde{R}_{\\partial}(s,\\pi(s),s') + \\gamma \\tilde{U}^\\gamma_{\\partial}(s' | \\pi) \\right] \\tag{3}$$\n",
    "\n",
    "Sie beschreibt die erwartete kumulative Belohnung, die ein Agent erhält, wenn er eine bestimmte Strategie $\\pi$ verfolgt.\n",
    "Hierbei steht:\n",
    "- $\\tilde{U}^\\gamma_O(s | \\pi)$ für den erwarteten kumulativen Diskontbelohnung, die der Agent erhält, wenn er im Zustand $s$ startet und die Politik $\\pi$ verfolgt.\n",
    "- $\\tilde{P}_O(s,\\pi(s),s')$ ist die approximierte Übergangswahrscheinlichkeit basierend auf Definition 3.\n",
    "- $\\tilde{R}_O(s,\\pi(s),s')$ ist die approximierte unmittelbare Belohnung basierend auf Definition 4.\n",
    "- $\\gamma$ ist der Diskontfaktor, der zukünftige Belohnungen abwertet.\n",
    "\n",
    "Die Gleichung (3) kann mit den Methoden aus [Abschnitt 4.1.3](http://localhost:8888/lab/tree/KE4_Reinforcement_Learning/4.1.-Markov-Entscheidungsprozess.ipynb) gelöst werden. Es kann weiterhin gezeigt werden, dass bei einer \"genügend großen\" Anzahl an Probelaufen die Werte $\\tilde{U}^\\gamma_O(s | \\pi)$ gegen die tatsächlichen Werte $U^\\gamma_D(s | \\pi)$ des generierenden Markov-Entscheidungsprozesses konvergieren. Das bedeutet, dass die approximierten Werte immer genauer werden, je mehr Probelaufe durchgeführt werden. Dies ist ein zentraler Aspekt des bestärkenden Lernens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Beispiel 3.** <a name=\"b3\"></a> Fortsetzung von Beispiel 2. \n",
    "\n",
    "Hier war $O = \\{O_1,O_2,O_3\\}$ gegeben durch:\n",
    "\n",
    "$$\n",
    "O1 = ((s_{1,1}^{1}, \\text{clean}, s_{0,1}^{1}, 10), (s_{0,1}^{1}, \\text{move}, s_{0,1}^{2}, -1), (s_{0,1}^{2}, \\text{clean}, s_{0,0}^{2}, 10), (s_{0,0}^{2}, \\text{move}, s_{0,0}^{1}, -1), (s_{0,0}^{1}, \\text{charge}, s_{t}, 0)) $$\n",
    "$$O2 = ((s_{1,1}^{1}, \\text{clean}, s_{1,1}^{1}, 0), (s_{1,1}^{1}, \\text{clean}, s_{0,1}^{1}, 10), (s_{0,1}^{1}, \\text{move}, s_{0,1}^{1}, -1), (s_{0,1}^{1}, \\text{move}, s_{0,1}^{2}, -1), (s_{0,1}^{2}, \\text{clean}, s_{0,0}^{2}, 10), (s_{0,0}^{2}, \\text{move}, s_{0,0}^{1}, -1), (s_{0,0}^{1}, \\text{charge}, s_{t}, 0)) $$\n",
    "$$O3 = ((s_{1,1}^{1}, \\text{clean}, s_{0,1}^{1}, 10), (s_{0,1}^{1}, \\text{move}, s_{0,1}^{1}, -1), (s_{0,1}^{1}, \\text{move}, s_{0,1}^{1}, -1), (s_{0,1}^{1}, \\text{move}, s_{0,1}^{2}, -1), (s_{0,1}^{2}, \\text{clean}, s_{0,0}^{2}, 10), (s_{0,0}^{2}, \\text{move}, s_{0,0}^{1}, -1), (s_{0,0}^{1}, \\text{charge}, s_{t}, 0))\n",
    "$$\n",
    "\n",
    "\n",
    "In den obigen Probelaufen gibt es insgesamt vier Beobachtungen, bei denen der Agent in Zustand $s_{1,1}^1$ die Aktion $\\pi_{vc} = \\text{clean}$ durchführt. Bei drei von diesen vier Beobachtungen (Gelb) landet der Agent in Zustand $s_{0,1}^1$ und bei einer Beobachtung wieder in Zustand $s_{1,1}^1$ (Grün). \n",
    "\n",
    "![beispiel 3](.\\b3.PNG)\n",
    "\n",
    "Wir erhalten also\n",
    "\n",
    "- $\\tilde{P}_{\\partial}(s_{1,1}^1,\\pi(s_{1,1}^1),s_{0,1}^1) = \\frac{3}{4}$, da der Agent in drei von vier Fällen in den Zustand $s_0^{1,1}$​ übergeht.\n",
    "\n",
    "- $\\tilde{P}_{\\partial}(s_{1,1}^1,\\pi(s_{1,1}^1),s_{1,1}^1) = \\frac{1}{4}$, da der Agent in einem von vier Fällen im Zustand $s_1^{1,1}$ bleibt.\n",
    "  \n",
    "- $\\tilde{P}_{\\partial}(s_{1,1}^1,\\pi(s_{1,1}^1),s') = 0$ für alle anderen Zustände s′, die nicht $s_0^{1,1}$​ oder $s_1^{1,1}$​ sind\n",
    "\n",
    "Weiterhin erhalten wir\n",
    "\n",
    "- $\\tilde{R}_{\\partial}(s_{1,1}^1,\\pi(s_{1,1}^1),s_{0,1}^1) = 10$: der Agent erhält eine Belohnung von 10, wenn er in den Zustand $s_0^{1,1}$​ übergeht.\n",
    "\n",
    "- $\\tilde{R}_{\\partial}(s_{1,1}^1,\\pi(s_{1,1}^1),s_{1,1}^1) = 0$: der Agent erhält eine Belohnung von 0, wenn er in den Zustand $s_1^{1,1}$​ bleibt.\n",
    "\n",
    "Die Berechnung der übrigen Werte für $\\tilde{P}_{\\partial}$ und $\\tilde{R}_{\\partial}$ geschieht analog. Wir schauen uns einen weiteren Beispiel an:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Angenommen, wir haben die folgenden Probelaufe:\n",
    "\n",
    "$$\n",
    "O_1 = ((s_{1,1}^{1}, \\text{clean}, s_{0,1}^{1}, 10), (s_{0,1}^{1}, \\text{move}, s_{0,1}^{2}, -1))\n",
    "$$\n",
    "$$\n",
    "O_2 = ((s_{1,1}^{1}, \\text{clean}, s_{1,1}^{1}, 0), (s_{1,1}^{1}, \\text{clean}, s_{0,1}^{1}, 10))\n",
    "$$\n",
    "\n",
    "Wir möchten die approximierten Übergangswahrscheinlichkeiten $\\tilde{P}_{\\partial}$ und unmittelbaren Belohnungen $\\tilde{R}_{\\partial}$ für den Zustand $s_{1,1}^{1}$ und die Aktion \"clean\" berechnen.\n",
    "\n",
    "1. **Berechnung von $\\tilde{P}_{\\partial}(s_{1,1}^{1}, \\text{clean}, s')$:**\n",
    "\n",
    "   Es gibt insgesamt drei Beobachtungen, bei denen der Agent im Zustand $s_{1,1}^{1}$ die Aktion \"clean\" durchführt. Bei zwei dieser drei Beobachtungen wechselt der Agent in den Zustand $s_{0,1}^{1}$ und bei einer Beobachtung bleibt er im Zustand $s_{1,1}^{1}$. Daher erhalten wir:\n",
    "\n",
    "   - $\\tilde{P}_{\\partial}(s_{1,1}^{1}, \\text{clean}, s_{0,1}^{1}) = \\frac{2}{3}$\n",
    "   - $\\tilde{P}_{\\partial}(s_{1,1}^{1}, \\text{clean}, s_{1,1}^{1}) = \\frac{1}{3}$\n",
    "\n",
    "2. **Berechnung von $\\tilde{R}_{\\partial}(s_{1,1}^{1}, \\text{clean}, s')$:**\n",
    "\n",
    "   Die unmittelbaren Belohnungen für die Übergänge zu den Zuständen $s_{0,1}^{1}$ und $s_{1,1}^{1}$ nach Durchführung der Aktion \"clean\" im Zustand $s_{1,1}^{1}$ sind 10 und 0. Daher erhalten wir:\n",
    "\n",
    "   - $\\tilde{R}_{\\partial}(s_{1,1}^{1}, \\text{clean}, s_{0,1}^{1}) = 10$\n",
    "   - $\\tilde{R}_{\\partial}(s_{1,1}^{1}, \\text{clean}, s_{1,1}^{1}) = 0$\n",
    "\n",
    "Die Berechnung der übrigen Werte für $\\tilde{P}_{\\partial}$ und $\\tilde{R}_{\\partial}$ würde auf ähnliche Weise durchgeführt werden, indem die Anzahl der Übergänge und die entsprechenden unmittelbaren Belohnungen für andere Zustände und Aktionen gezählt werden. Diese Werte würden dann verwendet, um die erwartete kumulative Belohnung zu berechnen, wie in der Bellman-Gleichung beschrieben."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.3. Temporal Difference Learning <a name=\"4_2_3\"></a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nachteil :**\n",
    "\n",
    "Der bisherigen Methoden zum passiven Reinforcement Learning besteht darin, dass zunächst die in den Probelaufen enthaltenen Informationen extrahiert und anschließend die Nutzen aller Zustände berechnet werden.\n",
    "Bei großen Zustandsräumen kann dieses Vorgehen zu sehr langen Laufzeiten führen und nicht mehr praktikabel sein.\n",
    "\n",
    "Für typische Anwendungsfälle ist es oft nicht notwendig, den Nutzen aller Zustände zu bestimmen, da viele Zustände bei Ausführung einer optimalen Strategie niemals erreicht werden.\n",
    "\n",
    ">**Definition:** *Temporal Difference Learning (TD)*\n",
    ">\n",
    ">ist eine allgemeine Methode beim Reinforcement Learning, die die Nutzen der besuchten Zustände während der Probelaufen in der Umgebung laufend aktualisiert.\n",
    ">\n",
    ">Viele in der Praxis gebräuchliche Algorithmen zum *Reinforcement Learning* basieren auf dieser Methode.\n",
    "\n",
    "Im Folgenden wird die einfachste Instanz von TD zur Bestimmung der Nutzenwerte $U^\\gamma_D(s | \\pi)$ bezüglich einer festen Strategie $\\pi$ betrachtet:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ausgehend von einer beliebigen Initialisierung der Nutzenwerte $u^{\\gamma}(s | \\pi)$ für alle $s \\in S \\setminus S^t$ (beispielsweise mit 0). Temporal Difference Learning (TD) passt den Nutzenwert $u^{\\gamma}(s | \\pi)$ für jeden Zustand $s$ bei jeder neuen Beobachtung $o = (s,\\pi(s),s',r)$ an, um ihn mit $o$ kompatibel zu machen.\n",
    "\n",
    "> **Beispiel:**\n",
    ">\n",
    "> Angenommen, wir haben bereits Nutzenwerte für zwei Zustände $s_1$ und $s_2$ approximiert, und wir beobachten $o = (s_1,\\pi(s_1),s_2,3)$.\n",
    ">\n",
    "> Wenn der generierende Markov-Entscheidungsprozess deterministisch wäre, müsste die Gleichung $$u^\\gamma(s_1 | \\pi) = 3 + \\gamma u^\\gamma(s_2 | \\pi)$$ gelten.\n",
    ">\n",
    "> Für $\\gamma = 0.9$ evaluiert die rechte Seite der obigen Gleichung zu $3 + 0.9 \\cdot 2 = 4.8$, ein Wert, der etwas kleiner als unsere aktuelle Schätzung $u^\\gamma(s_1 | \\pi) = 7$ ist. Die Beobachtung $o$ zeigt, dass unsere aktuelle Schätzung $u^\\gamma(s_1 | \\pi) = 7$ vermutlich zu hoch ist und etwas nach unten korrigiert werden sollte.\n",
    "\n",
    "Die konkrete Updateregel des TD bei einer neuen Beobachtung $o = (s,\\pi(s),s',r)$ ist gegeben durch $$u^\\gamma(s | \\pi) := u^\\gamma(s | \\pi) + \\alpha(r + \\gamma u^\\gamma(s' | \\pi) - u^\\gamma(s | \\pi))$$, wobei $\\alpha \\in [0,1]$ der Lernparameter ist. Dieser ist üblicherweise nicht konstant, sondern wird mit steigender Anzahl von Beobachtungen geringer.\n",
    "\n",
    "Diese Regel vergleicht den gerade beobachteten Nutzen $(r + \\gamma u^\\gamma(s' | \\pi))$ mit dem bisher geschätzten Nutzen $u^\\gamma(s | \\pi)$ und aktualisiert ihn entsprechend in die richtige Richtung. Außerdem bezieht diese Regel <ins>kein Modell der Umgebung</ins>, d.h., die Übergangswahrscheinlichkeiten zwischen den Zuständen, in die Berechnung ein.\n",
    "\n",
    "Bei vielen aufeinanderfolgenden Aktualisierungen von $u^\\gamma(s | \\pi)$ mittels der Regel kommen die Zielzustände $s'$ entsprechend ihrer Verteilung unterschiedlich oft vor und werden daher auch unterschiedlich stark einbezogen. Wenn dies der Fall ist, kann wieder gezeigt werden, dass die Werte $u^\\gamma(s | \\pi)$ gegen die tatsächlichen Werte $U^\\gamma_D(s | \\pi)$ des generierenden Markov-Entscheidungsprozesses konvergieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Beispiel 4.** <a name=\"b4\"></a> Fortsetzung von Beispiel 3. \n",
    "\n",
    "Initial setzen wir die geschätzten Nutzen aller Zustände auf 0:\n",
    "\n",
    "$$\n",
    "u^\\gamma(s_{1,1}^1 | \\pi_{vc}) = u^\\gamma(s_{1,1}^2 | \\pi_{vc}) = \\ldots = u^\\gamma(s_{0,0}^2 | \\pi_{vc}) = 0\n",
    "$$\n",
    "\n",
    "Betrachten wir nun die erste Beobachtung $(s_{1,1}^1, \\text{clean}, s_{0,1}^1, 10)$ (dies entspricht der ersten Beobachtung aus dem Probelauf $O_1$ aus Beispiel 1). Nach Gleichung (4) erhalten wir (für $\\gamma = 0.9$ und $\\alpha = 0.5$):\n",
    "\n",
    "$$\n",
    "u^\\gamma(s_{1,1}^1 | \\pi_{vc}) := u^\\gamma(s_{1,1}^1 | \\pi_{vc}) + \\alpha(10 + \\gamma u^\\gamma(s_{0,1}^1 | \\pi_{vc}) - u^\\gamma(s_{1,1}^1 | \\pi_{vc}))\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 0 + 0.5(10 + 0.9 \\cdot 0 - 0) = 5\n",
    "$$\n",
    "\n",
    "Der Wert $u^\\gamma(s_{1,1}^1 | \\pi_{vc})$ wird also von der ersten Schätzung 0 auf die neue Schätzung 5 aktualisiert.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.4. Unterschiede zwischen Adaptive dynamische Programmierung und Temporal Difference Learning (TD) <a name=\"4_2_4\"></a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Hier sind die Hauptunterschiede zwischen Temporal Difference Learning (TD) und Adaptive Dynamic Programming (ADP) in Stichpunkten:\n",
    "\n",
    "- **Modellwissen**: TD ist modellfrei und benötigt kein Wissen über ein Modell der Umgebung¹. ADP hingegen erfordert ein vollständiges und genaues Modell der Umgebung².\n",
    "- **Lernprozess**: TD aktualisiert die Nutzenwerte der besuchten Zustände während der Probelaufe in der Umgebung laufend¹. ADP löst das Problem durch Rekursion und benötigt alle vorherigen Zustände².\n",
    "- **Anwendungsbereich**: TD ist besonders nützlich in Situationen, in denen das Modell der Umgebung unbekannt oder zu komplex ist, um direkt damit zu arbeiten¹. ADP ist effektiv, wenn das Modell der Umgebung bekannt ist und die Zustands- und Aktionsräume nicht zu groß sind².\n",
    "\n",
    "Und hier sind die Unterschiede in einer Tabelle:\n",
    "\n",
    "|  | Temporal Difference Learning | Adaptive Dynamic Programming |\n",
    "|---|---|---|\n",
    "| Modellwissen | Modellfrei¹ | Benötigt ein vollständiges und genaues Modell der Umgebung² |\n",
    "| Lernprozess | Aktualisiert die Nutzenwerte der besuchten Zustände laufend¹ | Löst das Problem durch Rekursion und benötigt alle vorherigen Zustände² |\n",
    "| Anwendungsbereich | Nützlich, wenn das Modell der Umgebung unbekannt oder zu komplex ist¹ | Effektiv, wenn das Modell der Umgebung bekannt ist und die Zustands- und Aktionsräume nicht zu groß sind² |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"up\"></a>**`[Go Up!^](#up)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
