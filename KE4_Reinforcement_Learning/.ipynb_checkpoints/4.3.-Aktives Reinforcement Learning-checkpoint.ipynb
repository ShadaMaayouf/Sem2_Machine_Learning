{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T15:05:33.584082Z",
     "start_time": "2024-01-22T15:05:33.537800Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# 4.3. Aktives Reinforcement Learning\n",
    ">## <ins>Table of contents</ins> <a name=\"up\"></a>[<sup>[1]</sup>](#cite_note-1)\n",
    ">* [**4.3.1. Exploration vs. Exploitation**](#4_3_1)\n",
    ">* [**4.3.2. ε-greedy Learning**](#4_3_2)\n",
    ">* [**4.3.3. Q-Learning**](#4_3_3)\n",
    ">* [**4.3.4. Unterschiede zwischen ε-greedy Learning und Q-Learning**](#4_3_4)\n",
    ">\n",
    ">## <ins>Beispiele</ins>\n",
    ">* [**Beispiel 1**: Das k-armige Banditenproblem](#b1)\n",
    ">* [**Beispiel 2**: ε-greedy Learning des Staubsaugerproblems$](#b2)\n",
    ">* [**Beispiel 3**: Q-Learning des Staubsaugerproblems](#b3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Problemstellung**:\n",
    "\n",
    "Nachdem wir uns in den [Unterkapiteln 4.1](http://localhost:8888/lab/tree/KE4_Reinforcement_Learning/4.1.-Markov-Entscheidungsprozess.ipynb) und [4.2](http://localhost:8888/lab/tree/KE4_Reinforcement_Learning/4.2.-Passives-Reinforcement-Learning.ipynb) mit grundlegenden Fragestellungen und Teilproblemen des Reinforcement Learnings beschäftigt haben, kommen wir nun zu der eigentlichen Herausforderung: nämlich <ins>des Lernens der optimalen Strategie in einer nicht-deterministischen und unbekannten Umgebung</ins>.-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.1. Exploration vs. Exploitation <a name=\"4_3_1\"></a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für die optimale Strategie $\\pi^*$ gilt (siehe auch Gleichung (1) aus Unterkapitel 4.1):\n",
    "\n",
    "[$$\\pi^*(s) = \\arg\\max_{a \\in A} \\sum_{s' \\in S} P(s,a,s') \\left( R(s,a,s') + \\gamma U^\\gamma_D(s') \\right) \\tag{1}$$](http://localhost:8888/lab/tree/KE4_Reinforcement_Learning/4.1.-Markov-Entscheidungsprozess.ipynb)  <a name=\"g1\"></a>\n",
    "\n",
    "Mit anderen Worten:\n",
    "Wenn wir korrekte und vollständige Informationen zu den Nutzen aller Zustände ($U^γ_D$), zu den Belohnungen in den Zuständen ($R$) und den Zustandsübergangswahrscheinlichkeiten ($P$) haben, sollten wir in jedem Zustand die Aktion auswählen, die den erwarteten Nutzen maximiert.\n",
    "- In [Unterkapitel 4.2](http://localhost:8888/lab/tree/KE4_Reinforcement_Learning/4.2.-Passives-Reinforcement-Learning.ipynb) haben wir Methoden behandelt, die zu einer gegebenen Strategie $π$ die Nutzen der Zustände $U^γ_D(s | π)$ berechnen.\n",
    "- Außerdem wissen wir, dass für die optimale Strategie gilt $U^γ_D(s) = U^γ_D(s | π*)$.\n",
    "- Bei dem Ansatz der adaptiven dynamischen Programmierung (siehe Abschnitt 4.2.1) konnten wir auch die Werte $P(s,a,s′)$ und $R(s,a,s′)$ approximieren.\n",
    "\n",
    "Auf den ersten Blick scheint es, dass wir unter der Abschätzung von $U^γ_D(s)$ durch $U^γ_D(s | π)$ alle notwendigen Komponenten haben, um durch Gleichung (1) die optimale Strategie zu ermitteln. Allerdings gibt es hierbei zwei grundlegende Probleme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allerdings gibt es hierbei **zwei grundlegende Probleme**:\n",
    "\n",
    "1. Mit der adaptiven dynamischen Programmierung können wir nur die Werte $P(s,a,s′)$ und $R(s,a,s′)$ für $a = π(s)$ approximieren. Da wir eine feste Strategie angenommen haben, hat unser Agent keine Erfahrung, was andere Aktionen im Zustand $s$ bewirken.\n",
    "\n",
    "2. Eine Abschätzung von $U^γ_D(s)$ durch $U^γ_D(s | π)$ für eine nicht-optimale Strategie π kann sehr fehlgeleitet sein.\n",
    "\n",
    "   Stellen Sie sich einen Roboter vor, der den schnellsten Weg aus einem Hochhaus finden soll. Ist $\\pi$ beispielsweise die Strategie, die vorschreibt in das jeweilige nächsthöhere Stockwerk zu gehen und im obersten Stockwerk den Fahrstuhl nach unten zu nehmen und dann das Gebäude zu verlassen, so hat der Zustand, bei dem sich der Agent im Erdgeschoss befindet, bzgl. $\\pi$ einen sehr kleinen Nutzen (schließlich ist es für $\\pi$ noch ein weiter Weg bis zum Ausgang). Für eine optimale Strategie $\\pi^*$ hätte dieser Zustand allerdings einen sehr hohen Nutzen, da wir direkt zum Ausgang gehen könnten.\n",
    "\n",
    "\n",
    "- Ein wichtiger Aspekt des aktiven Reinforcement Learning ist die **Exploration**, bei der in einem Zustand verschiedene Aktionen ausprobiert werden, um zu lernen, welche Aktion tatsächlich optimal ist.\n",
    "- Sobald der Agent gelernt hat, welche Aktionen zu welchen Zuständen führen, kann dieses Wissen ausgenutzt werden (engl. \"**exploitation**\"), um gewinnbringend in der Umgebung zu agieren.\n",
    "\n",
    "**Das Dilemma von Exploration vs. Exploitation** im Reinforcement Learning beschreibt, wie diese beiden Aspekte gegeneinander abgewogen werden müssen. Es stellt die Frage, wann ein Agent entscheiden kann, dass er genug ausprobiert hat und die bisher beste Strategie tatsächlich optimal ist.\n",
    "- Eine anschauliche Darstellung dieses Dilemmas wird durch die Betrachtung des k-armigen Banditenproblems gegeben."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Beispiel 1.** <a name=\"b1\"></a> Das k-armige Banditenproblem\n",
    "\n",
    "Ein <ins>einarmiger</ins> Bandit ist ein Spielautomat, der nach Einwerfen einer Münze einen Gewinn ausgibt, der gleichverteilt im Intervall $[a,b]$ wobei $a<b$ liegt. Ein <ins>k-armiger</ins> Bandit hat mehrere \"Arme\", die jeweils unterschiedliche Gewinnintervalle haben. Man wählt nach Einwerfen der Münze einen von den k Armen.\n",
    "  \n",
    "<ins>Betrachten wir den Fall k = 10</ins>: \n",
    "- wir nehmen an, dass wir eine große Menge an Geld zur Verfügung haben, z.B. 1000 EUR, D.h. wir könnten die ersten 100 Versuche nutzen, um jeden Arm jeweils 10 Mal zu betätigen und somit plausible Schätzungen der jeweiligen Gewinnintervalle $[a_i,b_i]$ zu erhalten.\n",
    "- Anschließend könnten wir die verbleibenden 900 EUR nutzen, um ausschließlich den Arm zu betätigen, der am gewinnbringendsten erscheint (beispielsweise den Arm $i$, bei dem $a_i$ maximal ist).\n",
    "\n",
    "**Die allgemeine Frage** beim k-Banditenproblem ist, wie viel Geld wir investieren sollten, um ein möglichst genaues Modell vom Banditen zu erhalten (Exploration), und wie viel Geld wir investieren sollten, um das gelernte Wissen zur Gewinnmaximierung zu nutzen (Exploitation).\n",
    "\n",
    "So mopdelliert man das k-Banditenproblem:\n",
    "- Es gibt einen *Startzustand* und *k Aktionen*, die aus dem Startzustand gewählt werden können.\n",
    "- Nach Ausführen einer Aktion enden wir direkt in einem Zielzustand und erhalten den Gewinn.\n",
    "- Es gibt also auch nur k verschiedene Strategien für dieses Problem.\n",
    "\n",
    "In realen Anwendungsszenarien ist die Anzahl der möglichen Strategien allerdings zu groß und es ist nicht mäglich, zunächst alle mäglichen Strategien ausreichend auszuprobieren (wie im obigen Beispiel angedeutet) und dann die beste Strategie auszuwählen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.2. ε-greedy Learning <a name=\"4_3_2\"></a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Die meisten Methoden des aktiven Reinforcement Learning verwenden eine *Meta-Strategie*, um das Dilemma zwischen Exploration und Exploitation zu adressieren. Diese Algorithmen beginnen mit einer initialen Strategie π und nutzen diese zunächst, um die Nutzenwerte der Zustände zu erlernen, die mit dieser Strategie erreicht werden.\n",
    "- Je nach Meta-Strategie wird gelegentlich von der Strategie π abgewichen und in einem Zustand s eine andere Aktion a ≠ π(s) gewählt, um neue Teile des Zustandsraums zu erkunden und die Nutzenwerte der Zustände entsprechend zu aktualisieren.\n",
    "- Wenn sich in den Nutzenwerten abzeichnet, dass in einem Zustand s tatsächlich eine andere Aktion als π(s) besser ist, wird π entsprechend geändert.\n",
    "\n",
    "- Die einfachste Instanz einer solchen Meta-Strategie ist die **ε-greedy-Strategie**. Hierbei ist $ε \\in [0,1]$ ein Parameter, der angibt, wie häufig die Exploration der Exploitation vorgezogen wird, bzw. der die Wahrscheinlichkeit bestimmt, mit der eine zufällige Aktion (Exploration) anstelle der besten bekannten Aktion (Exploitation) gewählt wird.\n",
    "    - Wenn `ε = 0`, wird immer die beste bekannte Aktion (Exploitation) gewählt, es findet also keine Exploration statt.\n",
    "    - Wenn `ε = 1`, wird immer eine zufällige Aktion gewählt, es findet also nur Exploration statt.\n",
    "    - Mit Wahrscheinlichkeit `ε zwischen 0 und 1` wird in einem Zustand s eine Balance zwischen Exploration und Exploitation gefunden D.h. Es wird eine zufällige Aktion ausgewählt und mit Wahrscheinlichkeit 1 − ε wird π(s) ausgeführt. Beispielsweise bedeutet ε = 0.1, dass in 10% der Fälle eine zufällige Aktion (Exploration) und in 90% der Fälle die beste bekannte Aktion (Exploitation) gewählt wird."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durch die Nutzung einer solchen Meta-Strategie können beide oben genannten Probleme gelöst werden:\n",
    "\n",
    "1. Wenn wir hinreichend oft zufällige Aktionen in einem Zustand $s$ ausführen, können wir sowohl $P(s,a,s')$ als auch $R(s,a,s')$ für $a \\neq \\pi(s)$ approximieren.\r\n",
    "\r\n",
    "2. Wenn die Strategie $\\pi$ nicht optimal ist, können dennoch durch eine zufällig gute Auswahl von Aktionen die tatsächlichen Nutzen von Zuständen erkannt werden. Für das Beispiel des Hochhauses würden wir irgendwann zufällig im Erdgeschoss direkt die Aktion des Verlassens des Gebäudes ausführen und könnten damit direkt den Nutzen des Zustands im Erdgeschoss zu sein, erhöhen.Eine Kombination der **ε-greedy-Strategie mit der adaptiven dynamischen Programmierung** ([siehe Abschnitt 4.2.1](http://localhost:8888/lab/tree/KE4_Reinforcement_Learning/4.2.-Passives-Reinforcement-Learning.ipynb)) ist \n",
    "in Algorithmus 1 formalisiert.\r\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![e-greedy](./e-greedy.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Algorithmus implementiert einen einzigen Schritt eines Agenten in einer Umgebung und erhält als Eingabe die aktuelle Beobachtung $o$ (falls der Agent im Startzustand ist, setzen wir $o = \\text{null}$) und gibt die als nächstes auszuführende Aktion aus. Der Algorithmus besitzt eine Reihe von statischen Datenstrukturen, die über die verschiedenen Aufrufe des Algorithmus erhalten bleiben:\n",
    "\n",
    "- Die Strategie $\\pi$ ist die aktuell beste Strategie des Agenten.\n",
    "- Der MDP $D$ ist das Modell der Umgebung, das der Agent mit jedem Aufruf besser erlernt.\n",
    "- Die Funktion $u$ speichert die Nutzen der Zustände (bzgl. $\\pi$).\n",
    "- Die Funktionen $n_1$ und $n_2$ speichern die Häufigkeiten, wie oft in einem Zustand $s$ die Aktion $a$ ausgeführt wurde ($n_1(s,a)$) bzw. wie häufig beim Ausführen der Aktion $a$ in $s$ der Zustand $s'$ beobachtet wurde ($n_2(s,a,s')$).\n",
    "\n",
    "Der Algorithmus funktioniert wie folgt:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Wenn `o` nicht `null` ist, werden die Häufigkeiten `n1(s,a)` und `n2(s,a,s')` aktualisiert.\n",
    "2. Die Übergangswahrscheinlichkeiten `P(s,a,s'')` für alle `s''` in `S` werden aktualisiert.\n",
    "3. Die Belohnung `R(s,a,s')` wird auf `r` gesetzt.\n",
    "4. Die Nutzenfunktion `u` wird mittels der Funktion `VAL` aktualisiert (werden die Nutzen der Zustände bzgl. der aktuellen Strategie\r\n",
    "aktualisier - Value Iteration.).\n",
    "5. Die Strategie `π` wird mittels der Funktion `POL` aktualisiert (die Unterfunktion POL benutzt dazu direkt die Charakterisierung aus [Gleichung (1)](#g1)).\n",
    "6. Wenn `s'` ein Terminalzustand ist, gibt der Algorithmus `null` zurück In diesem Fall muss der Agent in \n",
    "der Umgebung \r\n",
    "neu gestartet“ werden, wobei die statischen Datenstrukturen nicht neu initialisiert werden..\n",
    "7. Mit Wahrscheinlichkeit `ε` gibt der Algorithmus eine zufällige Aktion `a` aus.\n",
    "8. Mit Wahrscheinlichkeit `1 - ε` führt der Algorithmus die Aktion `π(s')` aus (oder `π(s0)` falls `o` `null` ist\n",
    "Es kann wieder gezeigt werden, dass bei hinreichend vielen Aufrufen von Algorithmus 1 die Strategie π gegen \n",
    "die optimale Strategi  konvergiert).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Beispiel 2.** <a name=\"b2\"></a> ε-greedy Learning für das Staubsaugerproblem\n",
    "\n",
    "Wir betrachten wieder das Beispiel des Staubsaugerroboters aus den vorherigen Unterkapiteln. Insbesondere ist die Menge der Zustände $S_{vc}$ gegeben durch\n",
    "\n",
    "$$\n",
    "S_{vc} = \\{s_{1,1}^1, s_{1,1}^2, s_{0,1}^1, s_{0,1}^2, s_{1,0}^1, s_{1,0}^2, s_{0,0}^1, s_{0,0}^2, s_t\\}\n",
    "$$\n",
    "\n",
    "wobei $s_{1,1}^1$ der Startzustand und $s_t$ der einzige Zielzustand ist. Die Menge der Aktionen $A_{vc}$ ist gegeben durch\n",
    "\n",
    "$$\n",
    "A_{vc} = \\{\\text{move}, \\text{clean}, \\text{charge}\\}\n",
    "$$\n",
    "\n",
    "Nehmen wir weiterhin an, die initiale Strategie $\\pi$ des Roboters ist gegeben durch\n",
    "\n",
    "$$\\pi(s_{1,1}^1) = \\pi(s_{1,1}^2) = \\pi(s_{0,1}^1) = \\pi(s_{0,1}^2) = \\pi(s_{1,0}^1) = \\pi(s_{1,0}^2) = \\pi(s_{0,0}^1) = \\pi(s_{0,0}^2) = \\text{move}$$\n",
    "\n",
    "Wir führen Algorithmus 1 exemplarisch zweimal aus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Beim ersten Aufruf befindet sich der Agent im Startzustand $s_{1,1,1}$. Zeilen 1–5 werden hierbei nicht ausgeführt. Da noch keine Belohnungen beobachtet wurden, gilt initial $R(s,a,s') = 0$ für alle $s,s' \\in S$, $a \\in A$. Es folgt, dass in Zeile 6, $u(s) = 0$ für alle $s \\in S$ errechnet wird. In Zeile 7 ergibt sich, dass alle Strategien gleich gut/schlecht sind, wir ändern die aktuelle Strategie also nicht ab. Da wir uns nicht in einem Zielzustand befinden und wir annehmen, dass der Zufallstest in Zeilen 9/10 zugunsten von Zeile 10 ausgefallen ist, geben wir $\\pi(s_{1,1,1}) = \\text{move}$ zurück.\n",
    "\n",
    "2. Beim zweiten Aufruf erhält der Algorithmus die Beobachtung $o = (s_{1,1,1}, \\text{move}, s_{1,1,2}, -1)$. Mit anderen Worten, der Roboter ist erfolgreich in Raum r2 gewechselt. Wir aktualisieren in den Zeilen 2–5 die Werte unserer Datenstrukturen wie folgt:\n",
    "    * $n1(s_{1,1,1}, \\text{move}) := n1(s_{1,1,1}, \\text{move})+1 = 0+1 = 1$\n",
    "    * $n2(s_{1,1,1}, \\text{move}, s_{1,1,2}) := n2(s_{1,1,1}, \\text{move}, s_{1,1,2})+1 = 0+1 = 1$\n",
    "    * $P(s_{1,1,1}, \\text{move}, s_{1,1,2}) := 1$\n",
    "    * $P(s_{1,1,1}, \\text{move}, s'') := 0$ für alle $s'' \\neq s_{1,1,2}$\n",
    "    * $R(s_{1,1,1}, \\text{move}, s_{1,1,2}) := -1$\n",
    "In Zeile 6 ergibt sich $u(s) := 0$ für alle $s$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein Nachteil von Algorithmus 1 ist, dass in Zeile 6 die Nutzen aller Zustände (potenziell) neu berechnet werden und anschließend die Strategie aktualisiert wird. Dies ist begründet in der Nutzung der adaptiven dynamischen Programmierung, bei der wir die gleiche Kritik schon in Abschnitt 4.2.2 benutzt haben, um den Ansatz des Temporal Difference Learning (TD) zu motivieren. In gleicher Weise können wir allerdings auch den TD-Ansatz für das aktive Reinforcement Learning erweitern. Dies werden wir im nächsten Abschnitt tun.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.3. Q-Learning <a name=\"4_3_3\"></a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.4. Unterschiede zwischen Adaptive dynamische Programmierung und Temporal Difference Learning (TD) <a name=\"4_2_4\"></a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Definition 2. Multimenge der Probeläufen und der Nutzen $U^{\\gamma}_{\\partial}(s | \\pi)$**\n",
    ">\n",
    "> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Beispiel 1.** <a name=\"b1\"></a> Das k-armige Banditenproblem\n",
    "\n",
    "Ein"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"up\"></a>**`[Go Up!^](#up)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
