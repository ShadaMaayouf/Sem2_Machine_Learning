{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T15:05:33.584082Z",
     "start_time": "2024-01-22T15:05:33.537800Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# 4.1. Markov-Entscheidungsprozesse\n",
    "\n",
    ">## <ins>Table of contents</ins>\n",
    ">* [**4.1.1. Modellierung einer Umgebung**](#4_1_1)\n",
    ">    * [**Das Markov-Entscheidungsprozess**](#4_1_1_a)\n",
    ">    * [**Strategie**](#4_1_1_b)\n",
    ">* [**4.1.2. Iterative Entwicklung der Zustandsnutzen**](#4_1_2)\n",
    ">* [**4.1.3. Iterative Strategieentwicklung**](#4_1_3)\n",
    ">* [**4.1.4. Glossar**](#4_1_4)\n",
    ">\n",
    ">## <ins>Beispiele</ins>\n",
    ">* [**Beispiel 1**: Staubsauger](#b1)\n",
    ">* [**Beispiel 2**: Episoden $\\pi$ des Beispiels *Staubsauger*](#b2)\n",
    ">* [**Beispiel 3**: Nutzen einer Episode $e$ *Staubsauger*](#b3)\n",
    ">* [**Beispiel 4**: Nutzen einer Strategie $\\pi$ von *Staubsauger*](#b4)\n",
    ">* [**Beispiel 5.**: Nutzen eines Zustands $s$ von *Staubsauger*](#b5)\n",
    ">* [**Beispiel 6.**: Value Iteration des Zustands $s_1^{1,1}$ *Staubsauger*](#b6)\n",
    ">* [**Beispiel 7.**: Policy Iteration des Zustands $s_1^{1,1}$ *Staubsauger*](#b7)\n",
    "\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-09T10:34:19.676666Z",
     "end_time": "2024-02-09T10:34:23.200352Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damit ein Agent lernt Entscheidungen in einer gewissen Umgebung zu treffen, muss diese Umgebung erst modelliert werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.1. Modellierung einer Umgebung <a name=\"4_1_1\"></a>\n",
    "\n",
    "### Der Markov-Entscheidungsprozess <a name=\"4_1_1_a\"></a>\n",
    "Ein MDP ist ein mathematisches Modell, das die Umgebung eines lernenden Agenten beschreibt. Es besteht aus einem <ins>Zustandsraum</ins>, einem <ins>Aktionsraum</ins>, einer <ins>Übergangswahrscheinlichkeitsfunktion</ins>, einer <ins>Belohnungsfunktion</ins>, einem <ins>Startzustand</ins> und einer <ins>Menge von Endzuständen</ins>.\n",
    "\n",
    "\n",
    "> **Definition 1.: Markov-Entscheidungsprozess**\n",
    ">\n",
    "> Ein Markov-Entscheidungsprozess (engl. Markov decision process, MDP) D ist ein Tupel $D = (S,A,P,R,s_0,S_t)$ mit folgenden Eigenschaften:\n",
    "> 1. $S$ ist eine Menge von Zuständen (der Zustandsraum)\n",
    "> 2. $A$ ist eine Menge von Aktionen (der Aktionsraum)\n",
    "> 3. $P : (S \\setminus S_t) \\times A \\times S \\rightarrow [0,1]$ ist eine Funktion (die Übergangswahrscheinlichkeitsfunktion) mit $\\sum_{s' \\in S} P(s,a,s') = 1$ für alle $s \\in S$, $a \\in A$.\n",
    "> 4. $R : (S \\setminus S_t) \\times A \\times S \\rightarrow \\mathbb{R}$ ist eine beliebige reellwertige Funktion (die Belohnungsfunktion, engl. reward function).\n",
    "> 5. $s_0 \\in S$ ist der Startzustand.\n",
    "> 6. $S_t \\subseteq S$ ist die Menge der Zielzustände.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In jedem *Zustand* $s \\in S$ kann der Agent eine *Aktion* $a \\in A$ ausführen, die zu einem neuen Zustand führt. Die *Übergangswahrscheinlichkeitsfunktion* $P$ gibt die Wahrscheinlichkeit an, dass eine bestimmte Aktion in einem bestimmten Zustand zu einem bestimmten neuen Zustand führt. Die *Belohnungsfunktion* $R$ gibt an, welche Belohnung (oder Strafe, wenn negativ) der Agent erhält, wenn er eine bestimmte Aktion in einem bestimmten Zustand ausführt und zu einem bestimmten neuen Zustand wechselt. \n",
    "\n",
    "Das **Ziel** des Agenten im Reinforcement Learning ist es, eine Strategie zu lernen, die die kumulative Belohnung über die Zeit <ins>maximiert</ins>. Dies wird oft durch eine Kombination aus **Exploration** (neue Aktionen ausprobieren, um mehr über die Umgebung zu erfahren) und **Exploitation** (die besten bekannten Aktionen ausführen) erreicht. \n",
    "\n",
    "Es ist wichtig zu beachten, dass MDPs die Markov-Eigenschaft haben, d.h., die Wahrscheinlichkeit, einen bestimmten neuen Zustand zu erreichen, hängt nur vom aktuellen Zustand und der ausgeführten Aktion ab, nicht von der vorherigen Geschichte des Agenten. Dies vereinfacht das Problem erheblich, da der Agent nicht die gesamte Geschichte seiner Interaktionen mit der Umgebung speichern muss. \n",
    "\n",
    ">\n",
    "> #### **Beipiel:** Staupsauger\n",
    ">\n",
    "> 1. **Zusatndsmenge $S$**: Jeder Zustand könnte eine Kombination aus der aktuellen Position des Roboters (z.B. die verschiedenen Räume der Wohnung) und dem Ladestand der Batterie sein.\n",
    ">    Wenn es beispielsweise vier Räume gibt, in denen der Roboter sich aufhalten kann, und die Batterie drei verschiedene Ladestufen hat (niedrig, mittel, hoch), dann gibt es insgesamt 12 mögliche Zustände.\n",
    "> \n",
    "> 2. **Aktionsmenge $A$**: ”saugen“, ”laden“ oder sich in einen Nachbarraum bewegen.\n",
    ">    In allgemeinen MDPs ist das Resultat einer Aktion nicht immer deterministisch bestimmt (beispielsweise kann das Laden des Roboters fehlschlagen, wenn die Batterie eine Fehlfunktion hat).\n",
    ">    \n",
    "> 3. **Übergangswahrscheinlichkeitsfunktion $P$**:  Diese Funktion könnte davon abhängen, wie der Roboter sich bewegt und wie die Batterie entladen wird. Zum Beispiel könnte die Wahrscheinlichkeit, dass der Roboter sich erfolgreich zu einem anderen Raum bewegt, von der aktuellen Ladung der Batterie abhängen. \n",
    ">\n",
    "> 4. **Belohnungsfunktion $R$**: Die Belohnung könnte positiv sein, wenn der Roboter einen Raum saugt, und negativ sein, wenn der Roboter versucht, sich zu bewegen, aber die Batterie leer ist. Die genaue Definition der Belohnungsfunktion hängt davon ab, was wir den Roboter optimieren wollen. Wenn wir wollen, dass der Roboter so viel wie möglich saugt, könnten wir eine hohe positive Belohnung für das Saugen geben. Wenn wir wollen, dass der Roboter seine Batterie effizient nutzt, könnten wir eine negative Belohnung für das Bewegen geben, wenn die Batterie fast leer ist.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Beispiel 1.** <a name=\"b1\"></a> Ein stark vereinfachtes Modell des Staubsaugerroboterproblems\n",
    "\n",
    "Wir gehen davon aus, dass unsere Wohnung aus zwei Räumen, r1 und r2, besteht, die entweder sauber oder verschmutzt sein können. Der Zustandsraum ist dann definiert durch:\n",
    "\n",
    "$$S^{vc} = \\{s^{1,1}_1, s^{1,1}_2, s^{0,1}_1, s^{0,1}_2, s^{1,0}_1, s^{1,0}_2, s^{0,0}_1, s^{0,0}_2, s^t\\}$$\n",
    "\n",
    "wobei der Zustand $s^{k,l}_i$ repräsentiert, dass der Roboter in Raum $r_i$ ist:\n",
    "- Raum $r_1$ bei $k = 0$ sauber und bei $k = 1$ verschmutzt ist\n",
    "- und Raum $r_2$ bei $l = 0$ sauber und bei $l = 1$ verschmutzt ist.\n",
    "\n",
    "| Zustand | Raum $r_i$ | Raum $r_1$ | Raum $r_2$ |\n",
    "|---------|------------|------------|------------|\n",
    "| $s^{0,0}_1$ | Roboter in Raum $r_1$ | Sauber | Sauber |\n",
    "| $s^{1,0}_1$ | Roboter in Raum $r_1$ | Verschmutzt | Sauber |\n",
    "| $s^{0,1}_1$ | Roboter in Raum $r_1$ | Sauber | Verschmutzt |\n",
    "| $s^{1,1}_1$ | Roboter in Raum $r_1$ | Verschmutzt | Verschmutzt |\n",
    "| $s^{0,0}_2$ | Roboter in Raum $r_2$ | Sauber | Sauber |\n",
    "| $s^{1,0}_2$ | Roboter in Raum $r_2$ | Verschmutzt | Sauber |\n",
    "| $s^{0,1}_2$ | Roboter in Raum $r_2$ | Sauber | Verschmutzt |\n",
    "| $s^{1,1}_2$ | Roboter in Raum $r_2$ | Verschmutzt | Verschmutzt |\n",
    "\n",
    "\n",
    "$s^t$ ist der Zielzustand und $s^{1,1}_1$ ist der Startzustand. \n",
    "\n",
    "In jedem Zustand (außer $s^t$) hat der Roboter drei mögliche Aktionen:\n",
    "\n",
    "1. move: Gehe in den anderen Raum.\n",
    "2. clean: Reinige den aktuellen Raum.\n",
    "3. charge: Lade die Batterie.\n",
    "\n",
    "Also, $A_{vc} = \\{move, clean, charge\\}$. \n",
    "\n",
    "Die Übergangswahrscheinlichkeiten (modelliert durch $P_{vc}$) sind so definiert, dass sich in Raum $r_1$ eine Ladestation befindet und nach Ausführung der Aktion charge in $r_1$ der aktuelle \"Tag\" endet und wir zum finalen Zustand $s^t$ wechseln. \n",
    "\n",
    "Die Aktion move ist zu 90% erfolgreich (d.h., mit Wahrscheinlichkeit 0.1 bleibt der Roboter im aktuellen Raum) und die Aktion clean ist zu 80% erfolgreich (d.h., ein schmutziger Raum bleibt mit Wahrscheinlichkeit 0.2 schmutzig; ein sauberer Raum bleibt sauber). \n",
    "\n",
    "Die Belohnungen ($R_{vc}$) sind wie folgt definiert:\n",
    "\n",
    "1. Wenn der Roboter erfolgreich einen zuvor schmutzigen Raum reinigt, erhält er 10 Punkte.\n",
    "2. Wenn der Roboter einen sauberen Raum reinigt, erhält er -2 Punkte (da er unnötig Energie verbraucht hat).\n",
    "3. Wenn der Roboter in Raum $r_2$ zu laden versucht, erhält er -5 Punkte (er hat bei dem Versuch seine Batterie beschädigt).\n",
    "4. Wenn der Roboter in $r_1$ auflädt, ohne vorher alle Räume gereinigt zu haben, erhält er -7 Punkte.\n",
    "5. Für jede Bewegung mittels move (unabhängig davon, ob erfolgreich oder nicht) erhält der Roboter -1 Punkt (der Roboter soll möglichst effizient die Wohnung reinigen).\n",
    "\n",
    "Für alle weiteren Situationen beträgt die Belohnung 0 Punkte.\n",
    "\n",
    "Wir erhalten: $$ D_{vc} = (S_{vc}, A_{vc}, P_{vc}, R_{vc}, s^{1,1}_1, S_{vc}^t = \\{s^t\\})$$\n",
    "\n",
    "![markov_staubsauger_beispiel](./markov_staubsauger-beispiel.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Die konstante und optimale Strategie <a name=\"4_1_1_b\"></a>\n",
    "\n",
    "Um den Agenten in einem MDP zu steuern, besitzt dieser eine *Strategie* (engl. policy), die (im einfachsten Fall) für jeden Zustand die auszuwählende Aktion bestimmt.\n",
    "\n",
    ">**Definition 2.:** Strategie\n",
    ">\n",
    ">Sei $D = (S,A,P,R,s_0,S_t)$ ein MDP. Eine konstante Strategie $\\pi$ für $D$ ist eine Funktion $$\\pi : S \\setminus S_t \\rightarrow A$$\n",
    "\n",
    "\n",
    "Eine **konstante Strategie** ist eine Funktion, die jedem Zustand eine bestimmte Aktion zuweist. Sie ist deterministisch, d.h., sie gibt für jeden Zustand genau eine Aktion vor.\n",
    "\n",
    "Es gibt jedoch auch **probabilistische Strategien**, die jedem Zustand eine Wahrscheinlichkeitsverteilung über die möglichen Aktionen zuweisen. Diese Art von Strategie kann nützlich sein, wenn es Unsicherheit oder Zufälligkeit in der Umgebung gibt. \n",
    "\n",
    "In vielen Fällen, insbesondere bei komplexen MDPs, kann es vorteilhaft sein, probabilistische Strategien zu verwenden, um eine bessere Leistung zu erzielen. Aber für den Anfang ist es absolut sinnvoll, sich auf konstante Strategien zu konzentrieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Beispiel 2** <a name=\"b2\"></a> Fortsetzung von Beispiel 1\n",
    "\n",
    "Eine mögliche Strategie $π_{vc}$ ist wie folgt definiert:\n",
    "\n",
    "$$\\pi_{vc}(s^{1,1}_{1}) = \\pi_{vc}(s^{1,0}_{1}) = \\pi_{vc}(s^{1,1}_{2}) = \\pi_{vc}(s^{0,1}_{2}) = \\text{clean}$$\n",
    "$$\\pi_{vc}(s^{0,1}_{1}) = \\pi_{vc}(s^{1,0}_{2}) = \\pi_{vc}(s^{0,0}_{2}) = \\text{move}$$\n",
    "$$\\pi_{vc}(s^{0,0}_{1}) = \\text{charge}$$\n",
    "\n",
    "Das bedeutet, wenn der Roboter sich in einem verschmutzten Raum befindet, führt er die Aktion `clean` aus. Wenn der aktuelle Raum sauber ist und der andere Raum verschmutzt ist, wechselt der Roboter in den anderen Raum (`move`). Wenn beide Räume sauber sind, wechselt der Roboter zuerst in Raum `r1` und lädt dann auf (`charge`).\n",
    "\n",
    "Die Aufgabe eines Offline-Lernverfahrens besteht darin, <ins>eine optimale Strategie `π*` für ein gegebenes festes MDP zu erlernen</ins>. \n",
    "\n",
    "Eine **optimale Strategie** ist diejenige, die die akkumulierte erwartete Belohnung maximiert, die der Agent durch die Anwendung der Strategie erhält. \n",
    "\n",
    "Um dies zu definieren, benötigen wir einige zusätzliche Konzepte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Definition 3.:** Episode\n",
    ">\n",
    ">Sei $D = (S,A,P,R,s_0,S_t)$ ein MDP. Eine Episode `e` in $D$ ist eine (potenziell unendliche) Folge $e = (s_0, a_1, s_1, a_2, s_2, ...)$ mit $s_0, s_1 ... \\in S$ und $a_0, a_1 ... \\in A$ Die Wahrscheinlichkeit $P(e)$ ist definiert durch\n",
    ">$$P(e) = \\prod_{i>0} P(s_{i-1},a_i,s_i)$$\n",
    "\n",
    "Eine **Episode** `e` in einem MDP ist eine (potenziell unendliche) Folge von Zuständen und Aktionen, die mit dem Anfangszustand `s0` beginnt und die Aktionen `a1, a2, ...` ausführt, wobei der Agent in den Zuständen `s1, s2, ...` landet. \n",
    "\n",
    "Eine Episode ist **initial**, wenn sie mit dem Anfangszustand $s_0$ beginnt bzw. wenn $s_0 = s^0$ gilt, und sie ist **terminierend**, wenn sie mit einem Zustand $s_n$ endet, der zu den Endzuständen $S^t$ gehört d.h. $s_n \\in S^t$ .\n",
    "\n",
    "> Der unendliche Fall tritt beispielsweise für den Roboter aus unserem Staubsaugerbeispiel auf, wenn dieser permanent nur die move-Aktion ausführt oder wenn eine move-Aktion permanent fehlschlägt (wobei für solch eine Episode die Wahrscheinlichkeit gegen 0 geht).\n",
    "\n",
    "Jede Episode akkumuliert gewisse Belohnungen und die Summe aller Belohnungen nennen wir **Nutzen** der Episode (engl. utility oder auch return)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Definition 4. Nutzen einer Episode γ**\n",
    ">\n",
    ">Sei $D = (S,A,P,R,s_0,St)$ ein MDP und $e = (s_0,a_1,s_1,a_2,s_2,...)$ eine Episode in D. Für $\\gamma \\in [0,1]$ heißt $U^\\gamma_D(e)$ definiert via\n",
    ">$$U^\\gamma_D(e) = \\sum_{i>0} \\gamma^{i-1}R(s_{i-1},a_i,s_i)$$\n",
    ">der mit $\\gamma$ diskontierte Nutzen von $e$ in $D$.\n",
    "\n",
    "Der **Nutzen** einer Episode ist die Summe aller akkumulierten Belohnungen, wobei jede Belohnung mit einem **Discountfaktor** `γ` gewichtet wird. Dieser Diskontfaktor liegt im Bereich von 0 bis 1 und bestimmt, wie stark zukünftige Belohnungen im Vergleich zu sofortigen Belohnungen bewertet werden. \n",
    "- Bei kleinen Werten von `γ` werden Strategien bevorzugt, die schnell hohe Belohnungen erhalten,\n",
    "- während bei größeren Werten nahe 1 spätere Belohnungen wichtiger werden.\n",
    "\n",
    "> Üblicherweise wählt man für `γ` einen Wert echt kleiner als 1 (< 1), der aber immer noch recht nahe an 1 ist (beispielsweise 0.9 oder 0.99). Für Werte echt kleiner als 1 ist es auch gewährleistet, dass $U^γ_D(e)$ stets endlich ist, selbst bei unendlichen langen Episoden mit positiver Wahrscheinlichkeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Beispiel 3** <a name=\"b3\"></a> Fortsetzung von Beispiel 2\n",
    "\n",
    "Betrachten wir die folgende (initiale und terminierende) Episode $e_1$, definiert durch\n",
    "\n",
    "$$e_1 = (s_{1,1}^1, \\text{clean}, s_{0,1}^1, \\text{move}, s_{0,1}^2, \\text{clean}, s_{0,0}^2, \\text{move}, s_{0,0}^1, \\text{charge}, s_t)$$\n",
    "\n",
    "Der Roboter reinigt also zunächst Raum $r_1$, wechselt dann in Raum $r_2$, reinigt diesen, wechselt wieder in Raum $r_1$ und lädt sich dann auf. Alle Aktionen sind hier also erfolgreich. \n",
    "\n",
    "Von Beispiel 1 haben wir bereits einige der Übergangswahrscheinlichkeiten definiert:\n",
    "- Die Aktion `move` ist zu `90% erfolgreich`, d.h., mit Wahrscheinlichkeit 0.1 bleibt der Roboter im aktuellen Raum.\n",
    "- Die Aktion `clean` ist zu `80% erfolgreich`, d.h., ein schmutziger Raum bleibt mit Wahrscheinlichkeit 0.2 schmutzig; ein sauberer Raum bleibt sauber.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Wahrscheinlichkeit einer Episode $P(e_1)$ kann berechnet werden, indem die Wahrscheinlichkeiten aller Zustandsübergänge in der Episode multipliziert werden. In $e_1$ sind alle Aktionen erfolgreich, daher können wir die Wahrscheinlichkeiten direkt aus den gegebenen Übergangswahrscheinlichkeiten entnehmen.\n",
    "\n",
    "Die Episode $e_1$ besteht aus den folgenden Zustandsübergängen:\n",
    "\n",
    "1. $s_{1,1}^1$ (clean) $\\rightarrow$ $s_{0,1}^1$: Der Roboter reinigt Raum $r_1$. Die Wahrscheinlichkeit für diesen Übergang ist 0.8 (da die Aktion `clean` zu 80% erfolgreich ist).\n",
    "2. $s_{0,1}^1$ (move) $\\rightarrow$ $s_{0,1}^2$: Der Roboter wechselt in Raum $r_2$. Die Wahrscheinlichkeit für diesen Übergang ist 0.9 (da die Aktion `move` zu 90% erfolgreich ist).\n",
    "3. $s_{0,1}^2$ (clean) $\\rightarrow$ $s_{0,0}^2$: Der Roboter reinigt Raum $r_2$. Die Wahrscheinlichkeit für diesen Übergang ist 0.8.\n",
    "4. $s_{0,0}^2$ (move) $\\rightarrow$ $s_{0,0}^1$: Der Roboter wechselt zurück in Raum $r_1$. Die Wahrscheinlichkeit für diesen Übergang ist 0.9.\n",
    "5. $s_{0,0}^1$ (charge) $\\rightarrow$ $s_t$: Der Roboter lädt sich auf und erreicht den Zielzustand. Da sich die Ladestation in Raum $r_1$ befindet, ist dieser Übergang immer erfolgreich, d.h., die Wahrscheinlichkeit ist 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir erhalten als Wahrscheinlichkeit $P_{e_1}$ hier\n",
    "\n",
    "$$P(e_1) = P(s_{1,1}^1, \\text{clean}, s_{0,1}^1)P(s_{0,1}^1, \\text{move}, s_{0,1}^2)P(s_{0,1}^2, \\text{clean}, s_{0,0}^2)P(s_{0,0}^2, \\text{move}, s_{0,0}^1)P(s_{0,0}^1, \\text{charge}, s_t)$$\n",
    "$$= 0.8 \\cdot 0.9 \\cdot 0.8 \\cdot 0.9 \\cdot 1.0 = 0.5184$$\n",
    "\n",
    "und mit $\\gamma = 0.9$ als Nutzen\n",
    "\n",
    "$$U^\\gamma_D(e_1) = R(s_{1,1}^1, \\text{clean}, s_{0,1}^1)+\\gamma R(s_{0,1}^1, \\text{move}, s_{0,1}^2)+\\gamma^2P(s_{0,1}^2, \\text{clean}, s_{0,0}^2)+\\gamma^3R(s_{0,0}^2, \\text{move}, s_{0,0}^1)+\\gamma^4R(s_{0,0}^1, \\text{charge}, s_t)$$\n",
    "$$= 10+0.9 \\cdot(-1)+0.9^2 \\cdot 10+0.9^3 \\cdot(-1)+0.9^4 \\cdot 0 = 10-0.9+8.1-0.729 = 16.471$$\n",
    "\n",
    "Schauen wir uns eine weitere Episode $e_2$ mit\n",
    "\n",
    "$$e_2 = (s_{1,1}^1, \\text{clean}, s_{1,1}^1, \\text{clean}, s_{0,1}^1, \\text{charge}, s_t)$$\n",
    "\n",
    "an. Hier versucht der Roboter zunächst vergeblich, Raum $r_1$ zu reinigen. Nachdem es beim zweiten Versuch geklappt hat, geht er direkt an die Ladestation. Wir erhalten\n",
    "\n",
    "$$P(e_2) = P(s_{1,1}^1, \\text{clean}, s_{1,1}^1)P(s_{1,1}^1, \\text{clean}, s_{0,1}^1)P(s_{0,1}^1, \\text{charge}, s_t) = 0.2 \\cdot 0.8 \\cdot 1 = 0.16$$\n",
    "\n",
    "und\n",
    "\n",
    "$$U^\\gamma_D(e_2) = R(s_{1,1}^1, \\text{clean}, s_{1,1}^1)+\\gamma R(s_{1,1}^1, \\text{clean}, s_{0,1}^1)+\\gamma^2R(s_{0,1}^1, \\text{charge}, s_t) = 0+0.9 \\cdot 10+0.9^2(-7) = 9-5.67 = 3.33$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun können wir den Nutzen einer Strategie $\\pi$ als den erwarteten Nutzen aller durch $\\pi$ generierten Episoden definieren:\n",
    "\n",
    "Eine Episode $e = (s_0,a_1,s_1,a_2,s_2,...)$ wird aus $\\pi$ generiert, geschrieben $\\pi \\sim e$, wenn $\\pi(s_{i-1}) = a_i$ für alle $i$ gilt. Das heißt, für jeden Zustand $s_{i-1}$ in der Episode wählt die Strategie π die Aktion $a_i$​ aus, die als nächstes ausgeführt wird. Ist $e$ zusätzlich initial (d.h., sie beginnt mit dem initialen Zustand des Problems), so schreiben wir $\\pi \\sim_0 e$._0 e$.\n",
    "\n",
    "Der **Nutzen** einer Strategie $\\pi$ ist der erwartete Nutzen aller durch $\\pi$ generierten Episoden. Das heißt, wir betrachten alle möglichen Episoden, die durch die Strategie $\\pi$ generiert werden könnten, berechnen den Nutzen jeder Episode (z.B. durch Summieren der Belohnungen in der Episode), und nehmen dann den Durchschnitt dieser Nut, wobei wir jede Episode mit ihrer Wahrscheinlichkeit gewichten.enwerte. Dies gibt uns eine Maßzahl dafür, wie gut die Strategie $\\pi$ im Durchschngen haben."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Definition 5. Nutzen von $\\pi$ in $D$**\n",
    ">\n",
    "> Sei $D = (S,A,P,R,s_0,S_t)$ ein Markov-Entscheidungsprozess (MDP), $\\gamma \\in [0,1]$ und $\\pi : S \\setminus S_t \\rightarrow A$ eine Strategie für $D$. Der Nutzen $U^\\gamma_D(\\pi)$ von $\\pi$ in $D$ ist definiert durch\n",
    "> \n",
    "$$U^\\gamma_D(\\pi) = E_{\\pi \\sim_0 e} \\left[ U^\\gamma_D(e) \\right] = \\sum_{\\pi \\sim_0 e} P(e)U^\\gamma_D(e)\n",
    " $$\n",
    "\n",
    "Mit anderen Worten, der Nutzen $U^\\gamma_D(\\pi)$ von $\\pi$ in $D$ ist der durchschnittliche Nutzen aller aus $\\pi$ generierten initialen Episoden, gewichtet nach deren Wahrscheinlichkeit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Beispiel 4.**<a name=\"b4\"></a>  Fortsetzung Beispiel 3\n",
    "\n",
    "Wir führen Beispiel 3 fort und betrachten die Strategie $\\pi_{vc}$ aus Beispiel 2. \n",
    "\n",
    "Beachten Sie, dass jede von $\\pi_{vc}$ generierte initiale Episode die folgende Struktur $e_{n1,n2,n3,n4}$ für alle $n1,n2,n3,n4 \\in \\mathbb{N}^+$ hat:\n",
    "\n",
    "$$\n",
    "e_{n1,n2,n3,n4} = (\\underbrace{s^{1,1}_1, \\text{clean},\\ldots,s^{1,1}_1, \\text{clean}}_{n_1 \\text{mal}},\\underbrace{s^{0,1}_1, \\text{move},\\ldots,s^{0,1}_1, \\text{move}}_{n_2-\\text{mal}},\\underbrace{s^{0,1}_2, \\text{clean},\\ldots,s^{0,1}_2, \\text{clean}}_{n_3-\\text{mal}},\\underbrace{s^{0,0}_2, \\text{move},\\ldots,s^{0,0}_2, \\text{move}}_{n_4-\\text{mal}},s^{0,0}_1, \\text{charge},s_t)\n",
    "$$\n",
    "- Der Roboter beginnt im Zustand $s^{1,1}_1$ und führt die Aktion `clean` $n_1$-mal aus. Dies wird durch den Ausdruck $\\underbrace{s^{1,1}_1, \\text{clean},\\ldots,s^{1,1}_1, \\text{clean}}_{n_1 \\text{mal}}$ dargestellt.\n",
    "- Dann führt der Roboter die Aktion `move` $n_2$-mal aus, während er sich im Zustand $s^{0,1}_1$ befindet. Dies wird durch den Ausdruck $\\underbrace{s^{0,1}_1, \\text{move},\\ldots,s^{0,1}_1, \\text{move}}_{n_2-\\text{mal}}$ dargestellt.\n",
    "- Als nächstes führt der Roboter die Aktion `clean` $n_3$-mal aus, während er sich im Zustand $s^{0,1}_2$ befindet. Dies wird durch den Ausdruck $\\underbrace{s^{0,1}_2, \\text{clean},\\ldots,s^{0,1}_2, \\text{clean}}_{n_3-\\text{mal}}$ dargestellt.\n",
    "- Schließlich führt der Roboter die Aktion `move` $n_4$-mal aus, während er sich im Zustand $s^{0,0}_2$ befindet. Dies wird durch den Ausdruck $\\underbrace{s^{0,0}_2, \\text{move},\\ldots,s^{0,0}_2, \\text{move}}_{n_4-\\text{mal}}$ dargestellt.\n",
    "- Der Roboter endet die Episode, indem er sich im Zustand $s^{0,0}_1$ auflädt und dann in den Endzustand $s_t$ übergeht.\n",
    "\n",
    "Die Zahlen $n_1$, $n_2$, $n_3$ und $n_4$ repräsentieren die Anzahl der Male, die jede Aktion fehlschlägt, bevor sie erfolgreich ist. Daher unterscheiden sich die einzelnen Episoden darin, wie oft das erste `clean`, das erste `move`, das zweite `clean` und das zweite `move` fehlschlagen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Wahrscheinlichkeit, dass eine Aktion erfolgreich ist, wird mit der Wahrscheinlichkeit eines Fehlschlags multipliziert, um die Gesamtwahrscheinlichkeit für eine Sequenz von Aktionen zu berechnen. In unserem Modell funktioniert der `charge`-Übergang immer, wenn der Roboter sich im Raum r1​ befindet, da dort die Ladestation steht. Daher ist die Wahrscheinlichkeit, dass der charge-Übergang erfolgreich ist, `1`.\n",
    "\n",
    "Es gilt\n",
    "$$\n",
    "P(e_{n1,n2,n3,n4}) = 0.2^{n1-1} \\cdot 0.8 \\cdot 0.1^{n2-1} \\cdot 0.9 \\cdot 0.2^{n3-1} \\cdot 0.8 \\cdot 0.1^{n4-1} \\cdot 0.9 \\cdot 1.0 $$\n",
    "$$= 0.5184 \\cdot 0.2^{n1+n3-2} \\cdot 0.1^{n2+n4-2}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "und weiterhin\n",
    "\n",
    "$$\n",
    "U^\\gamma_D(e_{n1,n2,n3,n4}) = \\gamma^{n1-1}10+\\gamma^{n1+n2+n3-1}10-\\sum_{i=n1}^{n1+n2-1}\\gamma^i-\\sum_{i=n1+n2+n3}^{n1+n2+n3+n4-1}\\gamma^i\n",
    "$$\n",
    "\n",
    ">1. $\\gamma^{n1-1}10$: Dieser Term repräsentiert die Belohnung von 10 Punkten, die der Roboter erhält, wenn er erfolgreich einen zuvor schmutzigen Raum reinigt. Diese Aktion findet nach $n1-1$ Schritten statt, daher wird die Belohnung mit $\\gamma^{n1-1}$ diskontiert.\n",
    ">\n",
    ">2. $\\gamma^{n1+n2+n3-1}10$: Dieser Term repräsentiert die Belohnung von 10 Punkten, die der Roboter erhält, wenn er erfolgreich einen weiteren zuvor schmutzigen Raum reinigt. Diese Aktion findet nach $n1+n2+n3-1$ Schritten statt, daher wird die Belohnung mit $\\gamma^{n1+n2+n3-1}$ diskontiert.\n",
    ">\n",
    ">3. $-\\sum_{i=n1}^{n1+n2-1}\\gamma^i$: Dieser Term repräsentiert die Kosten von -1 Punkt für jede Bewegung mittels `move`, die der Roboter ausführt, um vom Raum $r1$ zum Raum $r2$ zu gelangen. Da diese Aktionen zwischen den Schritten $n1$ und $n1+n2-1$ stattfinden, werden die Kosten entsprechend diskontiert.\n",
    ">\n",
    ">4. $-\\sum_{i=n1+n2+n3}^{n1+n2+n3+n4-1}\\gamma^i$: Dieser Term repräsentiert die Kosten von -1 Punkt für jede Bewegung mittels `move`, die der Roboter ausführt, um vom Raum $r2$ zurück zum Raum $r1$ zu gelangen. Da diese Aktionen zwischen den Schritten $n1+n2+n3$ und $n1+n2+n3+n4-1$ stattfinden, werden die Kosten entsprechend diskontiert.\n",
    "\n",
    "Die Summe dieser Terme gibt den erwarteten kumulierten diskontierten Ertrag für die Episode $e_{n1,n2,n3,n4}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damit folgt\n",
    "\n",
    "$$\n",
    "U^\\gamma_D(\\pi_{vc}) = \\sum_{n1,n2,n3,n4 \\in \\mathbb{N}^+} P(e_{n1,n2,n3,n4})U^\\gamma_D(e_{n1,n2,n3,n4}) $$\n",
    "$$= \\sum_{n1,n2,n3,n4 \\in \\mathbb{N}^+} \\left( 0.5184 \\cdot 0.2^{n1+n3-2} \\cdot 0.1^{n2+n4-2} \\right) \\left( \\gamma^{n1-1}10+\\gamma^{n1+n2+n3-1}10-\\sum_{i=n1}^{n1+n2-1}\\gamma^i-\\sum_{i=n1+n2+n3}^{n1+n2+n3+n4-1}\\gamma^i \\right)\n",
    "$$\n",
    "\n",
    "Mit numerischen Methoden erhalten wir $U^\\gamma_D(\\pi_{vc}) \\approx 15.529$.\n",
    "\n",
    "Eine Strategie $\\pi^*$ ist nun optimal, wenn sie den Nutzen maximiert:\n",
    "\n",
    "$$\n",
    "\\pi^* = \\arg\\max_\\pi U^\\gamma_D(\\pi)\n",
    "$$\n",
    "\n",
    "Eine naive Methode, eine optimale Strategie zu bestimmen, besteht darin, für alle möglichen Strategien ihren Nutzen zu berechnen und eine Strategie mit maximalem Nutzen auszuwählen. Betrachten wir nur konstante Strategien, so ist deren Anzahl $|A|^{|S \\setminus S_t|}$ und dies macht natürlich diese naive Methode nicht praktikabel. Im Folgenden werden wir uns zwei effektivere Verfahren zur Ermittlung einer optimalen Strategie $\\pi^*$, für den Fall, dass der MDP $D$ bekannt ist, anschauen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.2.Iterative Entwicklung der Zustandsnutzen <a name=\"4_1_2\"></a>\n",
    "[policy and value iteration](https://www.youtube.com/watch?v=l87rgLg90HI)\n",
    "\n",
    "Die iterative Entwicklung des Zustandsnutzens (**Value Iteration**) ist ein wichtiger Algorithmus in der Verstärkungslerntheorie. Wir haben zuvor schon den Nutzen von Episoden und Strategien definiert. Der Nutzen eines Zustands $s$ bezüglich einer Strategie $\\pi$ ist definiert als der erwartete Nutzen aller Episoden, die in $s$ starten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ">**Definition 6:** Nutzen von Zuständen\n",
    ">\n",
    "> Sei $D = (S,A,P,R,s_0,S_t)$ ein Markov-Entscheidungsprozess (MDP), $\\gamma \\in [0,1]$ ist der Diskontierungsfaktor, $s \\in S$ ist ein Zustand, und $\\pi : S \\setminus S_t \\rightarrow A$ ist eine Strategie für $D$. Der Nutzen $U^\\gamma_D(s | \\pi)$ von $s$ bezüglich $\\pi$ in $D$ ist definiert durch:\n",
    ">\n",
    ">$$U^\\gamma_D(s | \\pi) = E_{\\pi\\sim e=(s,a_1,s_1,...)}\\left[U^\\gamma_D(e)\\right] = \\sum_{\\pi\\sim e=(s,a_1,s_1,...)} P(e)U^\\gamma_D(e)$$\n",
    "\n",
    "Diese Formel besagt, dass der Nutzen eines Zustands s unter einer Strategie π gleich dem erwarteten (E) Nutzen aller Episoden e ist, die in s starten und der Strategie π folgen. Dieser Erwartungswert wird berechnet, indem über alle solchen Episoden summiert wird, wobei jede Episode mit ihrer Wahrscheinlichkeit P(e) gewichtet wird und der Nutzen dieser Episode $U^γ_D(e)$ berücksichtigt wird.\n",
    "\n",
    "Wenn $\\pi^*$ optimal ist, schreiben wir statt $U^\\gamma_D(s | \\pi^*)$ nur $U^\\gamma_D(s)$, was dann der optimale erwartete Nutzen von $s$ ist. Das bedeutet, dass es keine Strategie gibt, die einen höheren erwarteten Nutzen für Zustand s liefert als π*. Dies ist ein zentraler Aspekt der Theorie des Verstärkungslernens und bildet die Grundlage für viele Algorithmen in diesem Bereich."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Beispiel 5.** <a name=\"b5\"></a> Fortsetzung von Beispiel 4.\n",
    "\n",
    "Wie vorher erwähnt: Wenn $\\pi^*$ optimal ist, schreiben wir statt $U^\\gamma_D(s | \\pi^*)$ nur $U^\\gamma_D(s)$. Wir können nach diese Überlegung dort leicht sehen, dass\n",
    "\n",
    "$$U^\\gamma_D(s_{1,1}^1 | \\pi_{vc}) = U^\\gamma_D(\\pi_{vc}) \\approx 15.529$$\n",
    "\n",
    "gilt. Schauen wir uns den Zustand $s_{0,0}^2$ an (der Agent ist in Raum $r2$ und beide Räume sind sauber). Jede von $\\pi_{vc}$ in $s_{0,0}^2$ startende Episode $e^m$ (für $m \\in \\mathbb{N}$) hat die Form\n",
    "\n",
    "$$e^m = (s_{0,0}^2, \\text{{move}}, \\underbrace{s_{0,0}^1, \\text{{charge}}, s_t, \\ldots}_{m \\text{{-mal}}})$$\n",
    "\n",
    "und damit die Wahrscheinlichkeit\n",
    "\n",
    "$$P(e^m) = 0.9 \\cdot 0.1^{m-1} \\cdot 1$$\n",
    "\n",
    "und den Nutzen\n",
    "\n",
    "$$U^\\gamma_D(e^m) = -\\sum_{i=1}^{m} 0.9^{i-1}$$\n",
    "\n",
    "Es folgt\n",
    "\n",
    "$$U^\\gamma_D(s_{0,0}^2 | \\pi_{vc}) = -\\sum_{m=1}^{\\infty} \\left(0.9 \\cdot 0.1^{m-1} \\sum_{i=1}^{m} 0.9^{i-1}\\right) \\approx -1.099$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Beispiel betrachten wir den Zustand $s_{0,0}^2$, in dem der Agent sich im Raum $r2$ befindet und beide Räume sauber sind. Jede Episode $e^m$, die in $s_{0,0}^2$ startet und der Strategie $\\pi_{vc}$ folgt, hat die Form:\n",
    "\n",
    "$$e^m = (s_{0,0}^2, \\text{{move}}, \\underbrace{s_{0,0}^1, \\text{{charge}}, s_t, \\ldots}_{m \\text{{-mal}}})$$\n",
    "\n",
    "Die Wahrscheinlichkeit $P(em)$ für jede solche Episode ist gegeben durch:\n",
    "\n",
    "$$P(e^m) = 0.9 \\cdot 0.1^{m-1} \\cdot 1$$\n",
    "\n",
    "Der Nutzen $U^\\gamma_D(e^m)$ für jede solche Episode ist gegeben durch:\n",
    "\n",
    "$$U^\\gamma_D(e^m) = -\\sum_{i=1}^{m} 0.9^{i-1}$$\n",
    "\n",
    "Daher ist der Nutzen $U^\\gamma_D(s_{0,0}^2 | \\pi_{vc})$ von $s_{0,0}^2$ unter der Strategie $\\pi_{vc}$ gleich:\n",
    "\n",
    "$$U^\\gamma_D(s_{0,0}^2 | \\pi_{vc}) = -\\sum_{m=1}^{\\infty} \\left(0.9 \\cdot 0.1^{m-1} \\sum_{i=1}^{m} 0.9^{i-1}\\right) \\approx -1.099$$\n",
    "\n",
    "Das bedeutet, dass der erwartete Nutzen des Zustands $s_{0,0}^2$ unter der Strategie $\\pi_{vc}$ etwa $-1.099$ beträgt. Dieser negative Wert zeigt an, dass es für den Agenten nachteilig ist, sich in diesem Zustand zu befinden und der gegebenen Strategie zu folgen. Der Agent würde also versuchen, diesen Zustand zu vermeiden oder eine andere Strategie zu wählen, um den Nutzen zu maximieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun können wir die optimale Strategie $\\pi^* $ einfach mithilfe der VI-Ansatz bestimmen.\n",
    "\n",
    "#### Der Bellmann-Update $u_{i+1}(s)$\n",
    "\n",
    "Wenn der Nutzen $U^\\gamma_D(s)$ für alle Zustände $s$ gegeben ist, kann eine optimale Strategie $\\pi^*$ einfach durch die folgende Formel bestimmt werden:\n",
    "\n",
    "$$\\pi^*(s) = \\arg\\max_{a \\in A} \\sum_{s' \\in S} P(s,a,s') \\left[R(s,a,s') + \\gamma U^\\gamma_D(s')\\right] \\tag{1}$$\n",
    "\n",
    "Mit anderen Worten, $\\pi^*$ wählt in jedem Zustand $s$ die Aktion aus, die im Erwartungswert in einen Zustand mit hohem Nutzen führt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Value Iteration (VI)**: Der VI-Ansatz versucht, die Werte $U^\\gamma_D(s)$ iterativ zu bestimmen, um daraus eine optimale Strategie ableiten zu können. Zentral dafür ist der folgende rekursive Zusammenhang zwischen den Zustandswerten, der aus der obigen Charakterisierung der optimalen Strategie einfach abgeleitet werden kann:\n",
    "\n",
    " $$U^\\gamma_D(s) = \\max_{a \\in A} \\sum_{s' \\in S} P(s,a,s') \\left[R(s,a,s') + \\gamma U^\\gamma_D(s')\\right] \\tag{2}$$\n",
    "\n",
    "   Mit anderen Worten, der Nutzen von $s$ ist gleich der Summe des erwarteten Nutzens der direkten Belohnung und des erwarteten Nutzens des Folgezustands, gegeben, dass der Agent die beste Aktion ausführt. Gleichungen der Form (2) nennen wir **Bellman-Gleichungen** und sie bilden einen wichtigen Bestandteil für die Entwicklung von Algorithmen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lösung des Gleichungssystems**: Die Bestimmung der Werte $U^\\gamma_D(s)$ für alle $s \\in S$ (und damit die Bestimmung einer optimalen Strategie) kann nun als Gleichungssystem mit $|S|$ Gleichungen der Form (2) und $|S|$ Unbekannten (den Werten $U^\\gamma_D(s)$ für alle $s \\in S$) dargestellt werden. Dieses Gleichungssystem ist allerdings nicht-linear (aufgrund des max-Operators) und ermöglicht keine direkte analytische Lösung. Durch die Nutzung iterativer Techniken kann die Lösung jedoch beliebig genau approximiert werden. Dabei kann (2) folgendermaßen als Update-Regel benutzt werden:\n",
    "\n",
    ">**Bellman-Update**: Definiere Startnutzen $u_0(s) := 0$ und setze für $i \\geq 0$:\n",
    ">\n",
    ">$$u_{i+1}(s) := \\max_{a \\in A} \\sum_{s' \\in S} P(s,a,s') \\left[R(s,a,s') + \\gamma u_i(s')\\right] \\tag{3}$$\n",
    ">\n",
    ">Wobei $u_{i+1}(s)$ der geschätzte Nutzen des Zustands $s$ nach der $(i+1)$-ten Iteration des Updates ist.\n",
    "\n",
    "Die Regel (3) nennt man auch Bellman-Update und es kann gezeigt werden, dass die Folge der $u_i$ gegen die Nutzen der Zustände konvergiert.n des Updates.\n",
    "\n",
    "Die Idee hinter der Bellman-Update-Formel ist, dass der Nutzen eines Zustands gleich der maximalen erwarteten sofortigen Belohnung plus dem diskontierten erwarteten Nutzen des Folgezustands ist, gegeben, dass der Agent die beste Aktion ausführt. Durch wiederholtes Anwenden dieser Formel konvergieren die geschätzten Nutzenwerte schließlich gegen die wahren Nutzenwerte. Dies ermöglicht es dem Agenten, eine optimale Stra e konvergieren. Dieser Ansatz wird als **Value Iteration** bezeichnet und ist ein grundlegender Algorithmus in der Verstärkungslerntheorie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### VI-Algorithmus\n",
    ">Die Value Iteration **VI** ist ein Algorithmus, der in Bezug auf Markov-Entscheidungsprobleme (MDPs) verwendet wird, um eine optimale Strategie zu finden. \n",
    ">\n",
    ">In einem MDP besteht die Aufgabe darin, eine optimale Strategie zu finden, die die erwartete kumulative Belohnung maximiert. Die Value Iteration erreicht dies durch iteratives Aktualisieren der Wertfunktion, die den erwarteten kumulativen Nutzen darstellt, den ein Agent erhält, wenn er von einem bestimmten Zustand aus handelt und dabei einer bestimmten Strategie fol¹².\n",
    ">\n",
    ">Der Value Iteration Algorithmus beginnt mit einer anfänglichen Schätzung der Wertfunktion und aktualisiert diese Schätzung iterativ, bis die Änderungen unter einem bestimmten Schwellenwert ligen². In jeder Iteration wird für jeden Zustand der erwartete Nutzen für jede mögliche Aktion berechnet und der maximale erwartete Nutzen wird als neuer Wert für diesen Zustand festgegt¹².\n",
    ">\n",
    ">Die Value Iteration konvergiert schließlich gegen die tatsächliche Wertfunktion, und die daraus resultierende Strategie ist die optimale Strtegie. Es ist wichtig zu beachten, dass die Value Iteration nur in Situationen anwendbar ist, in denen der Zustands- und Aktionsraum ausreichend klein ist, um eine vollständige Aufzählung aller Zustände und Aktionen zu ermöglichen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Theorem 1.**\n",
    ">Für alle $s \\in S$, $\\lim_{i \\to \\infty} u_i(s) = U^\\gamma_D(s)$.\n",
    ">\n",
    ">Das gegebene Theorem besagt, dass für alle Zustände $s$ in $S$, der Grenzwert der Nutzenfunktion $u_i(s)$, wenn $i$ gegen Unendlich geht, gleich dem diskontierten Nutzen $U^\\gamma_D(s)$ ist.\n",
    ">\n",
    ">In anderen Worten, wenn wir die Bellman-Update-Regel unendlich oft anwenden (also $i$ gegen Unendlich geht), konvergiert der berechnete Nutzen $u_i(s)$ gegen den wahren diskontierten Nutzen $U^\\gamma_D(s)$ des Zustands $s$.\n",
    "\n",
    "\n",
    "#### **Beispiel 6.** <a name=\"b6\"></a> Fortsetzung von Beispiel 5.\n",
    "\n",
    "Wir setzen initial alle möglichen Zustände\n",
    "\n",
    "$$u_0(s^{1,1}_1) = u_0(s^{1,1}_2) = u_0(s^{1,0}_1) = u_0(s^{1,0}_2) = u_0(s^{0,1}_1) = u_0(s^{0,1}_2) = u_0(s^{0,0}_1) = u_0(s^{0,0}_2) = u_0(s^t) = 0$$und betrachten alle möglichen Aktionen (move, clean, charge) und berechnen wir den erwarteten Nutzen für jede Aktion mit für $\\gamma = 0.9$ exemplarisch (beachten Sie, dass wir nur Zustandsübergänge mit positiver Wahrscheinlichkeit in den Summen aufzählen)\n",
    "\n",
    "$$u_1(s^{1,1}_1) = \\max \\left\\{\n",
    "\\begin{array}{l}\n",
    "P(s^{1,1}_1,\\text{{move}},s^{1,1}_1) \\left[R(s^{1,1}_1,\\text{{move}},s^{1,1}_1) + \\gamma u_0(s^{1,1}_1)\\right] + P(s^{1,1}_1,\\text{{move}},s^{1,1}_2) \\left[R(s^{1,1}_1,\\text{{move}},s^{1,1}_2) + \\gamma u_0(s^{1,1}_2)\\right], \\\\\n",
    "P(s^{1,1}_1, \\text{{clean}},s^{1,1}_1) \\left[R(s^{1,1}_1, \\text{{clean}},s^{1,1}_1) + \\gamma u_0(s^{1,1}_1)\\right] + P(s^{1,1}_1, \\text{{clean}},s^{0,1}_1) \\left[R(s^{1,1}_1, \\text{{clean}},s^{0,1}_1) + \\gamma u_0(s^{0,1}_1)\\right], \\\\\n",
    "P(s^{1,1}_1, \\text{{charge}},s^t) \\left[R(s^{1,1}_1, \\text{{charge}},s^t) + \\gamma u_0(s^t)\\right]\n",
    "\\end{array}\n",
    "\\right\\}$$\n",
    "\n",
    "$$= \\max \\left\\{0.1[-1+0.9 \\cdot 0] + 0.9[-1+0.9 \\cdot 0], 0.2[0+0.9 \\cdot 0] + 0.8[10+0.9 \\cdot 0], 1[-7+0.9 \\cdot 0]\\right\\} = \\max \\{-1, 8, -7\\} = 8$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.3.Iterative Strategieentwicklung <a name=\"4_1_3\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.4. Glossar <a name=\"4_1_4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Definition 1.: Markov-Entscheidungsprozess**\n",
    ">\n",
    "> Ein Markov-Entscheidungsprozess (engl. Markov decision process, MDP) D ist ein Tupel $D = (S,A,P,R,s_0,S_t)$ mit folgenden Eigenschaften:\n",
    "> 1. $S$ ist eine Menge von Zuständen (der Zustandsraum)\n",
    "> 2. $A$ ist eine Menge von Aktionen (der Aktionsraum)\n",
    "> 3. $P : (S \\setminus S_t) \\times A \\times S \\rightarrow [0,1]$ ist eine Funktion (die Übergangswahrscheinlichkeitsfunktion) mit $\\sum_{s' \\in S} P(s,a,s') = 1$ für alle $s \\in S$, $a \\in A$.\n",
    "> 4. $R : (S \\setminus S_t) \\times A \\times S \\rightarrow \\mathbb{R}$ ist eine beliebige reellwertige Funktion (die Belohnungsfunktion, engl. reward function).\n",
    "> 5. $s_0 \\in S$ ist der Startzustand.\n",
    "> 6. $S_t \\subseteq S$ ist die Menge der Zielzustände.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Definition 2.: Strategie**\n",
    ">\n",
    ">Sei $D = (S,A,P,R,s_0,S_t)$ ein MDP. Eine konstante Strategie $\\pi$ für $D$ ist eine Funktion $$\\pi : S \\setminus S_t \\rightarrow A$$\n",
    ">\n",
    ">Eine **konstante Strategie** ist eine Funktion, die jedem Zustand $s$ eine bestimmte Aktion $a$ zuweist. Sie ist deterministisch, d.h., sie gibt für jeden Zustand genau eine Aktion vor.\n",
    ">\n",
    ">Es gibt jedoch auch **probabilistische Strategien**, die jedem Zustand eine Wahrscheinlichkeitsverteilung über die möglichen Aktionen zuweisen. Diese Art von Strategie kann nützlich sein, wenn es Unsicherheit oder Zufälligkeit in der Umgebung gibt. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Definition 3.: Episode**\n",
    ">\n",
    ">Sei $D = (S,A,P,R,s_0,S_t)$ ein MDP. Eine Episode `e` in $D$ ist eine (potenziell unendliche) Folge $e = (s_0, a_1, s_1, a_2, s_2, ...)$ mit $s_0, s_1 ... \\in S$ und $a_0, a_1 ... \\in A$ Die Wahrscheinlichkeit $P(e)$ ist definiert durch\n",
    ">$$P(e) = \\prod_{i>0} P(s_{i-1},a_i,s_i)$$\n",
    ">\n",
    ">Eine Episode ist **initial**, wenn sie mit dem Anfangszustand $s_0$ beginnt bzw. wenn $s_0 = s^0$ gilt, und sie ist **terminierend**, wenn sie mit einem Zustand $s_n$ endet, der zu den Endzuständen $S^t$ gehört d.h. $s_n \\in S^t$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Definition 4. mit $\\gamma$ diskontierte Nutzen von $e$ in $D$**\n",
    ">\n",
    ">Sei $D = (S,A,P,R,s_0,St)$ ein MDP und $e = (s_0,a_1,s_1,a_2,s_2,...)$ eine Episode in D. Für $\\gamma \\in [0,1]$ heißt $U^\\gamma_D(e)$ definiert via\n",
    ">$$U^\\gamma_D(e) = \\sum_{i>0} \\gamma^{i-1}R(s_{i-1},a_i,s_i)$$\n",
    ">der mit $\\gamma$ diskontierte Nutzen von $e$ in $D$.\n",
    ">\n",
    "> Der **Nutzen** einer Episode ist die Summe aller akkumulierten Belohnungen, wobei jede Belohnung mit einem **Discountfaktor** `γ` gewichtet wird. Dieser Diskontfaktor liegt im Bereich von 0 bis 1 und bestimmt, wie stark zukünftige Belohnungen im Vergleich zu sofortigen Belohnungen bewertet werden. \n",
    ">- Bei kleinen Werten von `γ` werden Strategien bevorzugt, die schnell hohe Belohnungen erhalten,\n",
    ">- während bei größeren Werten nahe 1 spätere Belohnungen wichtiger werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Definition 5. Nutzen von Strategie $\\pi$ in $D$**\n",
    ">\n",
    "> Sei $D = (S,A,P,R,s_0,S_t)$ ein Markov-Entscheidungsprozess (MDP), $\\gamma \\in [0,1]$ und $\\pi : S \\setminus S_t \\rightarrow A$ eine Strategie für $D$. Der Nutzen $U^\\gamma_D(\\pi)$ von $\\pi$ in $D$ ist definiert durch\n",
    ">\n",
    "> $$U^\\gamma_D(\\pi) = E_{\\pi \\sim_0 e} \\left[ U^\\gamma_D(e) \\right] = \\sum_{\\pi \\sim_0 e} P(e)U^\\gamma_D(e)$$\n",
    ">\n",
    ">Mit anderen Worten, der Nutzen $U^\\gamma_D(\\pi)$ von $\\pi$ in $D$ ist der durchschnittliche Nutzen aller aus $\\pi$ generierten initialen Episoden, gewichtet nach deren Wahrscheinlichkeit.\n",
    ">\n",
    "> **PS** $\\pi : S \\setminus S_t \\rightarrow A$ ist gelesen als \"für jeden Zustand in $S$, der kein Endzustand $S_t$ ist, weist $\\pi$ eine Aktion aus der Menge A zu.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Definition 6: Der Nutzen $U^\\gamma_D(s | \\pi)$ von Zustand $s$ bezüglich $\\pi$ in $D$**\n",
    ">\n",
    "> Sei $D = (S,A,P,R,s_0,S_t)$ ein Markov-Entscheidungsprozess (MDP), $\\gamma \\in [0,1]$ ist der Diskontierungsfaktor, $s \\in S$ ist ein Zustand, und $\\pi : S \\setminus S_t \\rightarrow A$ ist eine Strategie für $D$. Der Nutzen $U^\\gamma_D(s | \\pi)$ von $s$ bezüglich $\\pi$ in $D$ ist definiert durch:\n",
    ">\n",
    ">$$U^\\gamma_D(s | \\pi) = E_{\\pi\\sim e=(s,a_1,s_1,...)}\\left[U^\\gamma_D(e)\\right] = \\sum_{\\pi\\sim e=(s,a_1,s_1,...)} P(e)U^\\gamma_D(e)$$\n",
    ">\n",
    ">Diese Formel besagt, dass der Nutzen eines Zustands s unter einer Strategie π gleich dem erwarteten (E) Nutzen aller Episoden e ist, die in s starten und der Strategie π folgen. Dieser Erwartungswert wird berechnet, indem über alle solchen Episoden summiert wird, wobei jede Episode mit ihrer Wahrscheinlichkeit P(e) gewichtet wird und der Nutzen dieser Episode $U^γ_D(e)$ berücksichtigt wird.\n",
    ">\n",
    ">Wenn $\\pi^*$ optimal ist, schreiben wir statt $U^\\gamma_D(s | \\pi^*)$ nur $U^\\gamma_D(s)$, was dann der optimale erwartete Nutzen von $s$ ist. Das bedeutet, dass es keine Strategie gibt, die einen höheren erwarteten Nutzen für Zustand s liefert als π*. Dies ist ein zentraler Aspekt der Theorie des Verstärkungslernens und bildet die Grundlage für viele Algorithmen in diesem Bereich."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### VI-Algorithmus\n",
    ">Die Value Iteration **VI** ist ein Algorithmus, der in Bezug auf Markov-Entscheidungsprobleme (MDPs) verwendet wird, um eine optimale Strategie zu finden. \n",
    ">\n",
    ">In einem MDP besteht die Aufgabe darin, eine optimale Strategie zu finden, die die erwartete kumulative Belohnung maximiert. Die Value Iteration erreicht dies durch iteratives Aktualisieren der Wertfunktion, die den erwarteten kumulativen Nutzen darstellt, den ein Agent erhält, wenn er von einem bestimmten Zustand aus handelt und dabei einer bestimmten Strategie fol¹².\n",
    ">\n",
    ">Der Value Iteration Algorithmus beginnt mit einer anfänglichen Schätzung der Wertfunktion und aktualisiert diese Schätzung iterativ, bis die Änderungen unter einem bestimmten Schwellenwert ligen². In jeder Iteration wird für jeden Zustand der erwartete Nutzen für jede mögliche Aktion berechnet und der maximale erwartete Nutzen wird als neuer Wert für diesen Zustand festgegt¹².\n",
    ">\n",
    ">Die Value Iteration konvergiert schließlich gegen die tatsächliche Wertfunktion, und die daraus resultierende Strategie ist die optimale Strtegie. Es ist wichtig zu beachten, dass die Value Iteration nur in Situationen anwendbar ist, in denen der Zustands- und Aktionsraum ausreichend klein ist, um eine vollständige Aufzählung aller Zustände und Aktionen zu ermöglichen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Bellman-Update**: Definiere Startnutzen $u_0(s) := 0$ und setze für $i \\geq 0$:\n",
    ">\n",
    ">$$u_{i+1}(s) := \\max_{a \\in A} \\sum_{s' \\in S} P(s,a,s') \\left[R(s,a,s') + \\gamma u_i(s')\\right] \\tag{3}$$\n",
    ">\n",
    ">Sie wird verwendet, um den geschätzten Nutzen (oder Wert) eines Zustands in einem Markov-Entscheidungsproblem iterativ zu aktualisieren.\n",
    ">\n",
    ">Die Idee hinter der Bellman-Update-Formel ist, dass der Nutzen eines Zustands gleich der maximalen erwarteten sofortigen Belohnung plus dem diskontierten erwarteten Nutzen des Folgezustands ist, gegeben, dass der Agent die beste Aktion ausführt. Durch wiederholtes Anwenden dieser Formel konvergieren die geschätzten Nutzenwerte schließlich gegen die wahren Nutzenwerte. Dies ermöglicht es dem Agenten, eine optimale Strategie zu lernen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unterschiede zwischen der Iterativen Entwicklung der Zustandsnutzen und der Iterativen Strategieentwicklung\n",
    "\n",
    "Iterative Entwicklung der Zustandsnutzen und Iterative Strategieentwicklung sind zwei wichtige Konzepte im Bereich des Reinforcement Learning. Hier sind die Hauptunterschiede in Stichpunkten:\n",
    "\n",
    "- **Iterative Entwicklung der Zustandsnutzen (Value Iteration)**: Dieser Ansatz konzentriert sich auf die Berechnung der optimalen Nutzenwerte für jeden Zustand. Es beginnt mit einer anfänglichen Schätzung der Nutzenwerte und verbessert diese Schätzungen iterativ, bis sie konvergieren¹.\n",
    "- **Iterative Strategieentwicklung (Policy Iteration)**: Dieser Ansatz konzentriert sich auf die Verbesserung der Strategie. Es beginnt mit einer anfänglichen Strategie und verbessert diese Strategie iterativ, indem es abwechselnd die Nutzenwerte für die aktuelle Strategie berechnet und dann die Strategie anhand dieser Nutzenwerte aktualisiert².\n",
    "\n",
    "Und hier sind die Unterschiede in einer Tabelle:\n",
    "\n",
    "|  | Iterative Entwicklung der Zustandsnutzen | Iterative Strategieentwicklung |\n",
    "|---|---|---|\n",
    "| Fokus | Berechnung der optimalen Nutzenwerte für jeden Zustand¹ | Verbesserung der Strategie² |\n",
    "| Prozess | Beginnt mit einer anfänglichen Schätzung der Nutzenwerte und verbessert diese Schätzungen iterativ¹ | Beginnt mit einer anfänglichen Strategie und verbessert diese Strategie iterativ² |\n",
    "| Anwendung | Nützlich, wenn die genaue Strategie nicht wichtig ist und nur die Nutzenwerte benötigt werden¹ | Nützlich, wenn eine optimale Strategie benötigt wird² |\n",
    "\n",
    "Bitte beachten Sie, dass diese Unterschiede auf einem hohen Niveau sind und die tatsächlichen Algorithmen und Methoden für Value Iteration und Policy Iteration komplexer sind und zusätzliche Details und Überlegungen erfordern.\n",
    "\n",
    "Source: Conversation with Bing, 7.2.2024\n",
    "(1) Agiles Arbeiten – iterativ und inkrementell - Agile Academy. https://www.agile-academy.com/de/scrum-master/agiles-arbeiten-iterativ-und-inkrementell/.\n",
    "(2) Was ist ein iterativer Prozess? Bedeutung und Beispiele! - Asana. https://asana.com/de/resources/iterative-process.\n",
    "(3) Iterativ oder inkrementell - wo liegt der Unterschied?. https://www.visionandaim.com/2020/11/06/iterativ-oder-inkrementell-wo-liegt-der-unterschied/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alle Beispiele"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Beispiel 1.** <a name=\"b1\"></a> Ein stark vereinfachtes Modell des Staubsaugerroboterproblems\n",
    "\n",
    "Wir gehen davon aus, dass unsere Wohnung aus zwei Räumen, r1 und r2, besteht, die entweder sauber oder verschmutzt sein können. Der Zustandsraum ist dann definiert durch:\n",
    "\n",
    "$$S^{vc} = \\{s^{1,1}_1, s^{1,1}_2, s^{0,1}_1, s^{0,1}_2, s^{1,0}_1, s^{1,0}_2, s^{0,0}_1, s^{0,0}_2, s^t\\}$$\n",
    "\n",
    "wobei der Zustand $s^{k,l}_i$ repräsentiert, dass der Roboter in Raum $r_i$ ist:\n",
    "- Raum $r_1$ bei $k = 0$ sauber und bei $k = 1$ verschmutzt ist\n",
    "- und Raum $r_2$ bei $l = 0$ sauber und bei $l = 1$ verschmutzt ist.\n",
    "\n",
    "$s^t$ ist der Zielzustand und $s^{1,1}_1$ ist der Startzustand. \n",
    "\n",
    "In jedem Zustand (außer $s^t$) hat der Roboter drei mögliche Aktionen:\n",
    "\n",
    "1. move: Gehe in den anderen Raum.\n",
    "2. clean: Reinige den aktuellen Raum.\n",
    "3. charge: Lade die Batterie.\n",
    "\n",
    "Also, $A_{vc} = \\{move, clean, charge\\}$. \n",
    "\n",
    "Die Übergangswahrscheinlichkeiten (modelliert durch $P_{vc}$) sind so definiert, dass sich in Raum $r_1$ eine Ladestation befindet und nach Ausführung der Aktion charge in $r_1$ der aktuelle \"Tag\" endet und wir zum finalen Zustand $s^t$ wechseln. \n",
    "\n",
    "Die Aktion move ist zu 90% erfolgreich (d.h., mit Wahrscheinlichkeit 0.1 bleibt der Roboter im aktuellen Raum) und die Aktion clean ist zu 80% erfolgreich (d.h., ein schmutziger Raum bleibt mit Wahrscheinlichkeit 0.2 schmutzig; ein sauberer Raum bleibt sauber). \n",
    "\n",
    "Die Belohnungen ($R_{vc}$) sind wie folgt definiert:\n",
    "\n",
    "1. Wenn der Roboter erfolgreich einen zuvor schmutzigen Raum reinigt, erhält er 10 Punkte.\n",
    "2. Wenn der Roboter einen sauberen Raum reinigt, erhält er -2 Punkte (da er unnötig Energie verbraucht hat).\n",
    "3. Wenn der Roboter in Raum $r_2$ zu laden versucht, erhält er -5 Punkte (er hat bei dem Versuch seine Batterie beschädigt).\n",
    "4. Wenn der Roboter in $r_1$ auflädt, ohne vorher alle Räume gereinigt zu haben, erhält er -7 Punkte.\n",
    "5. Für jede Bewegung mittels move (unabhängig davon, ob erfolgreich oder nicht) erhält der Roboter -1 Punkt (der Roboter soll möglichst effizient die Wohnung reinigen).\n",
    "\n",
    "Für alle weiteren Situationen beträgt die Belohnung 0 Punkte.\n",
    "\n",
    "Wir erhalten: $$ D_{vc} = (S_{vc}, A_{vc}, P_{vc}, R_{vc}, s^{1,1}_1, S_{vc}^t = \\{s^t\\})$$\n",
    "\n",
    "#### **Beispiel 2** <a name=\"b2\"></a> Fortsetzung von Beispiel 1\n",
    "\n",
    "Eine mögliche Strategie $π_{vc}$ ist wie folgt definiert:\n",
    "\n",
    "$$\\pi_{vc}(s^{1,1}_{1}) = \\pi_{vc}(s^{1,0}_{1}) = \\pi_{vc}(s^{1,1}_{2}) = \\pi_{vc}(s^{0,1}_{2}) = \\text{clean}$$\n",
    "$$\\pi_{vc}(s^{0,1}_{1}) = \\pi_{vc}(s^{1,0}_{2}) = \\pi_{vc}(s^{0,0}_{2}) = \\text{move}$$\n",
    "$$\\pi_{vc}(s^{0,0}_{1}) = \\text{charge}$$\n",
    "\n",
    "Das bedeutet, wenn der Roboter sich in einem verschmutzten Raum befindet, führt er die Aktion `clean` aus. Wenn der aktuelle Raum sauber ist und der andere Raum verschmutzt ist, wechselt der Roboter in den anderen Raum (`move`). Wenn beide Räume sauber sind, wechselt der Roboter zuerst in Raum `r1` und lädt dann auf (`charge`).\n",
    "\n",
    "Die Aufgabe eines Offline-Lernverfahrens besteht darin, <ins>eine optimale Die Wahrscheinlichkeit einer Episode $P(e_1)$ kann berechnet werden, indem die Wahrscheinlichkeiten aller Zustandsübergänge in der Episode multipliziert werden. In $e_1$ sind alle Aktionen erfolgreich, daher können wir die Wahrscheinlichkeiten direkt aus den gegebenen Übergangswahrscheinlichkeiten entnehmen.\n",
    "Wir erhalten als Wahrscheinlichkeit $P_{e_1}$ hier\n",
    "\n",
    "$$P(e_1) = P(s_{1,1}^1, \\text{clean}, s_{0,1}^1)P(s_{0,1}^1, \\text{move}, s_{0,1}^2)P(s_{0,1}^2, \\text{clean}, s_{0,0}^2)P(s_{0,0}^2, \\text{move}, s_{0,0}^1)P(s_{0,0}^1, \\text{charge}, s_t)$$\n",
    "$$= 0.8 \\cdot 0.9 \\cdot 0.8 \\cdot 0.9 \\cdot 1.0 = 0.5184$$\n",
    "\n",
    "und mit $\\gamma = 0.9$ als Nutzen\n",
    "\n",
    "$$U^\\gamma_D(e_1) = R(s_{1,1}^1, \\text{clean}, s_{0,1}^1)+\\gamma R(s_{0,1}^1, \\text{move}, s_{0,1}^2)+\\gamma^2P(s_{0,1}^2, \\text{clean}, s_{0,0}^2)+\\gamma^3R(s_{0,0}^2, \\text{move}, s_{0,0}^1)+\\gamma^4R(s_{0,0}^1, \\text{charge}, s_t)$$\n",
    "$$= 10+0.9 \\cdot(-1)+0.9^2 \\cdot 10+0.9^3 \\cdot(-1)+0.9^4 \\cdot 0 = 10-0.9+8.1-0.729 = 16.471$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Beispiel 3** <a name=\"b3\"></a> Fortsetzung von Beispiel 2\n",
    "\n",
    "Betrachten wir die folgende (initiale und terminierende) Episode $e_1$, definiert durch\n",
    "\n",
    "$$e_1 = (s_{1,1}^1, \\text{clean}, s_{0,1}^1, \\text{move}, s_{0,1}^2, \\text{clean}, s_{0,0}^2, \\text{move}, s_{0,0}^1, \\text{charge}, s_t)$$\n",
    "\n",
    "Der Roboter reinigt also zunächst Raum $r_1$, wechselt dann in Raum $r_2$, reinigt diesen, wechselt wieder in Raum $r_1$ und lädt sich dann auf. Alle Aktionen sind hier also erfolgreich. \n",
    "\n",
    "Von Beispiel 1 haben wir bereits einige der Übergangswahrscheinlichkeiten definiert:\n",
    "- Die Aktion `move` ist zu `90% erfolgreich`, d.h., mit Wahrscheinlichkeit 0.1 bleibt der Roboter im aktuellen Raum.\n",
    "- Die Aktion `clean` ist zu `80% erfolgreich`, d.h., ein schmutziger Raum bleibt mit Wahrscheinlichkeit 0.2 schmutzig; ein sauberer Raum bleibt sauber.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Wahrscheinlichkeit einer Episode $P(e_1)$ kann berechnet werden, indem die Wahrscheinlichkeiten aller Zustandsübergänge in der Episode multipliziert werden. In $e_1$ sind alle Aktionen erfolgreich, daher können wir die Wahrscheinlichkeiten direkt aus den gegebenen Übergangswahrscheinlichkeiten entnehmen.\n",
    "\n",
    "Die Episode $e_1$ besteht aus den folgenden Zustandsübergängen:\n",
    "\n",
    "1. $s_{1,1}^1$ (clean) $\\rightarrow$ $s_{0,1}^1$: Der Roboter reinigt Raum $r_1$. Die Wahrscheinlichkeit für diesen Übergang ist 0.8 (da die Aktion `clean` zu 80% erfolgreich ist).\n",
    "2. $s_{0,1}^1$ (move) $\\rightarrow$ $s_{0,1}^2$: Der Roboter wechselt in Raum $r_2$. Die Wahrscheinlichkeit für diesen Übergang ist 0.9 (da die Aktion `move` zu 90% erfolgreich ist).\n",
    "3. $s_{0,1}^2$ (clean) $\\rightarrow$ $s_{0,0}^2$: Der Roboter reinigt Raum $r_2$. Die Wahrscheinlichkeit für diesen Übergang ist 0.8.\n",
    "4. $s_{0,0}^2$ (move) $\\rightarrow$ $s_{0,0}^1$: Der Roboter wechselt zurück in Raum $r_1$. Die Wahrscheinlichkeit für diesen Übergang ist 0.9.\n",
    "5. $s_{0,0}^1$ (charge) $\\rightarrow$ $s_t$: Der Roboter lädt sich auf und erreicht den Zielzustand. Da sich die Ladestation in Raum $r_1$ befindet, ist dieser Übergang immer erfolgreich, d.h., die Wahrscheinlichkeit ist 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir erhalten als Wahrscheinlichkeit $P_{e_1}$ hier\n",
    "\n",
    "$$P(e_1) = P(s_{1,1}^1, \\text{clean}, s_{0,1}^1)P(s_{0,1}^1, \\text{move}, s_{0,1}^2)P(s_{0,1}^2, \\text{clean}, s_{0,0}^2)P(s_{0,0}^2, \\text{move}, s_{0,0}^1)P(s_{0,0}^1, \\text{charge}, s_t)$$\n",
    "$$= 0.8 \\cdot 0.9 \\cdot 0.8 \\cdot 0.9 \\cdot 1.0 = 0.5184$$\n",
    "\n",
    "und mit $\\gamma = 0.9$ als Nutzen\n",
    "\n",
    "$$U^\\gamma_D(e_1) = R(s_{1,1}^1, \\text{clean}, s_{0,1}^1)+\\gamma R(s_{0,1}^1, \\text{move}, s_{0,1}^2)+\\gamma^2P(s_{0,1}^2, \\text{clean}, s_{0,0}^2)+\\gamma^3R(s_{0,0}^2, \\text{move}, s_{0,0}^1)+\\gamma^4R(s_{0,0}^1, \\text{charge}, s_t)$$\n",
    "$$= 10+0.9 \\cdot(-1)+0.9^2 \\cdot 10+0.9^3 \\cdot(-1)+0.9^4 \\cdot 0 = 10-0.9+8.1-0.729 = 16.471$$\n",
    "\n",
    "Schauen wir uns eine weitere Episode $e_2$ mit\n",
    "\n",
    "$$e_2 = (s_{1,1}^1, \\text{clean}, s_{1,1}^1, \\text{clean}, s_{0,1}^1, \\text{charge}, s_t)$$\n",
    "\n",
    "an. Hier versucht der Roboter zunächst vergeblich, Raum $r_1$ zu reinigen. Nachdem es beim zweiten Versuch geklappt hat, geht er direkt an die Ladestation. Wir erhalten\n",
    "\n",
    "$$P(e_2) = P(s_{1,1}^1, \\text{clean}, s_{1,1}^1)P(s_{1,1}^1, \\text{clean}, s_{0,1}^1)P(s_{0,1}^1, \\text{charge}, s_t) = 0.2 \\cdot 0.8 \\cdot 1 = 0.16$$\n",
    "\n",
    "und\n",
    "\n",
    "$$U^\\gamma_D(e_2) = R(s_{1,1}^1, \\text{clean}, s_{1,1}^1)+\\gamma R(s_{1,1}^1, \\text{clean}, s_{0,1}^1)+\\gamma^2R(s_{0,1}^1, \\text{charge}, s_t) = 0+0.9 \\cdot 10+0.9^2(-7) = 9-5.67 = 3.33$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Beispiel 4.**<a name=\"b4\"></a>  Fortsetzung Beispiel 3\n",
    "\n",
    "Wir führen Beispiel 3 fort und betrachten die Strategie $\\pi_{vc}$ aus Beispiel 2. \n",
    "\n",
    "Beachten Sie, dass jede von $\\pi_{vc}$ generierte initiale Episode die folgende Struktur $e_{n1,n2,n3,n4}$ für alle $n1,n2,n3,n4 \\in \\mathbb{N}^+$ hat:\n",
    "\n",
    "$$\n",
    "e_{n1,n2,n3,n4} = (\\underbrace{s^{1,1}_1, \\text{clean},\\ldots,s^{1,1}_1, \\text{clean}}_{n_1 \\text{mal}},\\underbrace{s^{0,1}_1, \\text{move},\\ldots,s^{0,1}_1, \\text{move}}_{n_2-\\text{mal}},\\underbrace{s^{0,1}_2, \\text{clean},\\ldots,s^{0,1}_2, \\text{clean}}_{n_3-\\text{mal}},\\underbrace{s^{0,0}_2, \\text{move},\\ldots,s^{0,0}_2, \\text{move}}_{n_4-\\text{mal}},s^{0,0}_1, \\text{charge},s_t)\n",
    "$$\n",
    "\n",
    "Die Zahlen $n_1$, $n_2$, $n_3$ und $n_4$ repräsentieren die Anzahl der Male, die jede Aktion fehlschlägt, bevor sie erfolgreich ist. Daher unterscheiden sich die einzelnen Episoden darin, wie oft das erste `clean`, das erste `move`, das zweite `clean` und das zweite `move` fehlschlagen. \n",
    "Die Wahrscheinlichkeit, dass eine Aktion erfolgreich ist, wird mit der Wahrscheinlichkeit eines Fehlschlags multipliziert, um die Gesamtwahrscheinlichkeit für eine Sequenz von Aktionen zu berechnen. In unserem Modell funktioniert der `charge`-Übergang immer, wenn der Roboter sich im Raum r1​ befindet, da dort die Ladestation steht. Daher ist die Wahrscheinlichkeit, dass der charge-Übergang erfolgreich ist, `1`.\n",
    "\n",
    "Es gilt\n",
    "$$\n",
    "P(e_{n1,n2,n3,n4}) = 0.2^{n1-1} \\cdot 0.8 \\cdot 0.1^{n2-1} \\cdot 0.9 \\cdot 0.2^{n3-1} \\cdot 0.8 \\cdot 0.1^{n4-1} \\cdot 0.9 \\cdot 1.0 $$\n",
    "$$= 0.5184 \\cdot 0.2^{n1+n3-2} \\cdot 0.1^{n2+n4-2}\n",
    "$$\n",
    "und weiterhin\n",
    "\n",
    "$$\n",
    "U^\\gamma_D(e_{n1,n2,n3,n4}) = \\gamma^{n1-1}10+\\gamma^{n1+n2+n3-1}10-\\sum_{i=n1}^{n1+n2-1}\\gamma^i-\\sum_{i=n1+n2+n3}^{n1+n2+n3+n4-1}\\gamma^i\n",
    "$$\n",
    "Damit folgt\n",
    "\n",
    "$$\n",
    "U^\\gamma_D(\\pi_{vc}) = \\sum_{n1,n2,n3,n4 \\in \\mathbb{N}^+} P(e_{n1,n2,n3,n4})U^\\gamma_D(e_{n1,n2,n3,n4}) $$\n",
    "$$= \\sum_{n1,n2,n3,n4 \\in \\mathbb{N}^+} \\left( 0.5184 \\cdot 0.2^{n1+n3-2} \\cdot 0.1^{n2+n4-2} \\right) \\left( \\gamma^{n1-1}10+\\gamma^{n1+n2+n3-1}10-\\sum_{i=n1}^{n1+n2-1}\\gamma^i-\\sum_{i=n1+n2+n3}^{n1+n2+n3+n4-1}\\gamma^i \\right)\n",
    "$$\n",
    "\n",
    "Mit numerischen Methoden erhalten wir $U^\\gamma_D(\\pi_{vc}) \\approx 15.529$.\n",
    "\n",
    "Eine Strategie $\\pi^*$ ist nun optimal, wenn sie den Nutzen maximiert:\n",
    "\n",
    "$$\n",
    "\\pi^* = \\arg\\max_\\pi U^\\gamma_D(\\pi)\n",
    "$$\n",
    "\n",
    "Wie vorher erwähnt: Wenn $\\pi^*$ optimal ist, schreiben wir statt $U^\\gamma_D(s | \\pi^*)$ nur $U^\\gamma_D(s)$. Wir können nach diese Überlegung dort leicht sehen, dass\n",
    "\n",
    "$$U^\\gamma_D(s_{1,1}^1 | \\pi_{vc}) = U^\\gamma_D(\\pi_{vc}) \\approx 15.529$$\n",
    "\n",
    "gilt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Beispiel 5.** <a name=\"b5\"></a> Fortsetzung von Beispiel 4.\n",
    "\n",
    "Wie vorher erwähnt: Wenn $\\pi^*$ optimal ist, schreiben wir statt $U^\\gamma_D(s | \\pi^*)$ nur $U^\\gamma_D(s)$. Wir können nach diese Überlegung dort leicht sehen, dass\n",
    "\n",
    "$$U^\\gamma_D(s_{1,1}^1 | \\pi_{vc}) = U^\\gamma_D(\\pi_{vc}) \\approx 15.529$$\n",
    "\n",
    "gilt. Schauen wir uns den Zustand $s_{0,0}^2$ an (der Agent ist in Raum $r2$ und beide Räume sind sauber). Jede von $\\pi_{vc}$ in $s_{0,0}^2$ startende Episode $e^m$ (für $m \\in \\mathbb{N}$) hat die Form\n",
    "\n",
    "$$e^m = (s_{0,0}^2, \\text{{move}}, \\underbrace{s_{0,0}^1, \\text{{charge}}, s_t, \\ldots}_{m \\text{{-mal}}})$$\n",
    "\n",
    "und damit die Wahrscheinlichkeit\n",
    "\n",
    "$$P(e^m) = 0.9 \\cdot 0.1^{m-1} \\cdot 1$$\n",
    "\n",
    "und den Nutzen\n",
    "\n",
    "$$U^\\gamma_D(e^m) = -\\sum_{i=1}^{m} 0.9^{i-1}$$\n",
    "\n",
    "Es folgt\n",
    "\n",
    "$$U^\\gamma_D(s_{0,0}^2 | \\pi_{vc}) = -\\sum_{m=1}^{\\infty} \\left(0.9 \\cdot 0.1^{m-1} \\sum_{i=1}^{m} 0.9^{i-1}\\right) \\approx -1.099$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Beispiel betrachten wir den Zustand $s_{0,0}^2$, in dem der Agent sich im Raum $r2$ befindet und beide Räume sauber sind. Jede Episode $e^m$, die in $s_{0,0}^2$ startet und der Strategie $\\pi_{vc}$ folgt, hat die Form:\n",
    "\n",
    "$$e^m = (s_{0,0}^2, \\text{{move}}, \\underbrace{s_{0,0}^1, \\text{{charge}}, s_t, \\ldots}_{m \\text{{-mal}}})$$\n",
    "\n",
    "Die Wahrscheinlichkeit $P(em)$ für jede solche Episode ist gegeben durch:\n",
    "\n",
    "$$P(e^m) = 0.9 \\cdot 0.1^{m-1} \\cdot 1$$\n",
    "\n",
    "Der Nutzen $U^\\gamma_D(e^m)$ für jede solche Episode ist gegeben durch:\n",
    "\n",
    "$$U^\\gamma_D(e^m) = -\\sum_{i=1}^{m} 0.9^{i-1}$$\n",
    "\n",
    "Daher ist der Nutzen $U^\\gamma_D(s_{0,0}^2 | \\pi_{vc})$ von $s_{0,0}^2$ unter der Strategie $\\pi_{vc}$ gleich:\n",
    "\n",
    "$$U^\\gamma_D(s_{0,0}^2 | \\pi_{vc}) = -\\sum_{m=1}^{\\infty} \\left(0.9 \\cdot 0.1^{m-1} \\sum_{i=1}^{m} 0.9^{i-1}\\right) \\approx -1.099$$\n",
    "\n",
    "Das bedeutet, dass der erwartete Nutzen des Zustands $s_{0,0}^2$ unter der Strategie $\\pi_{vc}$ etwa $-1.099$ beträgt. Dieser negative Wert zeigt an, dass es für den Agenten nachteilig ist, sich in diesem Zustand zu befinden und der gegebenen Strategie zu folgen. Der Agent würde also versuchen, diesen Zustand zu vermeiden oder eine andere Strategie zu wählen, um den Nutzen zu maximieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Beispiel 6.** <a name=\"b6\"></a> Fortsetzung von Beispiel 5.\n",
    "\n",
    "Wir setzen initial alle möglichen Zustände\n",
    "\n",
    "$$u_0(s^{1,1}_1) = u_0(s^{1,1}_2) = u_0(s^{1,0}_1) = u_0(s^{1,0}_2) = u_0(s^{0,1}_1) = u_0(s^{0,1}_2) = u_0(s^{0,0}_1) = u_0(s^{0,0}_2) = u_0(s^t) = 0$$\n",
    "\n",
    "und betrachten alle möglichen Aktionen (move, clean, charge) und berechnen wir den erwarteten Nutzen für jede Aktion mit $\\gamma = 0.9$ exemplarisch (beachten Sie, dass wir nur Zustandsübergänge mit positiver Wahrscheinlichkeit in den Summen aufzählen)\n",
    "\n",
    "$$u_1(s^{1,1}_1) = \\max \\left\\{\n",
    "\\begin{array}{l}\n",
    "P(s^{1,1}_1,\\text{{move}},s^{1,1}_1) \\left[R(s^{1,1}_1,\\text{{move}},s^{1,1}_1) + \\gamma u_0(s^{1,1}_1)\\right] + P(s^{1,1}_1,\\text{{move}},s^{1,1}_2) \\left[R(s^{1,1}_1,\\text{{move}},s^{1,1}_2) + \\gamma u_0(s^{1,1}_2)\\right], \\\\\n",
    "P(s^{1,1}_1, \\text{{clean}},s^{1,1}_1) \\left[R(s^{1,1}_1, \\text{{clean}},s^{1,1}_1) + \\gamma u_0(s^{1,1}_1)\\right] + P(s^{1,1}_1, \\text{{clean}},s^{0,1}_1) \\left[R(s^{1,1}_1, \\text{{clean}},s^{0,1}_1) + \\gamma u_0(s^{0,1}_1)\\right], \\\\\n",
    "P(s^{1,1}_1, \\text{{charge}},s^t) \\left[R(s^{1,1}_1, \\text{{charge}},s^t) + \\gamma u_0(s^t)\\right]\n",
    "\\end{array}\n",
    "\\right\\}$$\n",
    "\n",
    "$$= \\max \\left\\{0.1[-1+0.9 \\cdot 0] + 0.9[-1+0.9 \\cdot 0], 0.2[0+0.9 \\cdot 0] + 0.8[10+0.9 \\cdot 0], 1[-7+0.9 \\cdot 0]\\right\\} = \\max \\{-1, 8, -7\\} = 8$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beipsielscode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "class State:\n",
    "    def __init__(self, room1_dirty, room2_dirty, robot_in_room1, battery_charged):\n",
    "        self.room1_dirty = room1_dirty\n",
    "        self.room2_dirty = room2_dirty\n",
    "        self.robot_in_room1 = robot_in_room1\n",
    "        self.battery_charged = battery_charged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VacuumCleaner:\n",
    "    def __init__(self, state):\n",
    "        self.state = state\n",
    "        self.reward = 0\n",
    "        self.rewards = [] \n",
    "\n",
    "    def reward_for(self, a):\n",
    "        if a == 'clean':\n",
    "            if (self.state.robot_in_room1 and self.state.room1_dirty) or (not self.state.robot_in_room1 and self.state.room2_dirty):\n",
    "                return 10  # Belohnung für erfolgreiche Reinigung\n",
    "            else:\n",
    "                return -2  # Strafe für unnötige Reinigung\n",
    "        elif a == 'move':\n",
    "            return -1  # Strafe für Bewegung\n",
    "        elif a == 'charge':\n",
    "            if not self.state.robot_in_room1:\n",
    "                return -5  # Strafe für Versuch, in Raum 2 zu laden\n",
    "            elif self.state.room1_dirty or self.state.room2_dirty:\n",
    "                return -7  # Strafe für Aufladen ohne vorherige Reinigung\n",
    "            else:\n",
    "                return 0  # Keine Strafe oder Belohnung für das Aufladen in einem sauberen Raum\n",
    "        else:\n",
    "            return 0  # Keine Belohnung für unbekannte Aktionen\n",
    "\n",
    "\n",
    "    def move(self, erfolg):\n",
    "        if erfolg:  # 90% Erfolgschance\n",
    "            self.state.robot_in_room1 = not self.state.robot_in_room1\n",
    "            return 0.9\n",
    "        else:\n",
    "            return 0.1\n",
    "        self.state.battery_charged = False  # Batterie entlädt sich nach dem Bewegen\n",
    "        self.reward -= 1  # Belohnung für Bewegung\n",
    "\n",
    "            \n",
    "    def clean_room(self, room_dirty, erfolg):\n",
    "        \n",
    "        if room_dirty:\n",
    "            room_dirty = False  # Update the state of the room\n",
    "            self.reward += 10  # Belohnung für erfolgreiche Reinigung\n",
    "            self.rewards.append(10)\n",
    "            room_dirty = False  # Update the state of the room\n",
    "            return 0.8\n",
    "        else:\n",
    "            room_dirty = False\n",
    "            self.reward -= 2  # Strafe für unnötige Reinigung\n",
    "            self.rewards.append(-2)\n",
    "            return 0.2\n",
    "        \n",
    "\n",
    "    def clean(self, erfolg):\n",
    "        self.state.battery_charged = False  # Batterie entlädt sich nach dem Reinigen\n",
    "        if self.state.robot_in_room1:\n",
    "            temp = self.clean_room(self.state.room1_dirty, erfolg)\n",
    "            self.state.room1_dirty = temp == 0.2\n",
    "            return temp\n",
    "        else:\n",
    "            temp = self.clean_room(self.state.room2_dirty, erfolg)\n",
    "            self.state.room2_dirty = temp == 0.2\n",
    "            print(temp)\n",
    "            return temp\n",
    "\n",
    "    def charge(self, erfolg):\n",
    "        if self.state.robot_in_room1:  # Nur in Raum 1 kann geladen werden\n",
    "            if self.state.room1_dirty or self.state.room2_dirty:\n",
    "                self.rewards.append(-7)\n",
    "                self.reward -= 7  # Strafe für Aufladen ohne vorherige Reinigung\n",
    "            self.state.battery_charged = True\n",
    "            return 1.0  # Das Aufladen ist immer erfolgreich\n",
    "        else:\n",
    "            self.rewards.append(-5)\n",
    "            self.reward -= 5  # Strafe für Versuch, in Raum 2 zu laden\n",
    "            return 0.0  #  Das Aufladen ist in Raum 2 nicht möglich\n",
    "\n",
    "    def execute_episode(self, actions):\n",
    "        reward_points = []\n",
    "        probabilities = []\n",
    "        for action in actions:\n",
    "            if action[0] == 'clean':\n",
    "                probabilities.append(self.clean(action[1]))\n",
    "            elif action[0] == 'move':\n",
    "                self.rewards.append(-1)\n",
    "                probabilities.append(self.move(action[1]))\n",
    "            elif action[0] == 'charge':\n",
    "                probabilities.append(self.charge(action[1]))\n",
    "            else:\n",
    "                print(f'Unbekannte Aktion: {action[0]}')\n",
    "            if self.is_end_state(end_state):\n",
    "                break\n",
    "        reward_points = self.rewards\n",
    "        self.rewards = []\n",
    "        return (probabilities, reward_points)\n",
    "\n",
    "    def is_end_state(self, end_state):\n",
    "        return self.state.room1_dirty == end_state.room1_dirty and \\\n",
    "               self.state.room2_dirty == end_state.room2_dirty and \\\n",
    "               self.state.robot_in_room1 == end_state.robot_in_room1 and \\\n",
    "               self.state.battery_charged == end_state.battery_charged\n",
    "\n",
    "    def strategie_1(self):\n",
    "            if self.state.robot_in_room1:\n",
    "                if self.state.room1_dirty:\n",
    "                    return self.clean\n",
    "                elif self.state.room2_dirty:\n",
    "                    return self.move\n",
    "                else:\n",
    "                    return self.charge\n",
    "            else:\n",
    "                if self.state.room2_dirty:\n",
    "                    return self.clean\n",
    "                elif self.state.room1_dirty:\n",
    "                    return self.move\n",
    "                else:\n",
    "                    return self.move  # Bewegen Sie sich zuerst in Raum 1, bevor Sie aufladen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P_e(episode):\n",
    "    p = robot.execute_episode(episode)[0]\n",
    "    return np.prod(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def U_gamma_D(episode, gamma):\n",
    "        R = VacuumCleaner(State(True, True, True, False)).execute_episode(episode)[1]\n",
    "        print(R)\n",
    "        U = 0\n",
    "        for i, r in enumerate(R):  # Schritte von 2, da Zustände und Aktionen abwechselnd sind\n",
    "            U += (gamma ** i) * r  # Diskontierte Belohnung\n",
    "        return U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5184000000000001"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definieren Sie den Startzustand\n",
    "start_state = State(True, True, True, False)  # Angenommen, der Startzustand ist, wenn beide Räume verschmutzt sind, der Roboter sich in Raum 1 befindet und die Batterie nicht geladen ist\n",
    "\n",
    "# Erstellen Sie eine Instanz des Roboters mit dem Startzustand\n",
    "robot = VacuumCleaner(start_state)\n",
    "\n",
    "# Definieren Sie den Endzustand\n",
    "end_state = State(False, False, True, True)  # Angenommen, der Endzustand ist, wenn beide Räume sauber sind, der Roboter sich in Raum 1 befindet und die Batterie geladen ist\n",
    "\n",
    "episode_1 = [('clean', True), ('move', True), ('clean', True), ('move', True), ('charge', True)]\n",
    "\n",
    "gamma = 0.9\n",
    "\n",
    "P_e(episode_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n",
      "[10, -1, 10, -1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16.471000000000004"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U_gamma_D(episode_1, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16000000000000003"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode_2 = [('clean', False), ('clean', True), ('charge', True)]\n",
    "\n",
    "P_e(episode_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, -2, -7]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.5299999999999994"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U_gamma_D(episode_2, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
