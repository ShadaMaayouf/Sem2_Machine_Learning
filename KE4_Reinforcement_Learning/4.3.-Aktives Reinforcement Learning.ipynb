{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T15:05:33.584082Z",
     "start_time": "2024-01-22T15:05:33.537800Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# 4.3. Aktives Reinforcement Learning\n",
    ">## <ins>Table of contents</ins> <a name=\"up\"></a>[<sup>[1]</sup>](#cite_note-1)\n",
    ">* [**4.3.1. Exploration vs. Exploitation**](#4_3_1)\n",
    ">* [**4.3.2. Œµ-greedy Learning**](#4_3_2)\n",
    ">* [**4.3.3. Q-Learning**](#4_3_3)\n",
    ">* [**4.3.4. Unterschiede zwischen Œµ-greedy Learning und Q-Learning**](#4_3_4)\n",
    ">\n",
    ">## <ins>Beispiele</ins>\n",
    ">* [**Beispiel 1**: Das k-armige Banditenproblem](#b1)\n",
    ">* [**Beispiel 2**: Œµ-greedy Learning des Staubsaugerproblems$](#b2)\n",
    ">* [**Beispiel 3**: Q-Learning des Staubsaugerproblems](#b3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Problemstellung**:\n",
    "\n",
    "Nachdem wir uns in den [Unterkapiteln 4.1](http://localhost:8888/lab/tree/KE4_Reinforcement_Learning/4.1.-Markov-Entscheidungsprozess.ipynb) und [4.2](http://localhost:8888/lab/tree/KE4_Reinforcement_Learning/4.2.-Passives-Reinforcement-Learning.ipynb) mit grundlegenden Fragestellungen und Teilproblemen des Reinforcement Learnings besch√§ftigt haben, kommen wir nun zu der eigentlichen Herausforderung: n√§mlich <ins>des Lernens der optimalen Strategie in einer nicht-deterministischen und unbekannten Umgebung</ins>.-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.1. Exploration vs. Exploitation <a name=\"4_3_1\"></a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F√ºr die optimale Strategie $\\pi^*$ gilt (siehe auch Gleichung (1) aus Unterkapitel 4.1):\n",
    "\n",
    "[$$\\pi^*(s) = \\arg\\max_{a \\in A} \\sum_{s' \\in S} P(s,a,s') \\left( R(s,a,s') + \\gamma U^\\gamma_D(s') \\right) \\tag{1}$$](http://localhost:8888/lab/tree/KE4_Reinforcement_Learning/4.1.-Markov-Entscheidungsprozess.ipynb)  <a name=\"g1\"></a>\n",
    "\n",
    "Mit anderen Worten:\n",
    "Wenn wir korrekte und vollst√§ndige Informationen zu den Nutzen aller Zust√§nde ($U^Œ≥_D$), zu den Belohnungen in den Zust√§nden ($R$) und den Zustands√ºbergangswahrscheinlichkeiten ($P$) haben, sollten wir in jedem Zustand die Aktion ausw√§hlen, die den erwarteten Nutzen maximiert.\n",
    "- In [Unterkapitel 4.2](http://localhost:8888/lab/tree/KE4_Reinforcement_Learning/4.2.-Passives-Reinforcement-Learning.ipynb) haben wir Methoden behandelt, die zu einer gegebenen Strategie $œÄ$ die Nutzen der Zust√§nde $U^Œ≥_D(s | œÄ)$ berechnen.\n",
    "- Au√üerdem wissen wir, dass f√ºr die optimale Strategie gilt $U^Œ≥_D(s) = U^Œ≥_D(s | œÄ*)$.\n",
    "- Bei dem Ansatz der adaptiven dynamischen Programmierung (siehe Abschnitt 4.2.1) konnten wir auch die Werte $P(s,a,s‚Ä≤)$ und $R(s,a,s‚Ä≤)$ approximieren.\n",
    "\n",
    "Auf den ersten Blick scheint es, dass wir unter der Absch√§tzung von $U^Œ≥_D(s)$ durch $U^Œ≥_D(s | œÄ)$ alle notwendigen Komponenten haben, um durch Gleichung (1) die optimale Strategie zu ermitteln. Allerdings gibt es hierbei zwei grundlegende Probleme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allerdings gibt es hierbei **zwei grundlegende Probleme**:\n",
    "\n",
    "1. Mit der adaptiven dynamischen Programmierung k√∂nnen wir nur die Werte $P(s,a,s‚Ä≤)$ und $R(s,a,s‚Ä≤)$ f√ºr $a = œÄ(s)$ approximieren. Da wir eine feste Strategie angenommen haben, hat unser Agent keine Erfahrung, was andere Aktionen im Zustand $s$ bewirken.\n",
    "\n",
    "2. Eine Absch√§tzung von $U^Œ≥_D(s)$ durch $U^Œ≥_D(s | œÄ)$ f√ºr eine nicht-optimale Strategie œÄ kann sehr fehlgeleitet sein.\n",
    "\n",
    "   Stellen Sie sich einen Roboter vor, der den schnellsten Weg aus einem Hochhaus finden soll. Ist $\\pi$ beispielsweise die Strategie, die vorschreibt in das jeweilige n√§chsth√∂here Stockwerk zu gehen und im obersten Stockwerk den Fahrstuhl nach unten zu nehmen und dann das Geb√§ude zu verlassen, so hat der Zustand, bei dem sich der Agent im Erdgeschoss befindet, bzgl. $\\pi$ einen sehr kleinen Nutzen (schlie√ülich ist es f√ºr $\\pi$ noch ein weiter Weg bis zum Ausgang). F√ºr eine optimale Strategie $\\pi^*$ h√§tte dieser Zustand allerdings einen sehr hohen Nutzen, da wir direkt zum Ausgang gehen k√∂nnten.\n",
    "\n",
    "\n",
    "- Ein wichtiger Aspekt des aktiven Reinforcement Learning ist die **Exploration**, bei der in einem Zustand verschiedene Aktionen ausprobiert werden, um zu lernen, welche Aktion tats√§chlich optimal ist.\n",
    "- Sobald der Agent gelernt hat, welche Aktionen zu welchen Zust√§nden f√ºhren, kann dieses Wissen ausgenutzt werden (engl. \"**exploitation**\"), um gewinnbringend in der Umgebung zu agieren.\n",
    "\n",
    "**Das Dilemma von Exploration vs. Exploitation** im Reinforcement Learning beschreibt, wie diese beiden Aspekte gegeneinander abgewogen werden m√ºssen. Es stellt die Frage, wann ein Agent entscheiden kann, dass er genug ausprobiert hat und die bisher beste Strategie tats√§chlich optimal ist.\n",
    "- Eine anschauliche Darstellung dieses Dilemmas wird durch die Betrachtung des k-armigen Banditenproblems gegeben."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Beispiel 1.** <a name=\"b1\"></a> Das k-armige Banditenproblem\n",
    "\n",
    "Ein <ins>einarmiger</ins> Bandit ist ein Spielautomat, der nach Einwerfen einer M√ºnze einen Gewinn ausgibt, der gleichverteilt im Intervall $[a,b]$ wobei $a<b$ liegt. Ein <ins>k-armiger</ins> Bandit hat mehrere \"Arme\", die jeweils unterschiedliche Gewinnintervalle haben. Man w√§hlt nach Einwerfen der M√ºnze einen von den k Armen.\n",
    "  \n",
    "<ins>Betrachten wir den Fall k = 10</ins>: \n",
    "- wir nehmen an, dass wir eine gro√üe Menge an Geld zur Verf√ºgung haben, z.B. 1000 EUR, D.h. wir k√∂nnten die ersten 100 Versuche nutzen, um jeden Arm jeweils 10 Mal zu bet√§tigen und somit plausible Sch√§tzungen der jeweiligen Gewinnintervalle $[a_i,b_i]$ zu erhalten.\n",
    "- Anschlie√üend k√∂nnten wir die verbleibenden 900 EUR nutzen, um ausschlie√ülich den Arm zu bet√§tigen, der am gewinnbringendsten erscheint (beispielsweise den Arm $i$, bei dem $a_i$ maximal ist).\n",
    "\n",
    "**Die allgemeine Frage** beim k-Banditenproblem ist, wie viel Geld wir investieren sollten, um ein m√∂glichst genaues Modell vom Banditen zu erhalten (Exploration), und wie viel Geld wir investieren sollten, um das gelernte Wissen zur Gewinnmaximierung zu nutzen (Exploitation).\n",
    "\n",
    "So mopdelliert man das k-Banditenproblem:\n",
    "- Es gibt einen *Startzustand* und *k Aktionen*, die aus dem Startzustand gew√§hlt werden k√∂nnen.\n",
    "- Nach Ausf√ºhren einer Aktion enden wir direkt in einem Zielzustand und erhalten den Gewinn.\n",
    "- Es gibt also auch nur k verschiedene Strategien f√ºr dieses Problem.\n",
    "\n",
    "In realen Anwendungsszenarien ist die Anzahl der m√∂glichen Strategien allerdings zu gro√ü und es ist nicht m√§glich, zun√§chst alle m√§glichen Strategien ausreichend auszuprobieren (wie im obigen Beispiel angedeutet) und dann die beste Strategie auszuw√§hlen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.2. Œµ-greedy Learning <a name=\"4_3_2\"></a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Die meisten Methoden des aktiven Reinforcement Learning verwenden eine *Meta-Strategie*, um das Dilemma zwischen Exploration und Exploitation zu adressieren. Diese Algorithmen beginnen mit einer initialen Strategie œÄ und nutzen diese zun√§chst, um die Nutzenwerte der Zust√§nde zu erlernen, die mit dieser Strategie erreicht werden.\n",
    "- Je nach Meta-Strategie wird gelegentlich von der Strategie œÄ abgewichen und in einem Zustand s eine andere Aktion a ‚â† œÄ(s) gew√§hlt, um neue Teile des Zustandsraums zu erkunden und die Nutzenwerte der Zust√§nde entsprechend zu aktualisieren.\n",
    "- Wenn sich in den Nutzenwerten abzeichnet, dass in einem Zustand s tats√§chlich eine andere Aktion als œÄ(s) besser ist, wird œÄ entsprechend ge√§ndert.\n",
    "\n",
    "- Die einfachste Instanz einer solchen Meta-Strategie ist die **Œµ-greedy-Strategie**. Hierbei ist $Œµ \\in [0,1]$ ein Parameter, der angibt, wie h√§ufig die Exploration der Exploitation vorgezogen wird, bzw. der die Wahrscheinlichkeit bestimmt, mit der eine zuf√§llige Aktion (Exploration) anstelle der besten bekannten Aktion (Exploitation) gew√§hlt wird.\n",
    "    - Wenn `Œµ = 0`, wird immer die beste bekannte Aktion (Exploitation) gew√§hlt, es findet also keine Exploration statt.\n",
    "    - Wenn `Œµ = 1`, wird immer eine zuf√§llige Aktion gew√§hlt, es findet also nur Exploration statt.\n",
    "    - Mit Wahrscheinlichkeit `Œµ zwischen 0 und 1` wird in einem Zustand s eine Balance zwischen Exploration und Exploitation gefunden D.h. Es wird eine zuf√§llige Aktion ausgew√§hlt und mit Wahrscheinlichkeit 1 ‚àí Œµ wird œÄ(s) ausgef√ºhrt. Beispielsweise bedeutet Œµ = 0.1, dass in 10% der F√§lle eine zuf√§llige Aktion (Exploration) und in 90% der F√§lle die beste bekannte Aktion (Exploitation) gew√§hlt wird."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durch die Nutzung einer solchen Meta-Strategie k√∂nnen beide oben genannten Probleme gel√∂st werden:\n",
    "\n",
    "1. Wenn wir hinreichend oft zuf√§llige Aktionen in einem Zustand $s$ ausf√ºhren, k√∂nnen wir sowohl $P(s,a,s')$ als auch $R(s,a,s')$ f√ºr $a \\neq \\pi(s)$ approximieren.\r\n",
    "\r\n",
    "2. Wenn die Strategie $\\pi$ nicht optimal ist, k√∂nnen dennoch durch eine zuf√§llig gute Auswahl von Aktionen die tats√§chlichen Nutzen von Zust√§nden erkannt werden. F√ºr das Beispiel des Hochhauses w√ºrden wir irgendwann zuf√§llig im Erdgeschoss direkt die Aktion des Verlassens des Geb√§udes ausf√ºhren und k√∂nnten damit direkt den Nutzen des Zustands im Erdgeschoss zu sein, erh√∂hen.Eine Kombination der **Œµ-greedy-Strategie mit der adaptiven dynamischen Programmierung** ([siehe Abschnitt 4.2.1](http://localhost:8888/lab/tree/KE4_Reinforcement_Learning/4.2.-Passives-Reinforcement-Learning.ipynb)) ist \n",
    "in Algorithmus 1 formalisiert.\r\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![e-greedy](./e-greedy.PNG)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Algorithmus implementiert einen einzigen Schritt eines Agenten in einer Umgebung und erh√§lt als Eingabe die aktuelle Beobachtung $o$ (falls der Agent im Startzustand ist, setzen wir $o = \\text{null}$) und gibt die als n√§chstes auszuf√ºhrende Aktion aus. Der Algorithmus besitzt eine Reihe von statischen Datenstrukturen, die √ºber die verschiedenen Aufrufe des Algorithmus erhalten bleiben:\n",
    "\n",
    "- Die Strategie $\\pi$ ist die aktuell beste Strategie des Agenten.\n",
    "- Der MDP $D$ ist das Modell der Umgebung, das der Agent mit jedem Aufruf besser erlernt.\n",
    "- Die Funktion $u$ speichert die Nutzen der Zust√§nde (bzgl. $\\pi$).\n",
    "- Die Funktionen $n_1$ und $n_2$ speichern die H√§ufigkeiten, wie oft in einem Zustand $s$ die Aktion $a$ ausgef√ºhrt wurde ($n_1(s,a)$) bzw. wie h√§ufig beim Ausf√ºhren der Aktion $a$ in $s$ der Zustand $s'$ beobachtet wurde ($n_2(s,a,s')$).\n",
    "\n",
    "Der Algorithmus funktioniert wie folgt:\n",
    "\n",
    "1. Wenn `o` nicht `null` ist, werden die H√§ufigkeiten `n1(s,a)` und `n2(s,a,s')` aktualisiert.\n",
    "2. Die √úbergangswahrscheinlichkeiten `P(s,a,s'')` f√ºr alle `s''` in `S` werden aktualisiert.\n",
    "3. Die Belohnung `R(s,a,s')` wird auf `r` gesetzt.\n",
    "4. Die Nutzenfunktion `u` wird mittels der Funktion `VAL` aktualisiert (werden die Nutzen der Zust√§nde bzgl. der aktuellen Strategie\r\n",
    "aktualisier - Value Iteration.).\n",
    "5. Die Strategie `œÄ` wird mittels der Funktion `POL` aktualisiert (die Unterfunktion POL benutzt dazu direkt die Charakterisierung aus [Gleichung (1)](#g1)).\n",
    "6. Wenn `s'` ein Terminalzustand ist, gibt der Algorithmus `null` zur√ºck In diesem Fall muss der Agent in \n",
    "der Umgebung \r\n",
    "neu gestartet‚Äú werden, wobei die statischen Datenstrukturen nicht neu initialisiert werden..\n",
    "7. Mit Wahrscheinlichkeit `Œµ` gibt der Algorithmus eine zuf√§llige Aktion `a` aus.\n",
    "8. Mit Wahrscheinlichkeit `1 - Œµ` f√ºhrt der Algorithmus die Aktion `œÄ(s')` aus (oder `œÄ(s0)` falls `o` `null` ist\n",
    "Es kann wieder gezeigt werden, dass bei hinreichend vielen Aufrufen von Algorithmus 1 die Strategie œÄ gegen \n",
    "die optimale Strategi  konvergiert).\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Beispiel 2.** <a name=\"b2\"></a> Œµ-greedy Learning f√ºr das Staubsaugerproblem\n",
    "\n",
    "Wir betrachten wieder das Beispiel des Staubsaugerroboters aus den vorherigen Unterkapiteln. Insbesondere ist die Menge der Zust√§nde $S_{vc}$ gegeben durch\n",
    "\n",
    "$$\n",
    "S_{vc} = \\{s_{1,1}^1, s_{1,1}^2, s_{0,1}^1, s_{0,1}^2, s_{1,0}^1, s_{1,0}^2, s_{0,0}^1, s_{0,0}^2, s_t\\}\n",
    "$$\n",
    "\n",
    "wobei $s_{1,1}^1$ der Startzustand und $s_t$ der einzige Zielzustand ist. Die Menge der Aktionen $A_{vc}$ ist gegeben durch\n",
    "\n",
    "$$\n",
    "A_{vc} = \\{\\text{move}, \\text{clean}, \\text{charge}\\}\n",
    "$$\n",
    "\n",
    "Nehmen wir weiterhin an, die initiale Strategie $\\pi$ des Roboters ist gegeben durch\n",
    "\n",
    "$$\\pi(s_{1,1}^1) = \\pi(s_{1,1}^2) = \\pi(s_{0,1}^1) = \\pi(s_{0,1}^2) = \\pi(s_{1,0}^1) = \\pi(s_{1,0}^2) = \\pi(s_{0,0}^1) = \\pi(s_{0,0}^2) = \\text{move}$$\n",
    "\n",
    "Wir f√ºhren Algorithmus 1 exemplarisch zweimal aus:\n",
    "\n",
    "`Aufruf 1`\n",
    "\n",
    "1. Beim ersten Aufruf befindet sich der Agent im Startzustand $s^1_{1,1}$. Da $o = null$ ist werden Zeilen 1‚Äì5 hierbei nicht ausgef√ºhrt.\n",
    "2. Da noch keine Belohnungen beobachtet wurden, gilt initial $R(s,a,s') = 0$ f√ºr alle $s,s' \\in S$, $a \\in A$.\n",
    "3. Es folgt, dass in **Zeile 6**, $u(s) = 0$ f√ºr alle $s \\in S$ errechnet wird.\n",
    "4. In **Zeile 7** ergibt sich, dass alle Strategien gleich gut/schlecht sind, wir √§ndern die aktuelle Strategie also nicht ab.\n",
    "5. Da wir uns nicht in einem Zielzustand befinden und wir annehmen, dass der Zufallstest in Zeilen 9/10 zugunsten von Zeile 10 ausgefallen ist, geben wir $\\pi(s^1_{1,1}) = \\text{move}$ zur√ºck.\n",
    "\n",
    "\n",
    "\n",
    "`Aufruf 2`\n",
    "1. Beim zweiten Aufruf erh√§lt der Algorithmus die Beobachtung $$o = (s^1_{1,1}, \\text{move}, s^2_{1,1}, -1)$$Mit anderen Worten, der Roboter ist erfolgreich in Raum r2 gewechselt und $o \\neq null$. **Zeile 1** liefert den Wert `True`.\n",
    "\n",
    "2. Wir aktualisieren in den **Zeilen 2‚Äì5** die Werte unserer Datenstrukturen wie folgt:\n",
    "    * $n1(s^1_{1,1}, \\text{move}) := n1(s^1_{1,1}, \\text{move})+1 = 0+1 = 1$\n",
    "    * $n2(s^1_{1,1}, \\text{move}, s^2_{1,1}) := n2(s^1_{1,1}, \\text{move}, s^2_{1,1})+1 = 0+1 = 1$\n",
    "    * Die Aktualisierung von $P(s,a,s'')$ erfolgt gem√§√ü der Formel: $$P(s,a,s'') := \\frac{n2(s,a,s'')}{n1(s,a)}$$Daher ist $$P(s^1_{1,1}, \\text{move}, s^2_{1,1}) := \\frac{1}{1} = 1$$\n",
    "    * $P(s^1_{1,1}, \\text{move}, s'') := 0$ f√ºr alle $s'' \\neq s^2_{1,1}$\n",
    "    * $R(s^1_{1,1}, \\text{move}, s^2_{1,1}) := -1$\n",
    "\n",
    "3. In **Zeile 6** ergibt sich $$u(s) := 0 \\qquad \\qquad \\forall \\;s$$\n",
    "   Insbesondere √§ndert sich also (zun√§chst) nicht der Wert von $u(s^1_{1,1})$ und bleibt 0, da er in Zeile 6 √ºber das Maximum aller Aktionen berechnet wird und nur der Nutzen bei Ausf√ºhren der Aktion `move` auf -1 gesetzt wurde.\n",
    "5. In **Zeile 7** ergibt sich, dass f√ºr den Zustand $s^1_{1,1}$ jede Aktion au√üer move einen maximal erwarteten Nutzen von 0 bringt. Wir setzen zuf√§llig $œÄ(s^1_{1,1})$ = clean, f√ºr alle anderen Zust√§nde behalten wir die Strategie.\n",
    "6. In **Zeilen 9/10** w√§hlen wir eine zuf√§llige Aktion `charge` anstelle von $œÄ(s^1_{1,1})$.\n",
    "\n",
    "\n",
    "`Aufruf 3`\n",
    "\n",
    "Angenommen, der Roboter erh√§lt beim dritten Aufruf die Beobachtung: $o = (s_{1,1}^2, \\text{clean}, s_{1,0}^2, 10)$\r",
    "f 3`\r\n",
    "\r\n",
    "1. Da $o \\neq null$, f√ºhren wir die Zeilen 1‚Äì5 aus:\r\n",
    "    * $n1(s^2_{1,1}, \\text{clean}) := n1(s^2_{1,1}, \\text{clean})+1 = 0+1 = 1$\r\n",
    "    * $n2(s^2_{1,1}, \\text{clean}, s^2_{1,0}) := n2(s^2_{1,1}, \\text{clean}, s^2_{1,0})+1 = 0+1 = 1$\r\n",
    "    * Die Aktualisierung von $P(s,a,s'')$ erfolgt gem√§√ü der Formel: $$P(s,a,s'') := \\frac{n2(s,a,s'')}{n1(s,a)}$$Daher ist $$P(s^2_{1,1}, \\text{clean}, s^2_{1,0}) := \\frac{1}{1} = 1$$\r\n",
    "    * $P(s^2_{1,1}, \\text{clean}, s'') := 0$ f√ºr alle $s'' \\neq s^2_{1,0}$\r\n",
    "    * $R(s^2_{1,1}, \\text{clean}, s^2_{1,0}) := 10$\r\n",
    "\r\n",
    "2. In **Zeile 6** ergibt sich $$u(s) := 0 \\qquad \\qquad \\forall \\;s$$\r\n",
    "   Insbesondere √§ndert sich also (zun√§chst) nicht der Wert von $u(s^2_{1,1})$ und bleibt 0, da er in Zeile 6 √ºber das Maximum aller Aktionen berechnet wird und nur der Nutzen bei Ausf√ºhren der Aktion `clean` auf 10 gesetzt wurde.\r\n",
    "3. In **Zeile 7** ergibt sich, dass f√ºr den Zustand $s^2_{1,1}$ jede Aktion au√üer clean einen maximal erwarteten Nutzen von 0 bringt. Wir setzen zuf√§llig $œÄ(s^2_{1,1})$ = move, f√ºr alle anderen Zust√§nde behalten wir die Strategie.\r\n",
    "4. In **Zeilen 9/10** w√§hlen wir eine zuf√§llige Aktion `charge` anstelle von $œÄ(s^2_{1,1})$.\r\n",
    "\r\n",
    "Dies zeigt, wie der Algorithmus die Belohnungen und Strafen ber√ºcksichtigt, die er erh√§lt, um seine Strategie anzupassen und effektiv in seiner Umgebung zu navigieren. üòä aktualisiert wurden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein Nachteil von Algorithmus 1 ist, dass in Zeile 6 die Nutzen aller ZustaÃànde (potenziell) neu berechnet werden und anschlie√üend die Strategie aktualisiert wird. Dies ist begruÃàndet in der Nutzung der adaptiven dynamischen Programmierung, bei der wir die gleiche Kritik schon in Abschnitt 4.2.2 benutzt haben, um den Ansatz des Temporal Difference Learning (TD) zu motivieren. In gleicher Weise koÃànnen wir allerdings auch den TD-Ansatz fuÃàr das aktive Reinforcement Learning erweitern. Dies werden wir im naÃàchsten Abschnitt tun.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.3. Q-Learning <a name=\"4_3_3\"></a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q-Learning ist ein Temporal Difference (TD) Ansatz f√ºr aktives Reinforcement Learning. Es unterliegt den meisten modernen Ans√§tzen zum Reinforcement Learning.\n",
    "- Im Gegensatz zu anderen Ans√§tzen <ins>lernt Q-Learning nicht die Nutzen der Zust√§nde direkt, sondern die sogenannte **Q-Funktion**</ins>.\n",
    "- Die Q-Funktion $Q_\\gamma$ weist jedem Zustand $s$ und jeder Aktion $a$ den erwarteten maximalen Nutzen zu, den man bei Verfolgung der optimalen Strategie nach Ausf√ºhrung der Aktion $a$ in $s$ erh√§lt.\n",
    "- Der Zusammenhang zwischen $Q_\\gamma$ und den Nutzen der Zust√§nde ist gegeben durch $$U_\\gamma D(s) = \\max_{a \\in A} Q_\\gamma (a,s)$$ f√ºr alle Zust√§nde $s$.\n",
    "- Wenn die wahren $Q_\\gamma$-Werte eines jeden Zustands gegeben sind, kann die optimale Strategie $\\pi^*$ leicht bestimmt werden durch $$\\pi^*(s) = \\arg\\max_{a \\in A} Q_\\gamma (a,s) \\tag{2}$$\n",
    "- Der **Vorteil** der Nutzung der $Q_\\gamma$-Funktion anstelle der Nutzenwerte ist, <ins>dass wir zur Bestimmung der optimalen Strategie kein Modell der Umgebung (insbesondere der √úbergangswahrscheinlichkeiten $P$) erlernen m√ºssen</ins>.\n",
    "- Aus diesem Grund nennt man das Q-Learning auch einen **modellfreien Ansatz** (engl. model-free method)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![q-learning-with-e-greedy](./q-learning-with-e-greedy.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Um die Q-Funktion zu erlernen, verwendet Q-Learning den Temporal Difference (TD) Ansatz, .\n",
    "- Bei jeder Beobachtung $o = (s,a,s',r)$ wird die aktuelle Sch√§tzung $q_\\gamma (a,s)$ aktualisiert.\n",
    "- Die Aktualisierung erfolgt durch eine Updateregel, die f√ºr das Q-Learning definiert ist durch: $$q_\\gamma (a,s) := q_\\gamma (a,s)+\\alpha(r +\\gamma \\max_{a' \\in A} q_\\gamma (a',s')‚àíq_\\gamma (a,s))$$\n",
    "- Hierbei ist $\\alpha \\in [0,1]$ der **Lernparameter** und $r +\\gamma \\max_{a' \\in A} q_\\gamma (a',s')$ die **Sch√§tzung des Q-Wertes** aus der gerade get√§tigten Beobachtung.\n",
    "- In der Praxis ist $\\alpha$ nicht konstant, sondern wird mit steigender Anzahl von Beobachtungen geringer.\n",
    "- Um das Exploration vs. Exploitation-Dilemma mit Q-Learning zu l√∂sen, muss der oben beschriebene Ansatz mit einer Meta-Strategie zur Exploration kombiniert werden.\n",
    "- Eine Kombination der Œµ-greedy-Strategie mit Q-Learning ist in Algorithmus 2 formalisiert.\n",
    "- Unter wenigen formalen Annahmen kann gezeigt werden, dass bei hinreichend vielen Aufrufen von Algorithmus 2 die Strategie $\\pi$ gegen die optimale Strategie konvergiert.\n",
    "- Im direkten Vergleich von Algorithmus 1 und Algorithmus 2 ist Q-Learning einfacher und ben√∂tigt weniger Speicherplatz.\n",
    "- Ein **Nachteil** des Q-Learnings gegen√ºber des Ansatzes der adaptiven dynamischen Programmierung ist jedoch, dass ersteres weitaus <ins>langsamer</ins> gegen die optimale Strategie konvergiert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Beispiel 3.** <a name=\"b3\"></a> Q-Learning f√ºr das Staubsaugerproblem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Wir f√ºhren Beispiel 2 fort. Wir nehmen dabei an, dass $\\gamma = 0.9$, $\\alpha = 0.1$ und die initiale Strategie $\\pi$ des Roboters gegeben ist durch\r\n",
    "\r\n",
    "$$\r\n",
    "\\pi(s_{1,1}^1) = \\pi(s_{1,1}^2) = \\pi(s_{0,1}^1) = \\pi(s_{0,1}^2) = \\pi(s_{1,0}^1) = \\pi(s_{1,0}^2) = \\pi(s_{0,0}^1) = \\pi(s_{0,0}^2) = \\text{move}\r\n",
    "$$\r\n",
    "\r\n",
    "Wir f√ºhren Algorithmus 2 exemplarisch zweimal aus:\r\n",
    "\r\n",
    "1. Beim ersten Aufruf befindet sich der Agent im Startzustand $s_{1,1}^1$. Zeilen 1‚Äì3 werden hierbei nicht ausgef√ºhrt. Da wir uns nicht in einem Zielzustand befinden und wir annehmen, dass der Zufallstest in den Zeilen 5/6 zugunsten von Zeile 6 ausgefallen ist, geben wir $\\pi(s_{1,1}^1) = \\text{move}$ zur√ºck.\r\n",
    "\r\n",
    "2. Beim zweiten Aufruf erh√§lt der Algorithmus die Beobachtung $o = (s_{1,1}^1, \\text{move}, s_{1,1}^2, -1)$. Mit anderen Worten, der Roboter ist erfolgreich in Raum r2 gewechselt. Wir aktualisieren in den Zeilen 2 und 3 die Werte $q_\\gamma (\\text{move}, s_{1,1}^1)$ und $\\pi(s_{1,1}^1)$ wie folgt:\r\n",
    "\r\n",
    "$$\r\n",
    "q_\\gamma (\\text{move}, s_{1,1}^1) := q_\\gamma (\\text{move}, s_{1,1}^1) + \\alpha(r + \\gamma \\max\\{q_\\gamma (\\text{move}, s_{1,1}^2), q_\\gamma (\\text{clean}, s_{1,1}^2), q_\\gamma (\\text{charge}, s_{1,1}^2)\\} - q_\\gamma (\\text{move}, s_{1,1}^1))\r\n",
    "$$\r\n",
    "\r\n",
    "$$\r\n",
    "= 0 + 0.1(-1 + 0.9 \\cdot 0 - 0) = -0.1\r\n",
    "$$\r\n",
    "\r\n",
    "und\r\n",
    "\r\n",
    "$$\r\n",
    "\\pi(s_{1,1}^1) = \\text{clean}\r\n",
    "$$\r\n",
    "\r\n",
    "Beachten Sie, dass f√ºr Bestimmung von $\\pi(s_{1,1}^1)$ oben gilt\r\n",
    "\r\n",
    "$$\r\n",
    "q_\\gamma (\\text{move}, s_{1,1}^1) = -0.1, \\quad q_\\gamma (\\text{clean}, s_{1,1}^1) = 0, \\quad q_\\gamma (\\text{charge}, s_{1,1}^1) = 0\r\n",
    "$$\r\n",
    "\r\n",
    "Die Wahl $\\pi(s_{1,1}^1) = \\text{clean}$ wurde also zuf√§llig aus den beiden maximalen Werten f√ºr clean und charge getroffen. In den Zeilen 9/10 w√§hlen wir eine zuf√§llige Aktion charge anstelle von $\\pi(s_{1,1}^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.4. Unterschiede zwischen $\\epsilon$-greedy-Learning und Q-Learning <a name=\"4_3_4\"></a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sicher, hier ist eine Tabelle, die die Unterschiede zwischen Epsilon-Greedy-Learning und Q-Learning zusammenfasst:\n",
    "\n",
    "|  | Epsilon-Greedy-Learning | Q-Learning |\n",
    "|---|---|---|\n",
    "| **Was es ist** | Eine Methode zur Aktionenauswahl | Ein spezifischer Algorithmus im Reinforcement Learning |\n",
    "| **Hauptfunktion** | Balanciert Exploration und Exploitation | Sch√§tzt den erwarteten Nutzen einer Aktion in einem bestimmten Zustand |\n",
    "| **Wie es funktioniert** | W√§hlt mit einer Wahrscheinlichkeit von Œµ eine zuf√§llige Aktion (Exploration) und mit einer Wahrscheinlichkeit von 1-Œµ die Aktion mit dem h√∂chsten erwarteten Nutzen (Exploitation) | Lernt die Q-Werte f√ºr jedes Zustands-Aktions-Paar und verwendet diese, um die optimale Strategie zu bestimmen |\n",
    "| **Zus√§tzliche Informationen** | Wird oft als Teil des Q-Learning-Prozesses verwendet | Ist ein wertbasierter, modellfreier und off-policy Algorithmus |--|\r\n",
    "| **Verwendung w√§hrend des Tests** | Epsilon-Greedy wird oft auch w√§hrend der Testphase verwendet, da im Reinforcement Learning im Gegensatz zum √ºberwachten Lernen kein unabh√§ngiger Testdatensatz vorhanden ist¬≥. | Q-Learning wird haupts√§chlich w√§hrend der Trainingsphase verwendet, um die Q-Werte zu lernen und die optimale Strategie zu bestimmen¬π. |\r\n",
    "| **Abh√§ngigkeit von der Policy** | Epsilon-Greedy ist eine Methode zur Aktionenauswahl und ist nicht direkt von der Policy abh√§ngig¬π. | Q-Learning ist ein off-policy Algorithmus. Es sch√§tzt die Belohnung f√ºr Zustands-Aktions-Paare basierend auf der optimalen (greedy) Policy, unabh√§ngig von den Aktionen des Agenten¬π. |\r\n",
    "| **Erfordernis eines Modells** | Epsilon-Greedy erfordert kein Modell der Umgebung¬π. | Q-Learning ist ein modellfreier Ansatz. Der Agent erforscht die Umgebung und lernt aus den Ergebnissen der Aktionen direkt, ohne ein internes Modell oder einen Markov-Entscheidungsprozess zu konstruieren¬π. |\r\n",
    "| **Anpassungsf√§higkeit** | Der Epsilon-Wert in Epsilon-Greedy kann im Laufe der Zeit angepasst werden, um das Gleichgewicht zwischen Exploration und Exploitation zu optimieren‚Å¥. | Q-Learning passt die Q-Werte basierend auf den erhaltenen Belohnungen an, um die optimale Strategie zuleration-function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Definition 2. Multimenge der Probel√§ufen und der Nutzen $U^{\\gamma}_{\\partial}(s | \\pi)$**\n",
    ">\n",
    "> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Beispiel 1.** <a name=\"b1\"></a> Das k-armige Banditenproblem\n",
    "\n",
    "Ein"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"up\"></a>**`[Go Up!^](#up)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sources:\n",
    "\n",
    "(1) Why does Q-Learning use epsilon-greedy during testing?. https://stats.stackexchange.com/questions/270618/why-does-q-learning-use-epsilon-greedy-during-testing.\n",
    "(2) Epsilon-Greedy Q-learning | Baeldung on Computer Science. https://www.baeldung.com/cs/epsilon-greedy-q-learning.\n",
    "(3) Epsilon and learning rate decay in epsilon greedy q learning. https://stackoverflow.com/questions/53198503/epsilon-and-learning-rate-decay-in-epsilon-greedy-q-learning.\n",
    "(4) Exploration in Q learning: Epsilon greedy vs Exploration function. https://datascience.stackexchange.com/questions/94029/exploration-in-q-learning-epsilon-greedy-vs-exploration-function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
