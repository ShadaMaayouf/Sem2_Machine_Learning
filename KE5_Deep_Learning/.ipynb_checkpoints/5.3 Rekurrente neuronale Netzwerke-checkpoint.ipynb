{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T15:05:33.584082Z",
     "start_time": "2024-01-22T15:05:33.537800Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# 5.3 Rekurrente neuronale Netzwerke\n",
    ">## <ins>Table of contents</ins> <a name=\"up\"></a>[<sup>[1]</sup>](#cite_note-1)\n",
    ">* [**5.3.1. Motivation und Grundlagen**](#5_3_1)\n",
    ">* [**5.3.2. Padding und Stride**](#5_3_2)\n",
    ">* [**5.3.3. Die Pooling-Operation**](#5_3_3)\n",
    ">* [**5.3.4. CNN-Architektur**](#5_3_4)\n",
    ">\n",
    ">## <ins>Beispiele</ins>\n",
    ">* [**Beispiel 1**:Darstellung von AND-, OR- und XOR-Funktionen mit Perzeptronen](#b1)\n",
    ">* [**Beispiel 2**: Backpropagation-Algorithmus$](#b2)\n",
    ">* [**Beispiel 3**: Q-Learning des Staubsaugerproblems](#b3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.1. Motivation und Grundlagen <a name=\"5_3_1\"></a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### **Definition:** Rekurrente neuronale Netzwerke (RNNs) \n",
    ">\n",
    ">Rekurrente neuronale Netzwerke (RNNs) sind eine spezielle Form von Künstlichen neuronalen Netzwerken, die für die Verarbeitung von Sequenzen konzipiert sind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier ein Prototypsches Problem zum Verständnis: **Die Wortvorhersage**. Die Wortvorhersage wird bei der Erstellung von Nachrichtentexten auf Smartphones verwendet.\n",
    "\n",
    "> Gegeben den Anfang eines Satzes z.B. \"Die Katze jagt die...\". Die Aufgabe des Algorithmus zur Wortvorhersage ist es, das wahrscheinlichste nächste Wort vorherzusagen.\n",
    "> $$\\text{\"Die Katze jagt die...\"} \\rightarrow \\text{\"Maus\"}$$\n",
    "> \n",
    ">Dieses Problem kann als **Klassifikationsproblem** modelliert werden:\n",
    ">\n",
    "> Jedes Wort $w_i \\in \\Sigma = \\{w_1, ..., w_n\\}$, $i = 1,...,n$, kann als ein Vektor $x_{w_i} \\in \\mathbb{R}^n$ repräsentiert werdeno:\n",
    "> $$x_{w_i} = \\begin{bmatrix}\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "0 \\\\\n",
    "\\end{bmatrix}\n",
    "\\text{← i-te Position}$$ bei dem alle Komponenten außer der i-ten 0 sind und die i-te Komponente 1 ist\n",
    "\n",
    "\n",
    "Diese Art der Kodierung nennt man **One-Hot-Kodierung** und eignet sich üblicherweise für die Verarbeitung bei neuronalen Netzwerken besser als naive Kodierungsmethoden (wie beispielsweise die Kodierung von Wörtern durch verschiedene Zahlen).\n",
    "- Die **Eingabe** des Klassifikationsalgorithmus ist eine Sequenz $x = (x^{(1)},..., x^{(m)})$ solcher Vektoren\n",
    "- und die **Ausgabe** ein Vektor $o \\in \\mathbb{R}^n$, der (im besten Fall) die Vorhersagewahrscheinlichkeit der einzelnen Wörter beinhaltet.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Funktionsweise der One-Hot-Kodierung**\n",
    ">\n",
    ">Die One-Hot-Codierung ist eine Methode zur Darstellung kategorialer Daten als binäre Vektoren. Jeder Kategorie in den Daten wird ein eindeutiger binärer Vektor zugeordnet. Hier sind die Schritte, wie die One-Hot-Codierung funktioniert:\n",
    ">\n",
    ">1. **Kategorien identifizieren**: Zuerst identifizieren Sie alle einzigartigen Kategorien in Ihren Daten. Jede Kategorie wird durch einen eigenen binären Vektor repräsentiert.\n",
    ">\n",
    ">2. **Vektorlänge festlegen**: Die Länge des Vektors ist gleich der Anzahl der einzigartigen Kategorien. Wenn Sie beispielsweise vier Kategorien haben, ist die Länge jedes Vektors vier.\n",
    ">\n",
    ">3. **Vektoren zuweisen**: Jeder Kategorie wird ein einzigartiger Vektor zugewiesen, bei dem genau ein Element 1 ist und alle anderen 0 sind. Die Position der '1' in jedem Vektor ist eindeutig für jede Kategorie.\n",
    ">\n",
    ">Zum Beispiel, wenn Sie ein Alphabet mit vier Buchstaben {'a', 'b', 'c', 'd'} haben, könnte eine mögliche One-Hot-Codierung sein:\n",
    ">\n",
    ">- a: (1, 0, 0, 0)\n",
    ">- b: (0, 1, 0, 0)\n",
    ">- c: (0, 0, 1, 0)\n",
    ">- d: (0, 0, 0, 1)\n",
    ">\n",
    ">In dieser Codierung repräsentiert jede Position im Vektor einen Buchstaben aus dem Alphabet, und eine '1' an einer bestimmten Position zeigt an, dass der entsprechende Buchstabe vorhanden ist. Alle anderen Positionen sind '0'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beispiel 1. \n",
    "\n",
    "Sei $\\sum_1 = \\{\\text{die, katze, jagt, maus, lampe}\\}$ unser Vokabular (wir ignorieren hier Groß- und Kleinschreibung).\n",
    "\n",
    "Dann kann der Teilsatz *\"Die Katze jagt die...\"* repräsentiert werden als\n",
    "\n",
    "$$x = ((1,0,0,0,0)^T ,(0,1,0,0,0)^T ,(0,0,1,0,0)^T ,(1,0,0,0,0)^T)$$\n",
    "\n",
    "und eine mögliche Ausgabe eines Klassifikationsalgorithmus wäre\n",
    "\n",
    "$$o = (0,0.1,0,0.85,0.05)^T$$\n",
    "\n",
    "und würde damit \"Maus\" die größte Wahrscheinlichkeit zuordnen (und \"Katze\" und \"Lampe\" auch kleine positive Wahrscheinlichkeiten, da diese grammatikalisch zumindest noch Sinn ergeben würden)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unser Vokabular (wir ignorieren hier Groß- und Kleinschreibung)\n",
    "vocabulary = [\"die\", \"katze\", \"jagt\", \"maus\", \"lampe\"]\n",
    "\n",
    "# Der Teilsatz \"Die Katze jagt die...\" repräsentiert als\n",
    "x = [(1,0,0,0,0), (0,1,0,0,0), (0,0,1,0,0), (1,0,0,0,0)]\n",
    "\n",
    "# Eine mögliche Ausgabe eines Klassifikationsalgorithmus\n",
    "o = (0,0.1,0,0.85,0.05)\n",
    "\n",
    "# Damit würde \"Maus\" die größte Wahrscheinlichkeit zuordnen \n",
    "# (und \"Katze\" und \"Lampe\" auch kleine positive Wahrscheinlichkeiten, \n",
    "# da diese grammatikalisch zumindest noch Sinn ergeben würden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bei der Anwendung klassischer Feedforward-Netzwerke ergibt sich das erste **Problem**:       \n",
    "   - Die **Eingabelänge**, d.h., die Anzahl der Wörter in der gegebenen Teilsequenz, ist <ins>nicht notwendigerweise fest bestimmt und auch nicht notwendigerweise beschränkt</ins>.\n",
    "   - Die bisher betrachtete Architekturen für neuronale Netzwerke verfügten über eine <ins>fixe Anzahl an Eingaben</ins>.\n",
    "   - Eine weitere Herausforderung ist die **flexible Handhabung von Lokalität**. Die Vorhersage des fünften Wortes einer Sequenz sollte konzeptuell genauso gehandhabt werden wie die Vorhersage des achten Wortes. Die Parameter des Netzwerks sollten in ähnlicher Weise wie bei den **CNNs geteilt werden**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grundidee von rekurrenten neuronalen Netzwerken\n",
    "\n",
    "- Die **Grundidee von rekurrenten neuronalen Netzwerken** ist es, die Eingabesequenz wortweise abzuarbeiten. In jedem Schritt wird:\n",
    "  -  ein Wort gelesen und verarbeitet,\n",
    "  -  eine Ausgabe produziert,\n",
    "  -  und ein \"Gedächtnis\" aktualisiert.\n",
    "- Sowohl das Gedächtnis als auch das nächste Wort werden wieder in das Netzwerk gespeist und der Prozess wird iteriert, bis die Sequenz vollständig gelesen wurde.\n",
    "\n",
    "- Um diese Idee zu formalisieren, wird das Konzept des **Berechnungsgraphen** verwendet, das die Funktionsweise von insbesondere neuronalen Netzwerken geeignet abstrahieren kann.\n",
    "\n",
    "- Abbildung 1 zeigt beispielsweise einen Berechnungsgraphen für <ins>ein einfaches dreischichtiges Feedforward-Netzwerk</ins>. Der **Eingabevektor $x$** wird zunächst mit der **Gewichtsmatrix $W(0)$** multipliziert und anschließend durch eine entsprechende **Aktivierungsfunktion** geleitet. Dies resultiert im Vektor $a(1)$, der die Aktivierungswerte der ersten Schicht enthält.\n",
    "\n",
    "![berechnungsgraph1](berechnungsgraph1.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Beachte hier, dass insbesondere von der Anzahl der Neuronen in jeder Schicht abstrahiert wird.\n",
    "\n",
    "- Abbildung 2 \n",
    "zeigt den Berechnungsgraphen<ins> eines einfachen rekurrenten neuronalen Netzwerke</ins>s .Die Eingabe führt unter Anwendung einer **Parametermatrix $U$** zu der Bestimmung eines Vektors $h$. Dieser Vektor modelliert den **versteckten Zustand** des Netzwerks (engl. hidden state). Der versteckte Zustand wird verwendet:\n",
    "    - zur Bestimmung der **Ausgabe $o$** (unter Verwendung einer weiteren Gewichtsmatrix V),\n",
    "    - und (unter Verwendung einer Gewichtsmatrix W) zur Bestimmung des nächsten versteckten Zustands des Netzwerksionen sind '0'.\n",
    "\n",
    "- Ausgehend von einem initialen Zustandsvektor $h(0)$ und dem ersten Element $x(0)$ wird der erste versteckte Zustand $h(1)$ bestimmt.\n",
    "- Eine einfache Realisierung dieser Operation besteht darin, dass die Matrizen $U$ und $W$ und die Vektoren $x(0)$ und $h(0)$ einfach jeweils miteinander konkatenieren und dann multiplizieren.\n",
    "\n",
    "- Das Symbol $\\circ$ wird für die **spaltenweise Konkatenation** zweier Matrizen und die zeilenweise Konkatenation zweier Spaltenvektoren benutzt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genauer, sind $A \\in \\mathbb{R}^{n \\times m}$ und $B \\in \\mathbb{R}^{n \\times m'}$ zwei Matrizen mit gleicher Anzahl an Zeilen, so ist $A \\circ B \\in \\mathbb{R}^{n \\times (m + m')}$ die entsprechende Konkatenation:\n",
    "\n",
    "$$\n",
    "A = \\begin{pmatrix} a_{1,1} & \\ldots & a_{1,m} \\\\ \\vdots & \\ddots & \\vdots \\\\ a_{n,1} & \\ldots & a_{n,m} \\end{pmatrix}, \\quad\n",
    "B = \\begin{pmatrix} b_{1,1} & \\ldots & b_{1,m'} \\\\ \\vdots & \\ddots & \\vdots \\\\ b_{n,1} & \\ldots & b_{n,m'} \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "A \\circ B = \\begin{pmatrix} a_{1,1} & \\ldots & a_{1,m} & b_{1,1} & \\ldots & b_{1,m'} \\\\ \\vdots & \\ddots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{n,1} & \\ldots & a_{n,m} & b_{n,1} & \\ldots & b_{n,m'} \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Sind weiterhin $v \\in \\mathbb{R}^m$ und $w \\in \\mathbb{R}^{m'}$ zwei Spaltenvektoren (potentiell unterschiedlicher Länge), so ist $v \\circ w \\in \\mathbb{R}^{m + m'}$:\n",
    "\n",
    "$$\n",
    "v = (v_1, \\ldots, v_m)^T, \\quad w = (w_1, \\ldots, w_{m'})^T, \\quad v \\circ w = (v_1, \\ldots, v_m, w_1, \\ldots, w_{m'})^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3 10 11 12]\n",
      " [ 4  5  6 13 14 15]\n",
      " [ 7  8  9 16 17 18]]\n",
      "v ◦ w = [1 2 3 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Erstellen Sie die Matrizen A und B mit Dummy-Zahlen\n",
    "A = np.array([[1, 2, 3], \n",
    "              [4, 5, 6], \n",
    "              [7, 8, 9]])\n",
    "\n",
    "B = np.array([[10, 11, 12], \n",
    "              [13, 14, 15], \n",
    "              [16, 17, 18]])\n",
    "\n",
    "# Führen Sie die Konkatenation durch\n",
    "AB = np.concatenate((A, B), axis=1)\n",
    "print(AB)\n",
    "\n",
    "# Erstellen Sie die Vektoren v und w mit Dummy-Zahlen\n",
    "v = np.array([1, 2, 3])  # v = (v1, ..., vm)^T\n",
    "w = np.array([4, 5, 6])  # w = (w1, ..., wm')^T\n",
    "\n",
    "# Führen Sie die Konkatenation durch\n",
    "vw = np.concatenate((v, w))\n",
    "print(\"v ◦ w =\", vw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vergewissern Sie sich, dass für die obigen Definition gilt $(A \\circ B)(v \\circ w) = Av + Bw$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True]\n"
     ]
    }
   ],
   "source": [
    "result1 = np.dot(AB,vw)\n",
    "result2 = np.dot(A,v) + np.dot(B,w)\n",
    "print(result1 == result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sei weiterhin $\\text{act}$ eine beliebige Aktivierungsfunktion (die bei Anwendung auf einen Vektor komponentenweise angewendet wird). Dann berechnet sich $h(1)$ durch $h(1) = \\text{act}(Ux(1) + Wh(0))$. Aus $h(1)$ berechnen wir dann zunächst die erste Ausgabe $o(1)$ via $o(1) = \\text{act}(Vh(1))$.\n",
    "\n",
    "Im Allgemeinen gilt für eine Eingabe $x = (x^{(1)}, \\ldots, x^{(m)})$:\n",
    "\n",
    "$$\n",
    "h(i) = \\text{act}(Ux^{(i)} + Wh^{(i-1)}) \\quad \\text{(1)}\n",
    "$$ <a name=hi><a>\n",
    "\n",
    "$$\n",
    "o(i) = \\text{act}(Vh^{(i)}) \\quad \\text{(2)}\n",
    "$$<a name=oi><a>\n",
    "\n",
    "für $i = 1, \\ldots, m$. Zu beachten ist, dass diese Netzwerkarchitektur mit Eingaben beliebiger Länge umgehen kann, aber eine fixe Anzahl an Parametern besitzt (in den Matrizen $U$, $V$, $W$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beispiel 2. \n",
    "\n",
    "Wir führen Beispiel 1 fort. \n",
    "\n",
    "![beispiel2](beispiel2.PNG)\n",
    "\n",
    "\n",
    "Abbildung 3 zeigt ein \"entfaltetes\" trainiertes RNN für die Wortvorhersage bei Eingabe von \"Die Katze jagt die...\", repräsentiert durch\n",
    "\n",
    "$$\n",
    "x = \\left( \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix} \\right)$$\n",
    "$$\\text{bzw.} \\qquad x = ((1,0,0,0,0)^T ,(0,1,0,0,0)^T ,(0,0,1,0,0)^T ,(1,0,0,0,0)^T)$$\n",
    "\n",
    "\n",
    "Die letzte Ausgabeschicht $o(4)$ beinhaltet dann die Vorhersage des nächsten Wortes nach der Eingabesequenz. Die Ausgabeschichten $o(1)$ bis $o(3)$ beinhalten die Vorhersagen der entsprechenden Teilsequenzen, die bei der Anwendung für die Eingabe $x$ ignoriert werden können.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Die **Parameter** eines rekurrenten neuronalen Netzwerks (RNN) werden durch den Backpropagationsalgorithmus und einen Optimierungsalgorithmus wie Stochastic Gradient Descent gelernt.\n",
    "- Ein Datensatz für das Wortvorhersageproblem $D = {X_1,...,X_k}$ ist eine Menge von Sätzen $X_i = {x^1_i,...,x_i^{m_i}}$ und alle Teilsequenzen eines jeden Satzes $X_i$ werden genutzt, die Parameter zu trainieren.\n",
    "- Das zu lernende RNN wird entsprechend der um eins verkürzten Länge eines Satzes $X_i$ \"entfaltet\", der Satz wird an die Eingabe des RNNs angelegt (bis auf das letzte Wort) und die um eins verschobene Sequenz von $X_i$ wird an der Ausgabe erwartet.\n",
    "\n",
    "- Wenn das RNN darauf ausgelegt ist, an der Ausgabe eine Wahrscheinlichkeitsverteilung über die vorherzusagenden Wörter auszugeben, bietet sich als Kostenfunktion der negative **Log-Likelihood** an.\n",
    "\n",
    "- Die Kostenfunktion $L^{log}$ für ein einzelnes Beispiel berechnet sich durch die Summe der negativen Logarithmen der Wahrscheinlichkeiten der erwarteten Wörter in der Ausgabe:\n",
    "  $$\n",
    "L^{log}(X_i, \\phi_{U,V,W}) = -\\sum_{j=1}^{m_i -1} log \\left( (\\phi_{U,V,W}(X_i)^{(j)})^T x^{j+1}_i \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Wie beim normalen Backpropagation-Algorithmus können die partiellen Ableitungen bezüglich aller Gewichte berechnet werden.\n",
    "- Es muss beachtet werden, dass die mehrfachen Verwendungen der Parameter in den Matrizen U, V und W entsprechend beachtet werden und nur einmal aktualisiert werden.\n",
    "- Diese Variante der Backpropagation nennt man auch **back-propagation through time**, da die Backpropagation rückwärts durch die Sequenz geht.\n",
    "\n",
    ">**Back-propagation vs Back-propagation through time**\n",
    ">\n",
    ">**Back-propagation** is the most widely used algorithm to train feed-forward neural networks.\n",
    ">\n",
    "> On the other hand, **Back-propagation Through Time (BPTT)** is the generalization of the back-propagation algorithm to recurrent neural networks, which are used for processing sequence data. A recurrent neural network is shown one input each timestep and predicts one output. Conceptually, BPTT works by unrolling all input timesteps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.2 Long short-term memory-Netzwerke <a name=\"5_3_2\"></a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grundidee von RNNs** ist dass die versteckten Schichten $h^{(i)}$ eine Art \"Gedächtnis\" für die Verarbeitung einer Sequenz repräsentieren.\n",
    "\n",
    "**Beispiel:**\n",
    "\n",
    "In $h^{(i)}$ wird für die Worterkennung das Geschlecht des aktuellen Subjekts gespeichert. Beim Lesen eines neuen Wortes wird das Gedächtnis entsprechend nach [Aktualisierungsregel (1)](#hi) aktualisiert.\n",
    "\n",
    "**[Aktualisierungsregel (1)](#hi)** hat zwei Hauptnachteile bei sehr langen Sequenzen:\n",
    "  1. Einfluss von weit zurückliegenden versteckten Zuständen ist gering, da wiederholt der vorherige Zustand mit der Matrix $W$ multipliziert wird $\\longrightarrow$ Bei langen Sequenzen kann das RNN Schwierigkeiten haben, weit zurückliegende Informationen zu merken, was die Wortvorhersage erschwert.\n",
    "\n",
    "     **Beispiel:** Wortvorhersage in einem Satz wie *\"Die Katze, die Anna gestern auf der Straße begegnet ist, jagt die...\"* ist schwieriger für ein normales RNN im Vergleich zu *\"Die Katze jagt die...\"*, da die Information über das Subjekt bei wiederholter Multiplikation mit W leicht vergessen wird.\n",
    "  3. Wiederholte Multiplikationen mit $W$ erschweren das Lernen via Backpropagation, da Gradienten in frühen Schichten gegen Null gehen können ([Problem des verschwindenden Gradienten aus Kapitel 5.1.4.](KE5_Deep_Learning/5.1.-Künstliche-neuronale-Netwerke.ipynb)).\n",
    "\n",
    "**Lösung $\\longrightarrow$** LSTM-Netzwerke (Long short-term memory-Netzwerke)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Definition 1.** LSTM-Netzwerke (Long short-term memory-Netzwerke)\n",
    ">\n",
    "> Long Short-Term Memory (LSTM) Netzwerke sind eine spezielle Art von rekurrenten neuronalen Netzen (RNNs), die entwickelt wurden, um Probleme mit langfristigen Abhängigkeiten in Sequenzen zu lösen.\n",
    ">\n",
    "> Im Gegensatz zu herkömmlichen RNNs, die dazu neigen, Informationen über längere Zeiträume zu “vergessen”, sind LSTM-Netzwerke in der Lage, sich an Langzeit-Abhängigkeiten und frühere Erfahrungen zu erinnern3. Dies wird durch die Verwendung von drei speziellen Toren erreicht: Ein Eingangstor (Input Gate), ein Merk- und Vergesstor (Forget Gate) und ein Ausgangstor (Output Gate)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LSTM-Netzwerke (Long short-term memory-Netzwerke) verwenden eine intelligentere Aktualisierungsregel als normale RNNs.\n",
    "- Bei LSTMs gibt es zwei Vektoren, die das \"Gedächtnis\" modellieren:\n",
    "    - den **versteckten Zustand `h`**.\n",
    "    - und den **Zellzustand `s`**.\n",
    "- Der Zellzustand `s` stellt das \"Langzeitgedächtnis\" dar und wird durch zwei Operationen modifiziert:\n",
    "    - eine **Vergessensoperation**: realisiert durch die Zwischenzustände $f$ und $\\hat{s}$,\n",
    "    - und eine **Lernoperation**: realisiert durch die Zwischenzustände $g$, $k$ und \\hat{s}.\n",
    "\n",
    "- Eine LSTM ist parametrisiert durch Gewichtsmatrizen $W^f$, $U^f$, $W^g$, $U^g$, $W^k$, $U^k$, $W^o$, $U^o$.\n",
    "- Sei $x = (x^1, ..., x^m)$ eine Sequenz und $i ∈ {1,...,m}$ und seien $h^{(i−1)}$ und $s^{(i−1)}$ die vorherigen Zustände (initial kann man diese als Nullvektoren definieren).\n",
    "Dann berechnen wir zunächst:\n",
    "\n",
    "$$f^{(i)} = h^{logit} (U^f x^{(i)} + W^f h^{(i−1)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Der Vektor $f^{(i)}$ soll steuern, was aus dem Langzeitgedächtnis `s` vergessen werden soll (auch als **forget gate** bezeichnet).\n",
    "\n",
    "$$g^{(i)} = h^{logit} (U^g x^{(i)} + W^g h^{(i−1)})$$\n",
    "$$k^{(i)} = h^{tanh} (U^k x^{(i)} + W^k h^{(i−1)})$$\n",
    "- Der Vektor $g^{(i)}$ (**input gate**) steuert, welche Informationen aus $k^{(i)}$ in das Langzeitgedächtnis aufgenommen werden sollen.\n",
    "$$q^{(i)} = h^{logit} (U^o x^{(i)} + W^o h^{(i−1)})$$\n",
    "- Der Vektor $q^{(i)}$ (**output gate**) steuert, welche Information in die Ausgabe und den nächsten versteckten Zustand $h^{(i)}$ einfließt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Die Kernidee hinter LSTMs liegt in der Definition des Zellzustands: $$s^{(i)} = f^{(i)} \\cdot s^{(i-1)} + g^{(i)} \\cdot k^{(i)}$$\n",
    "- Durch die Verwendung der Addition zur Hinzunahme neuer Informationen wird vermieden, dass bei langen Sequenzen weit zurückliegende Informationen immer weiter vergessen werden.\n",
    "- Durch die Verwendung der gates $f^{(i)}$ und $g^{(i)}$ kann zu jedem Punkt der Sequenz gesteuert werden, wie viel Information erhalten bleibt und wie viel weitergegeben wird.\n",
    "- Die Ableitung von $s^{(i)}$ nach $s^{(i-1)}$ ist einfach $f^{(i)}$ und wird nicht durch eine (potentiell hohe) Potenz einer Gewichtsmatrix modifiziert, was eine robustere Weiterleitung des Gradienten bei der Backpropagation erlaubt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.3 Weitere Architekturen und Anwendungen <a name=\"5_3_3\"></a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "s gekoppelt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Eine wichtige Anwendung für **Rekurrente Neuronale Netze (RNNs)** ist die **automatische Übersetzung**. Ein einfaches RNN, das einen Text von einer Sprache in eine andere übersetzt, besteht aus zwei Teilen:\n",
    "    - Der erste Teil liest und \"kodiert\" den Eingabetext in eine abstrakte Form mithilfe der Gewichtsmatrizen $U$ und $W^e$,\n",
    "    - der zweite Teil \"dekodiert\" die Ausgabe mithilfe der Gewichtsmatrizen $V$ und $W^d$.\n",
    "    - Dazwischen ist eine Schnittstelle via der Gewichtsmatrix $W^i$, die der innere Zustand an den zweiten Teil leitet.\n",
    "\n",
    "![rnn_A5](rnn_A5.PNG)\n",
    "\n",
    "2. **Sprach- und Schrifterkennung** sind weitere wichtige Anwendungsgebiete für RNNs. Hier werden ähnliche Architekturen wie bei der automatischen Übersetzung verwendet.\n",
    "   - die Eingabe (eine Reihe von digital repräsentierten Audioschnipseln oder Bilder von Buchstaben/Wörtern) eingelesen\n",
    "   - und im zweiten Teil entsprechend ausgegeben (beispielsweise als natürlichsprachlicher Satz)\n",
    " \n",
    " ![rnn_A6](rnn_A6.PNG)\n",
    " \n",
    "3. Eine Erweiterung der RNNs, die **bidirektionalen RNNs**, sind besonders bei solchen Anwendungen relevant. Sie verfügen über zwei versteckte Schichten, die entgegengesetzt zueinander angeordnet sind und die Eingabe rückwärts lesen, um zukünftige Informationen in die Verarbeitung einzubinden.\n",
    "\n",
    "4. Bei Anwendungen im Bildbereich, wie bei der Schrifterkennung, werden RNNs auch mit **Convolutional Neural Networks (CNNs)** kombiniert. Hier wird die Eingabe zunächst mit einem CNN \"kodiert\" und dann in der versteckten Schicht weiterverarbeitet[^5.2^].\n",
    "\n",
    "5. Bei der **automatischen Bildbeschriftung** findet ebenfalls eine Kombination von CNNs und RNNs Anwendung. Hier besteht das RNN nur aus dem zweiten Teil der in Abbildung 5 dargestellten Architektur und wird direkt mit der Ausgabe des CNNs gekoppelt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.4. Beispiel <a name=\"5_2_4\"></a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beispiel 1\n",
    "\n",
    "Sei ein Alphabet gegeben durch\n",
    "\n",
    "Σ={a,e,s,t}\n",
    "\n",
    "1. Bestimmen Sie eine One-Hot-Codierung für Σ\n",
    ". (Anwortformat '(1,2,3,4,5,6)', Vektorlänge ist selbst zu wä\n",
    "2. Wie ist demnach das Wort test\n",
    " codiert \n",
    "\n",
    "(Anwortformat '((1,2,3,4,5,6),(7,8,9))')\n",
    "Eine One-Hot-Codierung für das gegebene Alphabet Σ={a,e,s,t} könnte wie folgt aussehen:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- a: (1, 0, 0, 0)\n",
    "- e: (0, 1, 0, 0)\n",
    "- s: (0, 0, 1, 0)\n",
    "- t: (0, 0, 0, 1)\n",
    "\n",
    "In dieser Codierung repräsentiert jede Position im Vektor einen Buchstaben aus dem Alphabet, und eine '1' an einer bestimmten Position zeigt an, dass der entsprechende Buchstabe vorhanden ist. Alle anderen Positionen sind '0'. Die Länge des Vektors ist gleich der Anzahl der Elemente im Alphabet. Bitte beachten Sie, dass die Reihenfolge der Elemente im Alphabet die Codierung beeinflusst. In diesem Fall habe ich die Elemente in der Reihenfolge genommen, in der sie im Alphabet angegeben wurden. Sie können die Reihenfolge ändern, wenn Sie möchten, aber dann ändert sich auch die Codierung entspr\n",
    "\n",
    "Unter Verwendung der zuvor definierten One-Hot-Codierung für das Alphabet Σ={a,e,s,t}, wird das Wort \"test\" wie folgt codiert:\n",
    "\n",
    "- t: (0, 0, 0, 1)\n",
    "- e: (0, 1, 0, 0)\n",
    "- s: (0, 0, 1, 0)\n",
    "- t: (0, 0, 0, 1)\n",
    "\n",
    "Daher ist die Codierung des Wortes \"test\" in dem von Ihnen angegebenen Antwortformat:\n",
    "\n",
    "((0, 0, 0, 1), (0, 1, 0, 0), (0, 0, 1, 0), (0, 0, 0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Beispiel 2\n",
    "Gegeben sei das abgebildete einfache RNN, wobei\n",
    "\n",
    "$$\\sum = \\{\\text{ist,nichts,niemand}\\} = \\{(1,0,0)^T,(0,1,0)^T,(0,0,1)^T\\}$$\n",
    "$$U= ((0, 0.9, 0.9), (0.5, 0.1, 0), (0.5, 0, 0.1))$$\n",
    "$$W = ((0, 0.45, 0.45), (0.25, 0.05, 0), (0.25, 0, 0.05))$$\n",
    "$$V = ((0.5, 0, 0), (0, 0.5, 0), (0, 0, 0.5))$$\n",
    "$$h_0 = (0,1,1)^T$$\n",
    "\n",
    "und die Aktivierungsfunktion $h^{relu}$ ist. Berechnen Sie die hidden states und die Ausgabe für die Eingabe $x = \\text{'Niemand ist'} = ((0,0,1)^T,(1,0,0)^T)$. \n",
    "(Antwortformat '(1,2,3.456)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst definieren wir die Aktivierungsfunktion $h^{relu}$. Die Funktion $h^{relu}$ ist definiert als $h^{relu}(x) = max(0, x)$.\n",
    "\n",
    "Nun berechnen wir die hidden states und die Ausgabe für die Eingabe $x = \\text{'Niemand ist'} = ((0,0,1)^T,(1,0,0)^T)$ mit den gegebenen Gewichtsmatrizen und dem Anfangszustand $h_0$.\n",
    "\n",
    "1. **Berechnung von $h_1$**:\n",
    "   Wir verwenden [Gleichung 1](#hi): $$h^i = \\text{act}(Ux^{(i)} + Wh^{(i-1)})$$\n",
    "   $$h(1) = h^{relu}(Ux^{(1)} + Wh^{(0)})$$\n",
    "   Setzen wir die gegebenen Werte ein, erhalten wir:\n",
    "   $$h(1) = h^{relu}(U*(0,0,1)^T + W*(0,1,1)^T)$$\n",
    "   \n",
    "3. Berechnung von o(1):\n",
    "   Wir verwenden [Gleichung 2](#oi)$$o^i = \\text{act}(Vh^{(i)})$$\n",
    "   $$o^1 = h^{relu}(Vh^1)$$\n",
    "   Nachdem wir $h^1$ berechnet haben, können wir es in diese Gleichung einsetzen, um $o^1$ zu berechnen.\n",
    "\n",
    "4. Berechnung von $h^2$:\n",
    "   $$h^2 = h^{relu}(Ux^2 + Wh^1)$$\n",
    "   Nachdem wir $h^{1}$ berechnet haben, können wir es in diese Gleichung einsetzen, um $h^{2}$ zu berechnen.\n",
    "\n",
    "5. Berechnung von o(2):\n",
    "   $$o^2 = h^{relu}(Vh^{2})$$\n",
    "   Nachdem wir $h^{2}$ berechnet haben, können wir es in diese Gleichung einsetzen, um $o^{2}$ zu berechnen.\n",
    "\n",
    "Bitte beachten Sie, dass Sie die genauen Berechnungen durchführen müssen, um die endgültigen Werte für $h^{1}$, $o^{1}$, $h^{2}$ und $o^{2}$ zu erhalten. Ich kann Ihnen dabei helfen, wenn Sie möchten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_1:  [1.8  0.05 0.15]\n",
      "h_2:  [0.09   0.9525 0.9575]\n",
      "y_1:  [0.9   0.025 0.075]\n",
      "y_2:  [0.045   0.47625 0.47875]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def hrelu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "U = np.array([[0, 0.9, 0.9], [0.5, 0.1, 0], [0.5, 0, 0.1]])\n",
    "W = np.array([[0, 0.45, 0.45], [0.25, 0.05, 0], [0.25, 0, 0.05]])\n",
    "V = np.array([[0.5, 0, 0], [0, 0.5, 0], [0, 0, 0.5]])\n",
    "h_0 = np.array([0,1,1]).T\n",
    "x = np.array([[0,0,1],[1,0,0]]).T\n",
    "\n",
    "h_1 = hrelu(np.dot(U, x[:,0]) + np.dot(W, h_0))\n",
    "h_2 = hrelu(np.dot(U, x[:,1]) + np.dot(W, h_1))\n",
    "\n",
    "o_1 = hrelu(np.dot(V, h_1))\n",
    "o_2 = hrelu(np.dot(V, h_2))\n",
    "\n",
    "print(\"h_1: \", h_1)\n",
    "print(\"h_2: \", h_2)\n",
    "print(\"o_1: \", o_1)\n",
    "print(\"o_2: \", o_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"up\"></a>**`[Go Up!^](#up)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sources:\n",
    "\n",
    "(1) Why does Q-Learning use epsilon-greedy during testing?. https://stats.stackexchange.com/questions/270618/why-does-q-learning-use-epsilon-greedy-during-testing.\n",
    "(2) Epsilon-Greedy Q-learning | Baeldung on Computer Science. https://www.baeldung.com/cs/epsilon-greedy-q-learning.\n",
    "(3) Epsilon and learning rate decay in epsilon greedy q learning. https://stackoverflow.com/questions/53198503/epsilon-and-learning-rate-decay-in-epsilon-greedy-q-learning.\n",
    "(4) Exploration in Q learning: Epsilon greedy vs Exploration function. https://datascience.stackexchange.com/questions/94029/exploration-in-q-learning-epsilon-greedy-vs-exploration-function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
