{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6bf00e0-0feb-47ce-87ec-69d1dc66bf58",
   "metadata": {},
   "source": [
    "# 2.5. Bayes-Klassifikator\n",
    "\n",
    ">## <ins>Table of contents</ins>\n",
    ">* [**2.5.0. Vorwissen: Der Satz von Bayes**](#2_5_0)\n",
    ">* [**2.5.1. Das Maximum-Likelihood-Prinzip**](#2_5_1)\n",
    ">* [**2.5.2. Bayes-Klassifikation und lineare Regression**](#2_5_2)\n",
    ">* [**2.5.3. Naive Bayes-Klassifikation**](#2_5_3)\n",
    ">* [**2.5.4. Kontinuierliche Merkmale**](#2_5_4)\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b365f923-eedf-4bc8-8a2f-8243f1fd6399",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-21T15:30:27.955971Z",
     "end_time": "2024-01-21T15:30:51.433572Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fafcac-a07d-42f9-b847-6ba87d761d2c",
   "metadata": {},
   "source": [
    "## 2.5.0. Vorwissen: Der Satz von Bayes <a name=\"2_5_0\"></a>\n",
    "---\n",
    "\n",
    "Wir betrachten ein zweistufiges Zufallsexperiment mit zwei Ereignissen `A`und `B`.\n",
    "\n",
    "**Gegeben**: `P(B|A)`, die Wahrscheinlichkeit von B unter der Bedingung, dass A eingetreten ist.\n",
    "\n",
    "![image.png](attachment:c1ff66a3-3c47-4482-af8d-2b8ab0ebc483.png)\n",
    "\n",
    "**Gesucht**:  $P(A|B)$ ist die Wahrscheinlichkeit von A unter der Bedingung, dass B eingetreten ist.\n",
    "\n",
    "![image.png](attachment:cceba301-99c9-4d78-9522-248c56285fa1.png)\n",
    "\n",
    "Nach dem Multiplikationssatz gilt: $$P(A \\cap B) = P(B) * P(A|B)$$\n",
    "Gleichung nach `P(A|B)` auflösen: $$P(A|B) = \\frac{ P(A \\cap B)}{P(B)}$$\n",
    "\n",
    "von daher der Satz von Bayes ist: $$P(A|B) = \\frac{P(A) * P(B|A)}{P(B)}$$\n",
    "$$P(A|B) = \\frac{P(A) * P(B|A)}{P(A) * P(B|A) + P(\\overline{A}) * P(B|\\overline{A})}$$\n",
    "\n",
    "\n",
    ">**Beispiel**\n",
    ">**Eine Schülerin fährt in 70 % der Schultage mit dem Bus. In 80 % dieser Fälle kommt sie pünktlich zur Schule. Durchschnittlich kommt sie aber nur an 60 % der Schultage pünktlich an.**\n",
    ">**Heute kommt die Schülerin pünktlich zur Schule. Mit welcher Wahrscheinlichkeit hat sie den Bus benutzt?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dbda0f-d1b7-492b-a576-52275ca35d17",
   "metadata": {},
   "source": [
    "\n",
    ">Für die Ereignisse werden folgende Bezeichnungen gewählt:\n",
    ">\n",
    ">$A$: Die Schülerin fährt mit dem Bus.\n",
    ">\n",
    ">$B$: Die Schülerin kommt pünktlich an.\n",
    ">\n",
    ">Demnach gilt:\n",
    ">\n",
    ">$\\overline{A}$: Die Schülerin fährt nicht mit dem Bus.\n",
    ">\n",
    ">$\\overline{B}$: Die Schülerin kommt nicht pünktlich an.\n",
    ">\n",
    ">Eine Schülerin fährt zu 70 % mit dem Bus: $P(A) = 0.7$\n",
    ">\n",
    ">In 80 % dieser Fälle kommt sie pünktlich. $P(B|A) = 0.8$\n",
    ">\n",
    ">Durchschnittlich kommt sie zu 60 % pünktlich: $P(B) = 0.6$\n",
    ">\n",
    ">Gesucht ist die Wahrscheinlichkeit für BUS unter der Bedingung PÜNKTLICH: `P(B|A)`\n",
    ">\n",
    ">Da P(B|A) gegeben und P(A|B) gesucht ist, lösen wir die Aufgabe mit dem Satz von Bayes:\n",
    ">\n",
    ">$$P(A|B) = \\frac{P(A) * P(B|A)}{P(B)} = \\frac{0.7 * 0.8}{0.6} = 0.93 = 93.3%$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589599b7-c858-4ebc-9d88-414138e967ad",
   "metadata": {},
   "source": [
    "## 2.5.1. Das Maximum-Likelihood-Prinzip <a name=\"2_5_1\"></a>\n",
    "---\n",
    "Das fundamentale Problem des maschinellen Lernens ist ein optimales Modell `h` aus einem Hypothesenraum `H` (z.B. der Menge aller linearen Funktionen im Fall der linearen Regression) zu finden, das die Trainingsdaten `D` am besten erklärt. \n",
    "Wir suchen die wahrscheinlichste Hypothese `h`, gegeben die beobachteten Daten `D`, dies bedeutet, dass wir den Wert `P(h | D)` maximieren wollen.\n",
    "\n",
    "### Bayes-Theorem\n",
    "Mit dem Bayes-Theorem können wir den Wert `P(h | D)` annähern:\n",
    "$$(1)$$\n",
    "$$P(h | D) = \\frac{P(D | h)P(h)}{P(D)}$$ \n",
    "\n",
    "Hierbei ist:\n",
    "- `P(h | D)` ist die **a posteriori Wahrscheinlichkeit** der Hypothese.\n",
    "- `P(h)` die **a priori Wahrscheinlichkeit** der Hypothese h, oft angenommen als Gleichverteilung auf H.\n",
    "- `P(D)` ist die Wahrscheinlichkeit, dass der Datensatz D beobachtet wurde,\n",
    "- und `P(D | h)` ist die Wahrscheinlichkeit, D zu beobachten, gegeben dass D von h generiert wurde.\n",
    "\n",
    "#### - Maximum-A-Posteriori-Hypothese (MAP-Hypothese)\n",
    "Unser Ziel ist es, eine Hypothese `h*` zu finden, die die die a posteriori Wahrscheinlichkeit `P(h | D)` (1) maximiert:\n",
    "$$(2)$$\n",
    "$$h* = \\arg\\max_{h \\in H} P(h | D)$$\n",
    "In anderen Worten, unter allen Hypothesen in `H` suchen wir diejenige, die die höchste Wahrscheinlichkeit hat, gegeben die beobachteten Daten `D`.\n",
    "Diese Methode berücksichtigt sowohl die Wahrscheinlichkeit der Daten gegeben die Hypothese `P(D | h)` als auch die a priori Wahrscheinlichkeit der Hypothese `P(h)`.\n",
    "\n",
    "**Beispiel:**\n",
    "\n",
    "Angenommen, Sie haben eine Münze und möchten herausfinden, ob sie fair ist (d.h., die Wahrscheinlichkeit für Kopf `P(Kopf)` ist 0,5) oder ob sie eine Präferenz für Kopf oder Zahl hat. Sie werfen die Münze 10 Mal und erhalten 7 Mal Kopf und 3 Mal Zahl. Die MAP-Hypothese würde die a priori Annahme berücksichtigen, dass die meisten Münzen fair sind (d.h., `P(h)` ist hoch für `h=0,5`). Daher könnte die MAP-Hypothese trotz der beobachteten Daten immer noch zu dem Schluss kommen, dass die Münze fair ist.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fefa643-8d78-4fab-93f6-115c61f2e6cd",
   "metadata": {},
   "source": [
    "#### - Maximum-Likelihood-Hypothese (ML-Hypothese)\n",
    "Wenn P(h) gleichverteilt ist, können wir den Term P(h) streichen und erhalten die Maximum-Likelihood-Hypothese (ML-Hypothese). Die ML-Hypothese ist eine Hypothese `h*`, die die Wahrscheinlichkeit `P(D | h)` maximiert, also die Wahrscheinlichkeit, die Daten `D` zu beobachten, gegeben die Hypothese `h`. Dies wird durch die folgende Formel ausgedrückt:\n",
    "\n",
    "$$h* = \\arg\\max_{h \\in H} P(D | h)$$\n",
    "\n",
    ">Im Gegensatz zur MAP-Hypothese berücksichtigt die ML-Hypothese nicht die a priori Wahrscheinlichkeit der Hypothese `P(h)`. Stattdessen basiert sie nur auf der Übereinstimmung der Hypothese mit den beobachteten Daten. Dieser Ansatz ist besonders nützlich, wenn alle Hypothesen als gleich wahrscheinlich angesehen werden (P(h) ist gleichverteilt). Es ist ein weit verbreiteter Ansatz im maschinellen Lernen für das Training von Modellen und die Vorhersage von Ergebnissen.\n",
    "\n",
    "**Beispiel:**\n",
    "Im gleichen Szenario mit der Münze würde die ML-Hypothese nur die beobachteten Daten berücksichtigen und die Hypothese wählen, die die Daten am besten erklärt, ohne Rücksicht auf irgendwelche a priori Annahmen. In diesem Fall würde die ML-Hypothese zu dem Schluss kommen, dass `P(Kopf)` 0,7 ist, da dies die Wahrscheinlichkeit maximiert, die beobachteten Daten (7 Kopf und 3 Zahl) zu sehen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea04fbf8-410f-4d53-bbfd-74e1653948ed",
   "metadata": {},
   "source": [
    "## 2.5.2. Die Bayes-Klassifikation und lineare Regression <a name=\"2_5_2\"></a>\n",
    "---\n",
    "\n",
    "Es wird ein Datensatz D mit Paaren von x und y gegeben, und das Ziel ist es, Parameter $\\theta^*$ zu finden, die die Summe der quadrierten Differenzen zwischen den vorhergesagten und tatsächlichen y-Werten minimieren.\n",
    "\n",
    "Das gelernte Modell $h_{\\theta^*}$ beschreibt eine Gerade im $R^n$, die den funktionalen Zusammenhang zwischen den Merkmalen x und der Zielvariablen y modelliert. Wir nehmen an, dass der Datensatz D verrauscht ist und die Werte $y(i)$ die Form $y(i) = \\hat{y}(i) + \\epsilon(i)$ haben, wobei $\\hat{y}(i)$ der wahre Wert der Merkmalsausprägung $x(i)$ ist und $\\epsilon(i)$ der Fehler. Wir nehmen auch an, dass die Fehler normalverteilt sind.\n",
    "\n",
    "Die Wahrscheinlichkeitsdichte, den Wert $y(i)$ anstatt $\\hat{y}(i)$ zu beobachten, ist durch die Normalverteilung gegeben. Die Wahrscheinlichkeitsdichte, ein Beispiel $d(i) = (x(i), y(i))$ unter der Hypothese $\\theta$ zu beobachten, ist ebenfalls durch die Normalverteilung gegeben.\n",
    "\n",
    "Im Bayes'schen Ansatz wird eine ML-Hypothese $\\theta^*$ ausgewählt, die die Wahrscheinlichkeitsdichte maximiert. Anstatt den Likelihood zu maximieren, kann auch der Log-Likelihood maximiert werden, da der natürliche Logarithmus eine monotone Funktion ist.\n",
    "\n",
    "Gegeben ist ein Datensatz `D` mit Merkmalen `Z1` bis `Zn` und Klassen `Z`. Die Idee der Bayes-Klassifikation ist, dass die Hypothese ausgewählt wird, die angesichts der Daten am wahrscheinlichsten ist. Für einen neuen Datenpunkt `x` wählen wir die Klasse `c` aus `Z`, die aufgrund von `D` am wahrscheinlichsten ist. Der Bayes-Klassifikator `clf_Bayes_D` wird formal definiert als:\n",
    "\n",
    "$$clf_{Bayes_D}(x) = \\arg\\max_{c \\in Z} P(c | x,D)$$\n",
    "\n",
    "wobei\n",
    "\n",
    "$$P(c | x,D) = \\frac{|{(x, c) \\in D}|}{\\sum_{c' \\in Z} |{(x, c') \\in D}|}$$\n",
    "\n",
    "Einem neuen Datenpunkt `x` wird die Klasse `c` aus `Z` zugewiesen, die am häufigsten `x` in `D` zugewiesen wird.\n",
    ">**Beispiell** Wir haben einen Trainingsdatensatz `D_movies2` für ein Filmempfehlungssystem. Die Filme werden anhand der binären Merkmale \"Hat der Film einen Oskar gewonnen?\" (`Z1`) und \"Wurde der Film in Europa gedreht?\" (`Z2`) klassifiziert. Die Filme werden als \"gut\" (`Z=1`) oder \"schlecht\" (`Z=0`) eingestuft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c1ab18c8-1072-432a-af45-225a4d0e5fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Z1  Z2  Z\n",
      "0   0   1  1\n",
      "1   1   0  1\n",
      "2   0   0  0\n",
      "3   0   1  1\n",
      "4   0   1  0\n",
      "5   0   0  1\n",
      "6   0   0  0\n",
      "7   1   0  0\n",
      "8   1   0  1\n",
      "9   1   0  1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Ihr Trainingsdatensatz D_movies2\n",
    "data = np.array([\n",
    "    [0, 1, 1],\n",
    "    [1, 0, 1],\n",
    "    [0, 0, 0],\n",
    "    [0, 1, 1],\n",
    "    [0, 1, 0],\n",
    "    [0, 0, 1],\n",
    "    [0, 0, 0],\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 1],\n",
    "    [1, 0, 1]\n",
    "])\n",
    "\n",
    "# Erstellen Sie einen DataFrame aus den Daten\n",
    "D_movies2 = pd.DataFrame(data, columns=['Z1', 'Z2', 'Z'])\n",
    "print(D_movies2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85721ba6-8c46-4374-a585-8fd3f0b525c8",
   "metadata": {},
   "source": [
    ">Sie möchten nun die Wahrscheinlichkeit berechnen, dass ein neuer Datenpunkt `x* = (0,1)` (ein in Europa gedrehter Film, der keinen Oskar gewonnen hat) der Klasse 0 oder 1 zugeordnet wird.\n",
    ">Hier ist ein einfacher Python-Code, der das tut:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a356331d-c42a-409d-8068-ef3651c25100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wahrscheinlichkeit für Klasse 0: 0.3333333333333333\n",
      "Wahrscheinlichkeit für Klasse 1: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "def c_given_x_bayes(x):\n",
    "    x_bayes = np.all(D_movies2[['Z1', 'Z2']] == x, axis=1)\n",
    "\n",
    "    p_class_0 = np.mean(D_movies2[x_bayes]['Z']==0)\n",
    "    p_class_1 = np.mean(D_movies2[x_bayes]['Z']==1)\n",
    "    \n",
    "    #p_class_1 = np.mean(good_films)\n",
    "    \n",
    "    print(f\"Wahrscheinlichkeit für Klasse 0: {p_class_0}\")\n",
    "    print(f\"Wahrscheinlichkeit für Klasse 1: {p_class_1}\")\n",
    "    \n",
    "x_star = np.array([0, 1])\n",
    "c_given_x_bayes(x_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196c81ee-a76d-46a4-af92-fb349fa2c181",
   "metadata": {},
   "source": [
    "Es gibt drei Beispiele in $D_{\\text{movies2}}$, die mit $x^*$ in der Merkmalsausprägung übereinstimmen (Nr. 0, 3 und 4), zwei davon sind als 1 und eins als 0 klassifiziert. Wir erhalten also\n",
    "\n",
    "$$\n",
    "P(c = 0 | x = x^*,D = D_{\\text{movies2}}) = \\frac{1}{3}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(c = 1 | x = x^*,D = D_{\\text{movies2}}) = \\frac{2}{3}\n",
    "$$\n",
    "\n",
    "Die Klasse 1 kommt also häufiger unter der Merkmalsausprägung $x^*$ vor und deshalb erhalten wir\n",
    "\n",
    "$$\n",
    "\\text{clf}_{\\text{Bayes}}^{D_{\\text{movies2}}}(x^*) = 1\n",
    "$$\n",
    "\n",
    "Der Film $x^*$ wird also als \"gut\"\n",
    "**Problem -->**\n",
    "1. wir müssen für jede zu erwartende Merkmalsausprägung $x^∗$ Beispiele im Trainingsdatensatz D haben, um $x^∗$ überhaupt klassifizieren zu können.\n",
    "   Z.B. Für den Datensatz $D_{\\text{{movies2}}}$ können wir für den Datenpunkt $x^{**} = (1,1)$ (d.h., ein in Europa gedrehter Film, der einen Oskar gewonnen hat) keine Klassifikation vorhersagen, da alle Wahrscheinlichkeiten undefiniert sind (d.h., gleich $\\frac{0}{0}$). klassifiziert.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "86a6152d-b6f0-40b6-aeed-6213aa06679f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wahrscheinlichkeit für Klasse 0: nan\n",
      "Wahrscheinlichkeit für Klasse 1: nan\n"
     ]
    }
   ],
   "source": [
    "c_given_x_bayes(np.array([1, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd057a9-cd78-46c0-a61f-765fb7b08d8e",
   "metadata": {},
   "source": [
    "2. Ein weiteres Problem (das auch prinzipiell schon beim KNN-Algorithmus aufgetreten ist) besteht darin, dass zur Berechnung von $\\text{{clf}}_{\\text{{Bayes}}_D}(x)$ stets der gesamte Datensatz $D$ vorgehalten werden muss. Insbesondere bei großen Datensätzen kann dies signifikant zu einer langen Berechnungszeit beitragen.\n",
    "\n",
    "**Lösung -->** von der <ins>allgemeinen Bayes-Klassifikation</ins> zur <ins>Naiven Bayes-Klassifikation</ins> wechseln.\n",
    "\n",
    "## 2.5.3. Klassifikationsalgorithmus: Naive Bayes-Klassifikation <a name=\"2_5_3\"></a>\n",
    "---\n",
    "\n",
    "Der Klassifikationsalgorithmus Naive Bayes-Klassifikator wird auf Szenarien angewendet, in denen Merkmale in einem endlichen Merkmalsraum definiert sind.\n",
    "\n",
    "Es unterscheidet sich von der allgemeinen Bayes-Klassifikation darin, dass, bei der Berechnung der bedingten Wahrscheinlichkeit $P(c | x,D)$, also der Wahrscheinlichkeit einer Klasse $c$ gegeben die Merkmale $x$ und den Datensatz $D$, wird eine Unabhängigkeitsannahme gemacht. \n",
    "\n",
    "Das bedeutet, dass wir \"naiv\" annehmen, dass die Merkmale $x$ unabhängig voneinander sind, wenn wir die Wahrscheinlichkeit berechnen. In der Realität sind die Merkmale oft nicht unabhängig, aber diese Annahme vereinfacht die Berechnung und macht den Algorithmus effizienter.\n",
    "\n",
    "Die resultierende Wahrscheinlichkeit $P(c | x,D)$ ist daher eine Schätzung des wahren Wertes. Diese Schätzung kann in einigen Fällen ungenau sein, insbesondere wenn die Unabhängigkeitsannahme stark verletzt ist. Aber in vielen Anwendungsfällen funktioniert der Naive Bayes-Klassifikator trotz dieser Annahme überraschend gut.\n",
    "**Formal:**\n",
    "Gegeben sei ein Datensatz $D$ und ein neuer Datenpunkt $x \\in Z_1 \\times ... \\times Z_n$. Der Naive Bayes-Klassifikator $clf_{NaiveBayes_D}$ wird definiert als:\n",
    "\n",
    "$$clf_{NaiveBayes_D}(x) = \\arg\\max_{c \\in Z} P(c | D)P(x_1 | c,D)P(x_2 | c,D)...P(x_n | c,D)$$\n",
    "\n",
    "mit\n",
    "\n",
    "$$P(c | D) = \\frac{|{(z, c) \\in D}|}{|D|}$$\n",
    "\n",
    "$$P(x_i | c,D) = \\frac{|{(z', c) \\in D | z' = (z_1,...,z_n),z_i = x_i}|}{|{(z, c) \\in D}|} \\text{ für } i = 1,...,n$$\n",
    "\n",
    "**Beispiel 2** (Fortsetzung für Beispiel 1)\n",
    "\n",
    "Wir führen Beispiel 1 fort und betrachten wieder den Datenpunkt `x* = (0,1)` (ein in Europa gedrehter Film, der keinen Oskar erhalten hat). Zunächst erhalten wir für die Verteilung der Klassen die Wahrscheinlichkeiten\n",
    "$$P(c = 0 | D = D_{\\text{movies2}}) = \\frac{4}{10}$$\n",
    "$$P(c = 1 | D = D_{\\text{movies2}}) = \\frac{6}{10}$$\n",
    "da die Beispiele 2, 4, 6 und 7 als Klasse 0 klassifiziert sind und alle anderen Beispiele als Klasse 1 klassifiziert sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ae976643-d68a-4816-87b9-f4d2e0c2d45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wahrscheinlichkeit für Klasse 0: 0.4\n",
      "Wahrscheinlichkeit für Klasse 1: 0.6\n"
     ]
    }
   ],
   "source": [
    "bad_films = np.all(D_movies2[['Z']] == 0, axis=1)\n",
    "good_films = np.all(D_movies2[['Z']] == 1, axis=1)\n",
    "\n",
    "p_class_0 = np.mean(bad_films)\n",
    "p_class_1 = np.mean(good_films)\n",
    "\n",
    "print(f\"Wahrscheinlichkeit für Klasse 0: {p_class_0}\")\n",
    "print(f\"Wahrscheinlichkeit für Klasse 1: {p_class_1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98fbb7d-b3a9-4d6f-9b9a-026af8a40e19",
   "metadata": {},
   "source": [
    "Nun schauen wir uns die Wahrscheinlichkeitsverteilungen der einzelnen Merkmalsauspäggungen(`x=0` und `x=1`)  bzgl. der verschiedene 6\n",
    "Klass(`c=0` und `c=1`) en an. \n",
    "\n",
    "Wir erhalten für `Klasse 0` (schlechte Filme):\n",
    "$$P(x_1 = 0 | c = 0, D = D_{\\text{movies2}}) = \\frac{3}{4}$$\n",
    "$$P(x_1 = 1 | c = 0, D = D_{\\text{movies2}}) = \\frac{1}{4}$$\n",
    "$$P(x_2 = 0 | c = 0, D = D_{\\text{movies2}}) = \\frac{3}{4}$$\n",
    "$$P(x_2 = 1 | c = 0, D = D_{\\text{movies2}}) = \\frac{1}{4}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f0f7ab96-bec2-48cb-81ad-258164e28249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wahrscheinlichkeit für Z1 = 0 gegeben, dass Z = 0: 0.75\n",
      "Wahrscheinlichkeit für Z1 = 1 gegeben, dass Z = 0: 0.25\n",
      "Wahrscheinlichkeit für Z2 = 0 gegeben, dass Z = 0: 0.75\n",
      "Wahrscheinlichkeit für Z2 = 1 gegeben, dass Z = 0: 0.25\n"
     ]
    }
   ],
   "source": [
    "# Filter the DataFrame for only those rows where Z is 0 (bad films class 0)\n",
    "bad_films_Z1 = D_movies2[bad_films]['Z1']\n",
    "bad_films_Z2 = D_movies2[bad_films]['Z2']\n",
    "\n",
    "# Calculate the probability of Z1 being 0 given that Z is 0\n",
    "p_Z2_0_given_Z_0 = np.mean(bad_films_Z2 == 0)\n",
    "p_Z1_0_given_Z_0 = np.mean(bad_films_Z1 == 0)\n",
    "\n",
    "# Calculate the probability of Z1 being 0 given that Z is 0\n",
    "p_Z1_0_given_Z_1 = np.mean(good_films_Z1 == 0)\n",
    "p_Z2_0_given_Z_1 = np.mean(good_films_Z2 == 0)\n",
    "\n",
    "print(f\"Wahrscheinlichkeit für Z1 = 0 gegeben, dass Z = 0: {p_Z1_0_given_Z_0}\")\n",
    "print(f\"Wahrscheinlichkeit für Z1 = 1 gegeben, dass Z = 0: {p_Z1_1_given_Z_0}\")\n",
    "print(f\"Wahrscheinlichkeit für Z2 = 0 gegeben, dass Z = 0: {p_Z2_0_given_Z_0}\")\n",
    "print(f\"Wahrscheinlichkeit für Z2 = 1 gegeben, dass Z = 0: {p_Z2_1_given_Z_0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b72ad66-fc28-42ae-97fc-5d23ccfe0929",
   "metadata": {},
   "source": [
    "und für `Klasse 1` (gute Filme):\n",
    "$$P(x_1 = 0 | c = 1, D = D_{\\text{movies2}}) = \\frac{3}{6}$$\n",
    "$$P(x_1 = 1 | c = 1, D = D_{\\text{movies2}}) = \\frac{3}{6}$$\n",
    "$$P(x_2 = 0 | c = 1, D = D_{\\text{movies2}}) = \\frac{4}{6}$$\n",
    "$$P(x_2 = 1 | c = 1, D = D_{\\text{movies2}}) = \\frac{2}{6}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "42734ba2-782c-4654-ac5a-f9cca9594185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wahrscheinlichkeit für Z1 = 0 gegeben, dass Z = 1: 0.5\n",
      "Wahrscheinlichkeit für Z1 = 1 gegeben, dass Z = 1: 0.5\n",
      "Wahrscheinlichkeit für Z2 = 0 gegeben, dass Z = 1: 0.6666666666666666\n",
      "Wahrscheinlichkeit für Z2 = 1 gegeben, dass Z = 1: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Filter the DataFrame for only those rows where Z is 1 (good films = class 1)\n",
    "good_films_Z1 = D_movies2[good_films]['Z1']\n",
    "good_films_Z2 = D_movies2[good_films]['Z2']\n",
    "\n",
    "# Calculate the probability of Z2 being 0 given that Z is 0\n",
    "p_Z1_1_given_Z_0 = np.mean(bad_films_Z1 == 1)\n",
    "p_Z2_1_given_Z_0 = np.mean(bad_films_Z2 == 1)\n",
    "\n",
    "# Calculate the probability of Z2 being 0 given that Z is 0\n",
    "p_Z1_1_given_Z_1 = np.mean(good_films_Z1 == 1)\n",
    "p_Z2_1_given_Z_1 = np.mean(good_films_Z2 == 1)\n",
    "\n",
    "print(f\"Wahrscheinlichkeit für Z1 = 0 gegeben, dass Z = 1: {p_Z1_0_given_Z_1}\")\n",
    "print(f\"Wahrscheinlichkeit für Z1 = 1 gegeben, dass Z = 1: {p_Z1_1_given_Z_1}\")\n",
    "print(f\"Wahrscheinlichkeit für Z2 = 0 gegeben, dass Z = 1: {p_Z2_0_given_Z_1}\")\n",
    "print(f\"Wahrscheinlichkeit für Z2 = 1 gegeben, dass Z = 1: {p_Z2_1_given_Z_1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e5b124-edde-4b34-a39c-63c3c45aac1f",
   "metadata": {},
   "source": [
    "Damit gelten für ein Datenpunkt `x* = (0,1)` die folgenden Wahrscheinlichkeiten :\n",
    "\n",
    "$$\n",
    "P(c = 0 | D = D_{\\text{movies2}})P(x_1 = 0 | c = 0,D = D_{\\text{movies2}})P(x_2 = 1 | c = 0,D = D_{\\text{movies2}}) = \\frac{4}{10} \\cdot \\frac{3}{4} \\cdot \\frac{1}{4} = \\frac{3}{40}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(c = 1 | D = D_{\\text{movies2}})P(x_1 = 0 | c = 1,D = D_{\\text{movies2}})P(x_2 = 1 | c = 1,D = D_{\\text{movies2}}) = \\frac{6}{10} \\cdot \\frac{3}{6} \\cdot \\frac{2}{6} = \\frac{1}{10}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2cc4b054-9c90-4651-a083-141281bcdb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wahrscheinlichkeit für Klasse 0 gegeben x* = [0 1]: 0.07500000000000001\n",
      "Wahrscheinlichkeit für Klasse 1 gegeben x* = [0 1]: 0.09999999999999999\n"
     ]
    }
   ],
   "source": [
    "def c_given_x_bayes(x):\n",
    "    # Wahrscheinlichkeit für Klasse 0 (schlechte Filme)\n",
    "    p_c_0_given_x_star = p_class_0 * (bad_films_Z1 == x[0]).mean() * (bad_films_Z2 == x[1]).mean()\n",
    "    # Wahrscheinlichkeit für Klasse 1 (gute Filme)\n",
    "    p_c_1_given_x_star = p_class_1 * (good_films_Z1 == x[0]).mean() * (good_films_Z2 == x[1]).mean()\n",
    "    \n",
    "    # Ausgabe der berechneten Wahrscheinlichkeiten\n",
    "    print(f\"Wahrscheinlichkeit für Klasse 0 gegeben x* = {x}: {p_c_0_given_x_star}\")\n",
    "    print(f\"Wahrscheinlichkeit für Klasse 1 gegeben x* = {x}: {p_c_1_given_x_star}\")\n",
    "    \n",
    "# Datenpunkt x* = (0,1)\n",
    "x_star = np.array([0, 1])\n",
    "c_given_x_bayes(x_star)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f0c536-7b35-405c-ad4e-22e6c7acbdef",
   "metadata": {},
   "source": [
    "Wie man sieht, tritt die Klasse 1 häufiger unter der Merkmalsausprägung $x^*$ auf und deshalb erhalten wir\n",
    "$$\n",
    "\\text{clf}_{D_{\\text{movies2}}}^{\\text{Naive Bayes}}(x^*) = 1\n",
    "$$\n",
    "Der Film $x^*$ wird also als \"gut\" klassifiziert.\n",
    "\n",
    "Betrachten wir nun den neuen Datenpunkt x∗∗ = (1,1) (d. h., ein in Europa gedrehter Film, der einen Oskar gewonnen hat), so erhalten wir:\n",
    "$$\n",
    "P(c = 0 | D = D_{\\text{movies2}})P(x_1 = 1 | c = 0,D = D_{\\text{movies2}})P(x_2 = 1 | c = 0,D = D_{\\text{movies2}}) = \\frac{4}{10} \\cdot \\frac{1}{4} \\cdot \\frac{1}{4} = \\frac{1}{40}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(c = 1 | D = D_{\\text{movies2}})P(x_1 = 1 | c = 1,D = D_{\\text{movies2}})P(x_2 = 1 | c = 1,D = D_{\\text{movies2}}) = \\frac{6}{10} \\cdot \\frac{3}{6} \\cdot \\frac{2}{6} = \\frac{1}{10}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f7092240-4e80-4d2d-80b7-5227e7bdff22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wahrscheinlichkeit für Klasse 0 gegeben x* = [1 1]: 0.025\n",
      "Wahrscheinlichkeit für Klasse 1 gegeben x* = [1 1]: 0.09999999999999999\n"
     ]
    }
   ],
   "source": [
    "# Datenpunkt x* = (1,1)\n",
    "c_given_x_bayes(np.array([1, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669399f5-75d9-43ec-8607-00a905579abe",
   "metadata": {},
   "source": [
    "und damit ist $$\n",
    "\\text{clf}_{D_{\\text{movies2}}}^{\\text{Naive Bayes}}(x^{**}) = 1\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37daaa6e-87aa-4312-8b41-0784cb6ddbbc",
   "metadata": {},
   "source": [
    "#### Der Kompakte Code mit `sklearn.naive_bayes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8e150c53-224c-4c21-992a-f7654e679b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 1 1 0 0 1 1 1]\n",
      "Modellgenauigkeit: 0.7\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Teilen Sie die Daten in Merkmale X und Klassenlabels y auf\n",
    "#X = data[:, :2]  # Z1 und Z2 sind die Merkmale\n",
    "#y = data[:, 2]   # Z ist das Klassenlabel\n",
    "X = D_movies2.iloc[:, :2].values\n",
    "y = D_movies2.iloc[:, 2].values\n",
    "\n",
    "# Erstellen Sie ein Naive Bayes Modell\n",
    "clf = GaussianNB().fit(X, y)\n",
    "\n",
    "# Machen Sie Vorhersagen mit den Daten\n",
    "y_pred = clf.predict(X)\n",
    "print(y_pred)\n",
    "\n",
    "# Berechnen Sie die Genauigkeit des Modells\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "\n",
    "print(f\"Modellgenauigkeit: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "71d6a96d-0ccb-4b1c-bce6-9d14d2815d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "Vorhersage für x* = [[0 1]]: 1\n",
      "[1]\n",
      "Vorhersage für x* = [[1 1]]: 1\n"
     ]
    }
   ],
   "source": [
    "def c_given_x_compact_bayes(clf, x):\n",
    "    # Vorhersage für x*\n",
    "    y_pred_x = clf.predict(x)\n",
    "    print(y_pred_x)\n",
    "    print(f\"Vorhersage für x* = {x}: {y_pred_x[0]}\")\n",
    "    \n",
    "# Der neue Datenpunkt x*\n",
    "x_star = np.array([[0, 1]])\n",
    "x_double_star = np.array([[1, 1]])\n",
    "\n",
    "c_given_x_compact_bayes(clf, x_star)\n",
    "c_given_x_compact_bayes(clf, x_double_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426b52fc-3023-46b8-9e4c-5e5f485ed172",
   "metadata": {},
   "source": [
    "## 2.5.4. Kontinuierliche Merkmale <a name=\"2_5_4\"></a>\n",
    "---\n",
    "Der Naive Bayes-Klassifikator kann auch auf kontinuierliche Merkmale angewendet werden, indem wir von einer Wahrscheinlichkeitsverteilung zu einer Wahrscheinlichkeitsdichte wechseln. Anstatt die Verteilung $$P(x_i | c,D)$$ direkt aus den Daten zu berechnen, verwenden wir eine abgeschätzte Dichte $$p(x_i | c,D)$$. Die Herausforderung besteht darin, diese Dichte geeignet abzuschätzen, da zusätzliche Annahmen erforderlich sind.\n",
    "\n",
    "Betrachten Sie den Datensatz `D_kont`. Hier haben wir ein kontinuierliches Merkmal `Z1` mit Merkmalsraum $$\\mathbb{R}$$. Eine genaue Inspektion der Verteilung der Merkmalsausprägungen legt nahe, dass `x1` für die Klasse 0 normalverteilt mit Erwartungswert 2 und für die Klasse 1 normalverteilt mit Erwartungswert 4 is\n",
    "| 10 | 3.8 | 1 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "57118195-5040-4c7a-86a8-432b37595d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "Vorhersage für x* = [[2.5]]: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maayo\\PycharmProjects\\Sem2_ML\\venv\\Lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but GaussianNB was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Datensatz D_kont als DataFrame\n",
    "D_kont = pd.DataFrame({\n",
    "    'Z1': [4.0, 2.2, 3.9, 4.0, 1.9, 4.1, 2.0, 1.9, 2.0, 3.8],\n",
    "    'Z': [1, 0, 1, 1, 0, 1, 0, 0, 0, 1]\n",
    "})\n",
    "\n",
    "# Aufteilen des Datensatzes in Merkmale und Zielvariable\n",
    "X = D_kont[['Z1']]   # Merkmale (Z1)\n",
    "y = D_kont['Z']  # Zielvariable (Z)\n",
    "\n",
    "# Erstellen des Naiven Bayes-Klassifikators\n",
    "clf = GaussianNB().fit(X, y)\n",
    "\n",
    "# Vorhersage für einen gegebenen Datenpunkt x*\n",
    "x_star = np.array([[2.5]])  # z.B. x* = 2.5\n",
    "c_given_x_compact_bayes(clf, x_star)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12816fb-9414-4a2c-87da-1b9f005a5727",
   "metadata": {},
   "source": [
    "## 2.5.5.Zusammenfassung <a name=\"2_5_5\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0194224d-4629-4d29-a9c4-b5206d6df605",
   "metadata": {},
   "source": [
    "Der folgende Code verwendet den Naiven Bayes-Klassifikator, um einen synthetischen Klassifikationsdatensatz zu modellieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67078150",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.2438174   2.64178576 -2.44433124 ...  2.35845448  0.02988481\n",
      "   0.02296293]\n",
      " [-0.42104111 -0.75854682 -0.02426976 ... -1.20398791 -2.30845398\n",
      "   0.55613795]\n",
      " [-0.5688051  -0.73101635  0.40454672 ... -0.8498266  -0.96568188\n",
      "   0.7168087 ]\n",
      " ...\n",
      " [ 0.49135006  1.5432398  -0.71724746 ...  1.89329607 -2.20603017\n",
      "  -1.19670352]\n",
      " [-1.32489877 -0.98367453  0.17375312 ... -1.41242742 -0.67473117\n",
      "  -1.03197684]\n",
      " [ 0.74248278 -0.13160448  1.28153067 ...  0.72390933  3.48271338\n",
      "  -0.13537485]]\n",
      "[0 0 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1\n",
      " 0 1 1 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0\n",
      " 1 0 0 1 1 0 0 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Diese Zeile erzeugt einen zufälligen Klassifikationsdatensatz mit der Funktion `make_classification` aus der `sklearn.datasets` Bibliothek. `X` enthält die Merkmale und `y` enthält die Klassenlabels.\n",
    "X, y = make_classification()\n",
    "\n",
    "print(X)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7992bd9f-e05f-4403-9f12-425ab8f85bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 0, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hier wird ein Naiver Bayes-Klassifikator mit einer Gaußschen Verteilungsannahme erstellt und auf den Datensatz angewendet.\n",
    "clf = GaussianNB().fit(X,y)\n",
    "\n",
    "y_pred = clf.predict(X)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce6509c6-bdaa-4c6d-afd6-9081ca5527b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "metrics.accuracy_score(y,clf.predict(X))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
