{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6bf00e0-0feb-47ce-87ec-69d1dc66bf58",
   "metadata": {},
   "source": [
    "# 2.5. Bayes-Klassifikator\n",
    "\n",
    ">## <ins>Table of contents</ins>\n",
    ">* [**2.5.0. Vorwissen: Der Satz von Bayes**](#2_5_0)\n",
    ">* [**2.5.1. Das Maximum-Likelihood-Prinzip**](#2_5_1)\n",
    ">* [**2.5.2. Bayes-Klassifikation und lineare Regression**](#2_5_2)\n",
    ">* [**2.5.3. Naive Bayes-Klassifikation**](#2_5_3)\n",
    ">* [**2.5.4. Kontinuierliche Merkmale**](#2_5_4)\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b365f923-eedf-4bc8-8a2f-8243f1fd6399",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-19T12:36:29.464174Z",
     "start_time": "2024-01-19T12:36:19.878915Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fafcac-a07d-42f9-b847-6ba87d761d2c",
   "metadata": {},
   "source": [
    "## 2.5.0. Vorwissen: Der Satz von Bayes <a name=\"2_5_0\"></a>\n",
    "---\n",
    "\n",
    "Wir betrachten ein zweistufiges Zufallsexperiment mit zwei Ereignissen `A`und `B`.\n",
    "\n",
    "**Gegeben**: `P(B|A)`, die Wahrscheinlichkeit von B unter der Bedingung, dass A eingetreten ist.\n",
    "\n",
    "![image.png](attachment:c1ff66a3-3c47-4482-af8d-2b8ab0ebc483.png)\n",
    "\n",
    "**Gesucht**:  $P(A|B)$ ist die Wahrscheinlichkeit von A unter der Bedingung, dass B eingetreten ist.\n",
    "\n",
    "![image.png](attachment:cceba301-99c9-4d78-9522-248c56285fa1.png)\n",
    "\n",
    "Nach dem Multiplikationssatz gilt: $$P(A \\cap B) = P(B) * P(A|B)$$\n",
    "Gleichung nach `P(A|B)` auflösen: $$P(A|B) = \\frac{ P(A \\cap B)}{P(B)}$$\n",
    "\n",
    "von daher der Satz von Bayes ist: $$P(A|B) = \\frac{P(A) * P(B|A)}{P(B)}$$\n",
    "$$P(A|B) = \\frac{P(A) * P(B|A)}{P(A) * P(B|A) + P(\\overline{A}) * P(B|\\overline{A})}$$\n",
    "\n",
    "\n",
    ">**Beispiel**\n",
    ">**Eine Schülerin fährt in 70 % der Schultage mit dem Bus. In 80 % dieser Fälle kommt sie pünktlich zur Schule. Durchschnittlich kommt sie aber nur an 60 % der Schultage pünktlich an.**\n",
    ">**Heute kommt die Schülerin pünktlich zur Schule. Mit welcher Wahrscheinlichkeit hat sie den Bus benutzt?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dbda0f-d1b7-492b-a576-52275ca35d17",
   "metadata": {},
   "source": [
    "\n",
    ">Für die Ereignisse werden folgende Bezeichnungen gewählt:\n",
    ">\n",
    ">$A$: Die Schülerin fährt mit dem Bus.\n",
    ">\n",
    ">$B$: Die Schülerin kommt pünktlich an.\n",
    ">\n",
    ">Demnach gilt:\n",
    ">\n",
    ">$\\overline{A}$: Die Schülerin fährt nicht mit dem Bus.\n",
    ">\n",
    ">$\\overline{B}$: Die Schülerin kommt nicht pünktlich an.\n",
    ">\n",
    ">Eine Schülerin fährt zu 70 % mit dem Bus: $P(A) = 0.7$\n",
    ">\n",
    ">In 80 % dieser Fälle kommt sie pünktlich. $P(B|A) = 0.8$\n",
    ">\n",
    ">Durchschnittlich kommt sie zu 60 % pünktlich: $P(B) = 0.6$\n",
    ">\n",
    ">Gesucht ist die Wahrscheinlichkeit für BUS unter der Bedingung PÜNKTLICH: `P(B|A)`\n",
    ">\n",
    ">Da P(B|A) gegeben und P(A|B) gesucht ist, lösen wir die Aufgabe mit dem Satz von Bayes:\n",
    ">\n",
    ">$$P(A|B) = \\frac{P(A) * P(B|A)}{P(B)} = \\frac{0.7 * 0.8}{0.6} = 0.93 = 93.3%$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589599b7-c858-4ebc-9d88-414138e967ad",
   "metadata": {},
   "source": [
    "## 2.5.1. Das Maximum-Likelihood-Prinzip <a name=\"2_5_1\"></a>\n",
    "---\n",
    "Das fundamentale Problem des maschinellen Lernens ist ein optimales Modell `h` aus einem Hypothesenraum `H` (z.B. der Menge aller linearen Funktionen im Fall der linearen Regression) zu finden, das die Trainingsdaten `D` am besten erklärt. \n",
    "Wir suchen die wahrscheinlichste Hypothese `h`, gegeben die beobachteten Daten `D`, dies bedeutet, dass wir den Wert `P(h | D)` maximieren wollen.\n",
    "\n",
    "### Bayes-Theorem\n",
    "Mit dem Bayes-Theorem können wir den Wert `P(h | D)` annähern:\n",
    "$$(1)$$\n",
    "$$P(h | D) = \\frac{P(D | h)P(h)}{P(D)}$$ \n",
    "\n",
    "Hierbei ist:\n",
    "- `P(h | D)` ist die **a posteriori Wahrscheinlichkeit** der Hypothese.\n",
    "- `P(h)` die **a priori Wahrscheinlichkeit** der Hypothese h, oft angenommen als Gleichverteilung auf H.\n",
    "- `P(D)` ist die Wahrscheinlichkeit, dass der Datensatz D beobachtet wurde,\n",
    "- und `P(D | h)` ist die Wahrscheinlichkeit, D zu beobachten, gegeben dass D von h generiert wurde.\n",
    "\n",
    "#### - Maximum-A-Posteriori-Hypothese (MAP-Hypothese)\n",
    "Unser Ziel ist es, eine Hypothese `h*` zu finden, die die die a posteriori Wahrscheinlichkeit `P(h | D)` (1) maximiert:\n",
    "$$(2)$$\n",
    "$$h* = \\arg\\max_{h \\in H} P(h | D)$$\n",
    "In anderen Worten, unter allen Hypothesen in `H` suchen wir diejenige, die die höchste Wahrscheinlichkeit hat, gegeben die beobachteten Daten `D`.\n",
    "Diese Methode berücksichtigt sowohl die Wahrscheinlichkeit der Daten gegeben die Hypothese `P(D | h)` als auch die a priori Wahrscheinlichkeit der Hypothese `P(h)`.\n",
    "\n",
    "**Beispiel:**\n",
    "\n",
    "Angenommen, Sie haben eine Münze und möchten herausfinden, ob sie fair ist (d.h., die Wahrscheinlichkeit für Kopf `P(Kopf)` ist 0,5) oder ob sie eine Präferenz für Kopf oder Zahl hat. Sie werfen die Münze 10 Mal und erhalten 7 Mal Kopf und 3 Mal Zahl. Die MAP-Hypothese würde die a priori Annahme berücksichtigen, dass die meisten Münzen fair sind (d.h., `P(h)` ist hoch für `h=0,5`). Daher könnte die MAP-Hypothese trotz der beobachteten Daten immer noch zu dem Schluss kommen, dass die Münze fair ist.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fefa643-8d78-4fab-93f6-115c61f2e6cd",
   "metadata": {},
   "source": [
    "#### - Maximum-Likelihood-Hypothese (ML-Hypothese)\n",
    "Wenn P(h) gleichverteilt ist, können wir den Term P(h) streichen und erhalten die Maximum-Likelihood-Hypothese (ML-Hypothese). Die ML-Hypothese ist eine Hypothese `h*`, die die Wahrscheinlichkeit `P(D | h)` maximiert, also die Wahrscheinlichkeit, die Daten `D` zu beobachten, gegeben die Hypothese `h`. Dies wird durch die folgende Formel ausgedrückt:\n",
    "\n",
    "$$h* = \\arg\\max_{h \\in H} P(D | h)$$\n",
    "\n",
    ">Im Gegensatz zur MAP-Hypothese berücksichtigt die ML-Hypothese nicht die a priori Wahrscheinlichkeit der Hypothese `P(h)`. Stattdessen basiert sie nur auf der Übereinstimmung der Hypothese mit den beobachteten Daten. Dieser Ansatz ist besonders nützlich, wenn alle Hypothesen als gleich wahrscheinlich angesehen werden (P(h) ist gleichverteilt). Es ist ein weit verbreiteter Ansatz im maschinellen Lernen für das Training von Modellen und die Vorhersage von Ergebnissen.\n",
    "\n",
    "**Beispiel:**\n",
    "Im gleichen Szenario mit der Münze würde die ML-Hypothese nur die beobachteten Daten berücksichtigen und die Hypothese wählen, die die Daten am besten erklärt, ohne Rücksicht auf irgendwelche a priori Annahmen. In diesem Fall würde die ML-Hypothese zu dem Schluss kommen, dass `P(Kopf)` 0,7 ist, da dies die Wahrscheinlichkeit maximiert, die beobachteten Daten (7 Kopf und 3 Zahl) zu sehen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d90745e7-77fa-4078-9c9f-07a32f139984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML-Hypothese: P(Kopf) = 0.0\n",
      "MAP-Hypothese: P(Kopf) = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maayo\\AppData\\Local\\Temp\\ipykernel_15760\\1393145229.py:23: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior_ml /= np.sum(posterior_ml)\n",
      "C:\\Users\\maayo\\AppData\\Local\\Temp\\ipykernel_15760\\1393145229.py:24: RuntimeWarning: invalid value encountered in divide\n",
      "  posterior_map /= np.sum(posterior_map)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import bernoulli\n",
    "\n",
    "# Anzahl der Würfe und Anzahl der \"Kopf\"-Ergebnisse\n",
    "n_wuerfe = 10\n",
    "n_kopf = 7\n",
    "\n",
    "# Mögliche Werte für die Wahrscheinlichkeit von \"Kopf\"\n",
    "p_values = np.linspace(0, 1, 100)\n",
    "\n",
    "# Prior-Verteilung (Gleichverteilung für ML, leicht zugunsten von 0.5 für MAP)\n",
    "prior_ml = np.ones_like(p_values) / len(p_values)\n",
    "prior_map = bernoulli.pmf(p_values, 0.5)\n",
    "\n",
    "# Likelihood-Funktion\n",
    "likelihood = bernoulli.pmf(n_kopf/n_wuerfe, p_values)\n",
    "\n",
    "# Posterior-Verteilung\n",
    "posterior_ml = likelihood * prior_ml\n",
    "posterior_map = likelihood * prior_map\n",
    "\n",
    "# Normalisierung der Posterior-Verteilungen\n",
    "posterior_ml /= np.sum(posterior_ml)\n",
    "posterior_map /= np.sum(posterior_map)\n",
    "\n",
    "# Bestimmung der ML- und MAP-Hypothesen\n",
    "ml_hypothesis = p_values[np.argmax(posterior_ml)]\n",
    "map_hypothesis = p_values[np.argmax(posterior_map)]\n",
    "\n",
    "print(f\"ML-Hypothese: P(Kopf) = {ml_hypothesis}\")\n",
    "print(f\"MAP-Hypothese: P(Kopf) = {map_hypothesis}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea04fbf8-410f-4d53-bbfd-74e1653948ed",
   "metadata": {},
   "source": [
    "## 2.5.2. Die Bayes-Klassifikation und lineare Regression\n",
    "---\n",
    "\n",
    "Es wird ein Datensatz D mit Paaren von x und y gegeben, und das Ziel ist es, Parameter $\\theta^*$ zu finden, die die Summe der quadrierten Differenzen zwischen den vorhergesagten und tatsächlichen y-Werten minimieren.\n",
    "\n",
    "Das gelernte Modell $h_{\\theta^*}$ beschreibt eine Gerade im $R^n$, die den funktionalen Zusammenhang zwischen den Merkmalen x und der Zielvariablen y modelliert. Wir nehmen an, dass der Datensatz D verrauscht ist und die Werte $y(i)$ die Form $y(i) = \\hat{y}(i) + \\epsilon(i)$ haben, wobei $\\hat{y}(i)$ der wahre Wert der Merkmalsausprägung $x(i)$ ist und $\\epsilon(i)$ der Fehler. Wir nehmen auch an, dass die Fehler normalverteilt sind.\n",
    "\n",
    "Die Wahrscheinlichkeitsdichte, den Wert $y(i)$ anstatt $\\hat{y}(i)$ zu beobachten, ist durch die Normalverteilung gegeben. Die Wahrscheinlichkeitsdichte, ein Beispiel $d(i) = (x(i), y(i))$ unter der Hypothese $\\theta$ zu beobachten, ist ebenfalls durch die Normalverteilung gegeben.\n",
    "\n",
    "Im Bayes'schen Ansatz wird eine ML-Hypothese $\\theta^*$ ausgewählt, die die Wahrscheinlichkeitsdichte maximiert. Anstatt den Likelihood zu maximieren, kann auch der Log-Likelihood maximiert werden, da der natürliche Logarithmus eine monotone Funktion ist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cedbfc9-0ad1-4c50-ac58-84571e505187",
   "metadata": {},
   "source": [
    "Gegeben ist ein Datensatz `D` mit Merkmalen `Z1` bis `Zn` und Klassen `Z`. Die Idee der Bayes-Klassifikation ist, dass die Hypothese ausgewählt wird, die angesichts der Daten am wahrscheinlichsten ist. Für einen neuen Datenpunkt `x` wählen wir die Klasse `c` aus `Z`, die aufgrund von `D` am wahrscheinlichsten ist. Der Bayes-Klassifikator `clf_Bayes_D` wird formal definiert als:\r\n",
    "\r\n",
    "$$clf_{Bayes_D}(x) = \\arg\\max_{c \\in Z} P(c | x,D)$$\r\n",
    "\r\n",
    "wobei\r\n",
    "\r\n",
    "$$P(c | x,D) = \\frac{|{(x, c) \\in D}|}{\\sum_{c' \\in Z} |{(x, c') \\in D}|}$$\r\n",
    "\r\n",
    "Einem neuen Datenpunkt `x` wird die Klasse `c` aus `Z` zugewiesen, die am häufigsten `x` in `D` zugewies\n",
    "en wird.\n",
    ">**BeispieWirBeispiaben Sie einen Trainingsdatensatz `D_movies2` für ein Filmempfehlungssystem. Die Filme werden anhand der binären Merkmale \"Hat der Film einen Oskar gewonnen?\" (`Z1`) und \"Wurde der Film in Europa gedreht?\" (`Z2`) klassifiziert. Die Filme werden als \"gut\" (`Z=1`) oder \"schlecht\" (`Z=0`) eingestuft.\n",
    ">Sie möchten nun die Wahrscheinlichkeit berechnen, dass ein neuer Datenpunkt `x* = (0,1)` (ein in Europa gedrehter Film, der keinen Oskar gewonnen hat) der Klasse 0 oder 1 zugeordnet wird.\n",
    ">Hier ist ein einfacher Python-Code, der das tut:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1ab18c8-1072-432a-af45-225a4d0e5fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Z1  Z2  Z\n",
      "0   0   1  1\n",
      "1   1   0  1\n",
      "2   0   0  0\n",
      "3   0   1  1\n",
      "4   0   1  0\n",
      "5   0   0  1\n",
      "6   0   0  0\n",
      "7   1   0  0\n",
      "8   1   0  1\n",
      "9   1   0  1\n",
      "bad films\n",
      "0    False\n",
      "1    False\n",
      "2     True\n",
      "3    False\n",
      "4     True\n",
      "5    False\n",
      "6     True\n",
      "7     True\n",
      "8    False\n",
      "9    False\n",
      "dtype: bool\n",
      "good films\n",
      "0     True\n",
      "1     True\n",
      "2    False\n",
      "3     True\n",
      "4    False\n",
      "5     True\n",
      "6    False\n",
      "7    False\n",
      "8     True\n",
      "9     True\n",
      "dtype: bool\n",
      "Wahrscheinlichkeit für Klasse 0: 0.4\n",
      "Wahrscheinlichkeit für Klasse 1: 0.6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Ihr Trainingsdatensatz D_movies2\n",
    "data = np.array([\n",
    "    [0, 1, 1],\n",
    "    [1, 0, 1],\n",
    "    [0, 0, 0],\n",
    "    [0, 1, 1],\n",
    "    [0, 1, 0],\n",
    "    [0, 0, 1],\n",
    "    [0, 0, 0],\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 1],\n",
    "    [1, 0, 1]\n",
    "])\n",
    "\n",
    "# Erstellen Sie einen DataFrame aus den Daten\n",
    "D_movies2 = pd.DataFrame(data, columns=['Z1', 'Z2', 'Z'])\n",
    "print(D_movies2)\n",
    "\n",
    "bad_films = np.all(D_movies2[['Z']] == 0, axis=1)\n",
    "good_films = np.all(D_movies2[['Z']] == 1, axis=1)\n",
    "\n",
    "p_class_0 = np.mean(bad_films)\n",
    "p_class_1 = np.mean(good_films)\n",
    "\n",
    "print(f\"Wahrscheinlichkeit für Klasse 0: {p_class_0}\")\n",
    "print(f\"Wahrscheinlichkeit für Klasse 1: {p_class_1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e489193-b0d4-4e69-aca0-687fb2607197",
   "metadata": {},
   "source": [
    "Zunächst erhalten wir für die Verteilung der Klassen die Wahrscheinlichkeiten\n",
    "$$P(c = 0 | D = D_{\\text{movies2}}) = \\frac{4}{10}$$\n",
    "$$P(c = 1 | D = D_{\\text{movies2}}) = \\frac{6}{10}$$\n",
    "da die Beispiele 2, 4, 6 und 7 als Klasse 0 klassifiziert sind und alle anderen Beispiele als Klasse 1 klassifiziert sind.\n",
    "\n",
    "Nun \n",
    "schauen wir uns die Wahrscheinlichkeitsverteilungen der einzelnen Merkmalsauspäggungen(`x=0` und `x=1`)  bzgl. der verschiedene 6\r\n",
    "Klass(`c=0` und `c=1`) en an. Wir erhal:ten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af68bfd5-e70e-4ddc-b68f-9525feb583ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for only those rows where Z is 0 (bad films)\n",
    "bad_films_Z1 = D_movies2[bad_films]['Z1']\n",
    "bad_films_Z2 = D_movies2[bad_films]['Z2']\n",
    "\n",
    "# Filter the DataFrame for only those rows where Z is 1 (good films)\n",
    "good_films_Z1 = D_movies2[good_films]['Z1']\n",
    "good_films_Z2 = D_movies2[good_films]['Z2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0f7ab96-bec2-48cb-81ad-258164e28249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wahrscheinlichkeit für Z1 = 0 gegeben, dass Z = 0: 0.75\n",
      "Wahrscheinlichkeit für Z1 = 1 gegeben, dass Z = 0: 0.25\n",
      "Wahrscheinlichkeit für Z1 = 0 gegeben, dass Z = 1: 0.5\n",
      "Wahrscheinlichkeit für Z1 = 1 gegeben, dass Z = 1: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Calculate the probability of Z1 being 0 given that Z is 0\n",
    "p_Z1_0_given_Z_0 = np.mean(bad_films_Z1 == 0)\n",
    "p_Z1_1_given_Z_0 = np.mean(bad_films_Z1 == 1)\n",
    "\n",
    "# Calculate the probability of Z1 being 0 given that Z is 0\n",
    "p_Z1_0_given_Z_1 = np.mean(good_films_Z1 == 0)\n",
    "p_Z1_1_given_Z_1 = np.mean(good_films_Z1 == 1)\n",
    "\n",
    "print(f\"Wahrscheinlichkeit für Z1 = 0 gegeben, dass Z = 0: {p_Z1_0_given_Z_0}\")\n",
    "print(f\"Wahrscheinlichkeit für Z1 = 1 gegeben, dass Z = 0: {p_Z1_1_given_Z_0}\")\n",
    "print(f\"Wahrscheinlichkeit für Z1 = 0 gegeben, dass Z = 1: {p_Z1_0_given_Z_1}\")\n",
    "print(f\"Wahrscheinlichkeit für Z1 = 1 gegeben, dass Z = 1: {p_Z1_1_given_Z_1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b72ad66-fc28-42ae-97fc-5d23ccfe0929",
   "metadata": {},
   "source": [
    "Wir erhalten\n",
    "$$P(x_1 = 0 | c = 0, D = D_{\\text{movies2}}) = \\frac{3}{4}$$\n",
    "$$P(x_1 = 1 | c = 0, D = D_{\\text{movies2}}) = \\frac{1}{4}$$\n",
    "$$P(x_2 = 0 | c = 0, D = D_{\\text{movies2}}) = \\frac{3}{4}$$\n",
    "$$P(x_2 = 1 | c = 0, D = D_{\\text{movies2}}) = \\frac{1}{4}$$\n",
    "für Klasse 0 und\n",
    "$$P(x_1 = 0 | c = 1, D = D_{\\text{movies2}}) = \\frac{3}{6}$$\n",
    "$$P(x_1 = 1 | c = 1, D = D_{\\text{movies2}}) = \\frac{3}{6}$$\n",
    "$$P(x_2 = 0 | c = 1, D = D_{\\text{movies2}}) = \\frac{4}{6}$$\n",
    "$$P(x_2 = 1 | c = 1, D = D_{\\text{movies2}}) = \\frac{2}{6}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0872d5bd-7ca1-4944-8d9e-5bf242b53b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Der neue Datenpunkt x*\n",
    "x_star = pd.Series({'Z1': 0, 'Z2': 1})\n",
    "\n",
    "# Finden Sie die Zeilen in D_movies2, die mit x* übereinstimmen\n",
    "matching_rows = np.all(D_movies2[['Z1', 'Z2']] == x_star, axis=1)\n",
    "\n",
    "# Extrahieren Sie die Klassenlabels der übereinstimmenden Zeilen\n",
    "matching_labels = D_movies2.loc[matching_rows, 'Z']\n",
    "\n",
    "# Berechnen Sie die Wahrscheinlichkeiten für jede Klasse\n",
    "p_class_0 = np.mean(matching_labels == 0)\n",
    "p_class_1 = np.mean(matching_labels == 1)\n",
    "\n",
    "print(f\"Wahrscheinlichkeit für Klasse 0: {p_class_0}\")\n",
    "print(f\"Wahrscheinlichkeit für Klasse 1: {p_class_1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd057a9-cd78-46c0-a61f-765fb7b08d8e",
   "metadata": {},
   "source": [
    ">In diesem Code wird `numpy` verwendet, um die Zeilen in `D_movies2` zu finden, die mit `x*` übereinstimmen, und die Klassenlabels dieser Zeilen zu extrahieren. Dann wird die durchschnittliche Übereinstimmung mit jeder Klasse berechnet, um die Wahrscheinlichkeiten zu erhalten.\n",
    "\n",
    "Wie man sieht, tritt die Klasse 1 häufiger unter der Merkmalsausprägung $x^*$ auf und deshalb erhalten wir\n",
    "$$\\text{{clf}}_{\\text{{Bayes}}_{D_{\\text{{movies2}}}}}(x^*) = 1$$\n",
    "Der Film $x^*$ wird also als \"gut\" klassifiziert.\n",
    "\n",
    "**Problem -->**\n",
    "1. wir müssen für jede zu erwartende Merkmalsausprägung $x^∗$ Beispiele im Trainingsdatensatz D haben, um $x^∗$ überhaupt klassifizieren zu können.\n",
    "   Z.B. Für den Datensatz $D_{\\text{{movies2}}}$ können wir für den Datenpunkt $x^{**} = (1,1)$ (d.h., ein in Europa gedrehter Film, der einen Oskar gewonnen hat) keine Klassifikation vorhersagen, da alle Wahrscheinlichkeiten undefiniert sind (d.h., gleich $\\frac{0}{0}$).\n",
    "2. Ein weiteres Problem (das auch prinzipiell schon beim KNN-Algorithmus aufgetreten ist) besteht darin, dass zur Berechnung von $\\text{{clf}}_{\\text{{Bayes}}_D}(x)$ stets der gesamte Datensatz $D$ vorgehalten werden muss. Insbesondere bei großen Datensätzen kann dies signifikant zu einer langen Berechnungszeit beitragen.\n",
    "\n",
    "**Lösung -->** von der <ins>allgemeinen Bayes-Klassifikation</ins> zur <ins>Naiven Bayes-Klassifikation</ins> wechseln.\n",
    "\n",
    "\n",
    "## 2.5.3. Klassifikationsalgorithmus: Naive Bayes-Klassifikation\n",
    "---\n",
    "\n",
    "Der Klassifikationsalgorithmus Naive Bayes-Klassifikator wird auf Szenarien angewendet, in denen Merkmale in einem endlichen Merkmalsraum definiert sind.\n",
    "\n",
    "Es unterscheidet sich von der allgemeinen Bayes-Klassifikation darin, dass, bei der Berechnung der bedingten Wahrscheinlichkeit $P(c | x,D)$, also der Wahrscheinlichkeit einer Klasse $c$ gegeben die Merkmale $x$ und den Datensatz $D$, wird eine Unabhängigkeitsannahme gemacht. \n",
    "\n",
    "Das bedeutet, dass wir \"naiv\" annehmen, dass die Merkmale $x$ unabhängig voneinander sind, wenn wir die Wahrscheinlichkeit berechnen. In der Realität sind die Merkmale oft nicht unabhängig, aber diese Annahme vereinfacht die Berechnung und macht den Algorithmus effizienter.\n",
    "\n",
    "Die resultierende Wahrscheinlichkeit $P(c | x,D)$ ist daher eine Schätzung des wahren Wertes. Diese Schätzung kann in einigen Fällen ungenau sein, insbesondere wenn die Unabhängigkeitsannahme stark verletzt ist. Aber in vielen Anwendungsfällen funktioniert der Naive Bayes-Klassifikator trotz dieser Annahme überraschend gut.\n",
    "**Formal:**\n",
    "Gegeben sei ein Datensatz $D$ und ein neuer Datenpunkt $x \\in Z_1 \\times ... \\times Z_n$. Der Naive Bayes-Klassifikator $clf_{NaiveBayes_D}$ wird definiert als:\n",
    "\n",
    "$$clf_{NaiveBayes_D}(x) = \\arg\\max_{c \\in Z} P(c | D)P(x_1 | c,D)P(x_2 | c,D)...P(x_n | c,D)$$\n",
    "\n",
    "mit\n",
    "\n",
    "$$P(c | D) = \\frac{|{(z, c) \\in D}|}{|D|}$$\n",
    "\n",
    "$$P(x_i | c,D) = \\frac{|{(z', c) \\in D | z' = (z_1,...,z_n),z_i = x_i}|}{|{(z, c) \\in D}|} \\text{ für } i = 1,...,n$$\n",
    "\n",
    "Der folgende Code verwendet den Naiven Bayes-Klassifikator, um einen synthetischen Klassifikationsdatensatz zu modellieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67078150",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.2438174   2.64178576 -2.44433124 ...  2.35845448  0.02988481\n",
      "   0.02296293]\n",
      " [-0.42104111 -0.75854682 -0.02426976 ... -1.20398791 -2.30845398\n",
      "   0.55613795]\n",
      " [-0.5688051  -0.73101635  0.40454672 ... -0.8498266  -0.96568188\n",
      "   0.7168087 ]\n",
      " ...\n",
      " [ 0.49135006  1.5432398  -0.71724746 ...  1.89329607 -2.20603017\n",
      "  -1.19670352]\n",
      " [-1.32489877 -0.98367453  0.17375312 ... -1.41242742 -0.67473117\n",
      "  -1.03197684]\n",
      " [ 0.74248278 -0.13160448  1.28153067 ...  0.72390933  3.48271338\n",
      "  -0.13537485]]\n",
      "[0 0 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1\n",
      " 0 1 1 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0\n",
      " 1 0 0 1 1 0 0 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Diese Zeile erzeugt einen zufälligen Klassifikationsdatensatz mit der Funktion `make_classification` aus der `sklearn.datasets` Bibliothek. `X` enthält die Merkmale und `y` enthält die Klassenlabels.\n",
    "X, y = make_classification()\n",
    "\n",
    "print(X)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7992bd9f-e05f-4403-9f12-425ab8f85bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 0, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hier wird ein Naiver Bayes-Klassifikator mit einer Gaußschen Verteilungsannahme erstellt und auf den Datensatz angewendet.\n",
    "clf = GaussianNB().fit(X,y)\n",
    "\n",
    "y_pred = clf.predict(X)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce6509c6-bdaa-4c6d-afd6-9081ca5527b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y,clf.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37daaa6e-87aa-4312-8b41-0784cb6ddbbc",
   "metadata": {},
   "source": [
    "Auf `D_movies2`angewendet, sieht der Code so aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e150c53-224c-4c21-992a-f7654e679b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 1 1 0 0 1 1 1]\n",
      "Modellgenauigkeit: 0.7\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Teilen Sie die Daten in Merkmale X und Klassenlabels y auf\n",
    "#X = data[:, :2]  # Z1 und Z2 sind die Merkmale\n",
    "#y = data[:, 2]   # Z ist das Klassenlabel\n",
    "X = D_movies2.iloc[:, :2].values\n",
    "y = D_movies2.iloc[:, 2].values\n",
    "\n",
    "# Erstellen Sie ein Naive Bayes Modell\n",
    "clf = GaussianNB().fit(X, y)\n",
    "\n",
    "# Machen Sie Vorhersagen mit den Daten\n",
    "y_pred = clf.predict(X)\n",
    "print(y_pred)\n",
    "\n",
    "# Berechnen Sie die Genauigkeit des Modells\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "\n",
    "print(f\"Modellgenauigkeit: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71d6a96d-0ccb-4b1c-bce6-9d14d2815d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "Vorhersage für x*: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Der neue Datenpunkt x*\n",
    "x_star = np.array([[0, 1]])\n",
    "\n",
    "# Vorhersage für x*\n",
    "y_pred_x_star = clf.predict(x_star)\n",
    "print(y_pred_x_star)\n",
    "print(f\"Vorhersage für x*: {y_pred[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5aed176e-6ec4-4bf0-9d1b-61984ffb6c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vorhersage für x**: 1\n"
     ]
    }
   ],
   "source": [
    "# Der neue Datenpunkt x**\n",
    "x_double_star = np.array([[1, 1]])\n",
    "\n",
    "# Vorhersage für x*\n",
    "y_pred_x_double_star = clf.predict(x_double_star)\n",
    "\n",
    "print(f\"Vorhersage für x**: {y_pred_x_double_star[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1f7e8f-0f0f-43de-90ec-c31c5438775e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
